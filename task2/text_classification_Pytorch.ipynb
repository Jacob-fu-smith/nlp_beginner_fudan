{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b73fd198",
   "metadata": {},
   "source": [
    "### import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "550de543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle as pkl\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0dcc9b",
   "metadata": {},
   "source": [
    "## Data preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab495ae8",
   "metadata": {},
   "source": [
    "### split trian dev test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55aadb46",
   "metadata": {},
   "source": [
    "see task1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdabe9e9",
   "metadata": {},
   "source": [
    "### padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "543c7ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = pd.read_csv('./raw_data/train.tsv', sep='\\t')\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a758d738",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['Phrase_len'] = df_data['Phrase'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96111952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e89dda13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\nlp\\lib\\site-packages\\seaborn\\_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Phrase_len', ylabel='count'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "([], [])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(df_data['Phrase_len'])\n",
    "plt.xticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24bb48e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\nlp\\lib\\site-packages\\seaborn\\distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Phrase_len', ylabel='Density'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(df_data['Phrase_len']) # plt.hist and sns.kdeplot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d78c05d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_size = sorted(df_data['Phrase_len'])[round(len(df_data)*0.95)]\n",
    "pad_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4c306d",
   "metadata": {},
   "source": [
    "### build my vocab and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d18ed0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "UNK, PAD = '<UNK>', '<PAD>'  # 未知字，padding符号\n",
    "\n",
    "def tokenizer_lemma(line):\n",
    "    '''\n",
    "    diff to task 1\n",
    "    '''\n",
    "    def get_wordnet_pos(tag):\n",
    "        '''\n",
    "        get the part-of-speech\n",
    "        '''\n",
    "        if tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN    \n",
    "    wnl = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(line.lower())              # 分词,同时大写换小写\n",
    "    tagged_sent = pos_tag(tokens, tagset='universal')     # 词性标注\n",
    "    tokens_lemma = [wnl.lemmatize(word, pos=get_wordnet_pos(tag)) for word, tag in tagged_sent]\n",
    "    return tokens_lemma\n",
    "    \n",
    "def build_vocab(data_path, vocab_path):\n",
    "    data = [_.strip().split('\\t')[0] for _ in open(data_path, 'r', encoding='utf-8').readlines()]\n",
    "    word_cnt = dict()\n",
    "    for sentence in tqdm(data):\n",
    "        for token in tokenizer_lemma(sentence):\n",
    "            word_cnt[token] = word_cnt.get(token, 0) + 1\n",
    "    word_cnt = sorted(word_cnt.items(), key=lambda x:x[0], reverse=True)\n",
    "    print(len(word_cnt))\n",
    "    vocab = {_[0]: idx for idx, _ in enumerate(word_cnt)}\n",
    "    vocab.update({UNK: len(vocab), PAD: len(vocab) + 1})\n",
    "    pkl.dump(vocab, open(vocab_path, 'wb'))\n",
    "    print(\"vocab build successed, size : %d\" %len(vocab))\n",
    "    return vocab\n",
    "\n",
    "def build_dataset(config):\n",
    "    '''\n",
    "    变成[([],y),([],y),([],y),([],y)]\n",
    "    '''\n",
    "    if os.path.exists(config.vocab_path):\n",
    "        vocab = pkl.load(open(config.vocab_path, 'rb'))\n",
    "    else:\n",
    "        vocab = build_vocab(config.train_path, config.vocab_path)\n",
    "    config.n_vocab = len(vocab)\n",
    "    print(\"vocab loaded sucessed, size : %d \" %len(vocab))\n",
    "    def load_data(file_path, output_path, pad_size=config.pad_size):\n",
    "        if os.path.exists(output_path):\n",
    "            data = pkl.load(open(output_path, 'rb'))\n",
    "            print(\"%s loaded success, size: %d\" %(output_path, len(data)))\n",
    "            return data\n",
    "        data = open(file_path, 'r', encoding='utf-8').readlines()\n",
    "        all_data = list()\n",
    "        for line in tqdm(data):\n",
    "            try:\n",
    "                x, y = line.strip().split('\\t')\n",
    "            except:\n",
    "                print(line)\n",
    "            tokens = tokenizer_lemma(x)\n",
    "            if len(tokens) < pad_size:\n",
    "                tokens.extend([PAD] * (pad_size - len(tokens)))\n",
    "            else:\n",
    "                tokens = tokens[:pad_size]\n",
    "            token_map = [vocab.get(token, vocab.get(UNK)) for token in tokens] # diff to one-hot\n",
    "            all_data.append((token_map, int(y)))\n",
    "        pkl.dump(all_data, open(output_path, 'wb'))\n",
    "        print(\"%s loaded success, size: %d\" %(file_path, len(all_data)))\n",
    "        return all_data\n",
    "    test_set = load_data(config.test_path, config.test_set_path)\n",
    "    dev_set = load_data(config.dev_path, config.dev_set_path)\n",
    "    train_set = load_data(config.train_path, config.train_set_path)\n",
    "    return train_set, dev_set, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79523b1",
   "metadata": {},
   "source": [
    "### glove process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20082a64",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-bc4103f7c31a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mw2vlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw2vlist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2vlist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "glove_path = r'D:\\workspace\\pretrained_models\\word2vec\\standford\\glove.6B.300d.txt'\n",
    "with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "    vocab = {}\n",
    "    data = list()\n",
    "    for i,line in enumerate(f.readlines()):\n",
    "        w2vlist = line.split()\n",
    "        word, vector = w2vlist[0], w2vlist[1:]\n",
    "        vocab[word] = i\n",
    "        data.append(vector)\n",
    "    data.append([0]*len(data[0]))\n",
    "    data.append([0]*len(data[0]))\n",
    "    vocab.update({UNK: len(vocab), PAD: len(vocab) + 1})\n",
    "    data = np.array(data,float)\n",
    "pkl.dump(vocab, open(r'D:\\workspace\\pretrained_models\\word2vec\\standford\\vocab.pkl', 'wb'))\n",
    "pkl.dump(data, open(r'D:\\workspace\\pretrained_models\\word2vec\\standford\\glove.6B.300d.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74edbf7e",
   "metadata": {},
   "source": [
    "## unit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bfafe9",
   "metadata": {},
   "source": [
    "### config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f3300e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    def __init__(self, embed_path=None, vocab_path=None):\n",
    "        self.train_path = r'./data_set/train.txt'\n",
    "        self.dev_path = r'./data_set/dev.txt'\n",
    "        self.test_path = r'./data_set/test.txt'\n",
    "        self.vocab_path = vocab_path if vocab_path else r'./data_set/vocab.pkl'\n",
    "        self.train_set_path = r'./data_set/train.pkl'\n",
    "        self.dev_set_path = r'./data_set/dev.pkl'\n",
    "        self.test_set_path = r'./data_set/test.pkl'\n",
    "        self.class_list = [x.strip() for x in open(r'./data_set/labels.txt', encoding='utf-8').readlines()]\n",
    "        self.label_num = len(self.class_list)\n",
    "        self.n_vocab = 0    # assign after dataset built\n",
    "        self.batch_size = 32\n",
    "        self.epoch = 20\n",
    "        self.learning_rate = 1e-3\n",
    "        self.drop_out = 0.3\n",
    "        self.log_step = 1000\n",
    "        self.pad_size = 128\n",
    "        self.embedding_pretrained = torch.FloatTensor(pkl.load(open(embed_path, 'rb'))) if embed_path is not None else None\n",
    "        self.embedding_len = self.embedding_pretrained.size(1) if self.embedding_pretrained is not None else 300\n",
    "\n",
    "        self.rnn_hidden = 128\n",
    "        self.rnn_layer = 2\n",
    "        self.bi_rnn = False\n",
    "        \n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db7cd113",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(embed_path = r'D:\\workspace\\pretrained_models\\word2vec\\standford\\glove.6B.50d.pkl',\n",
    "                vocab_path = r'D:\\workspace\\pretrained_models\\word2vec\\standford\\vocab.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6a4a73",
   "metadata": {},
   "source": [
    "### datasetIterater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45feda88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetIterater(object):\n",
    "    '''\n",
    "    return the batch index of dataset\n",
    "    '''\n",
    "    def __init__(self, data_set, batch_size, device):\n",
    "        self.device = device\n",
    "        self.data_set = data_set\n",
    "        self.batch_size = batch_size\n",
    "        self.n_batches = len(data_set) // batch_size\n",
    "        self.residue = True if len(data_set) % self.n_batches != 0 else False\n",
    "        self.index = 0\n",
    "        \n",
    "    def to_tensor(self, raw_batch):\n",
    "        x = torch.LongTensor([_[0] for _ in raw_batch]).to(self.device)\n",
    "        y = torch.LongTensor([_[1] for _ in raw_batch]).to(self.device)\n",
    "        return x,y\n",
    "        \n",
    "    def __next__(self):\n",
    "        if self.index == self.n_batches and self.residue:\n",
    "            raw_batch = self.data_set[self.index * self.batch_size: len(self.data_set)]\n",
    "            batch = self.to_tensor(raw_batch)\n",
    "            self.index += 1\n",
    "            return batch\n",
    "        elif self.index >= self.n_batches:\n",
    "            self.index = 0\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            raw_batch = self.data_set[self.index * self.batch_size: (self.index+1) * self.batch_size]\n",
    "            batch = self.to_tensor(raw_batch)\n",
    "            self.index += 1\n",
    "            return batch\n",
    "           \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_batches+1 if self.residue else self.n_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569f47c0",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e93bab25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab loaded sucessed, size : 14666 \n",
      "./data_set/test.pkl loaded success, size: 15605\n",
      "./data_set/dev.pkl loaded success, size: 31211\n",
      "./data_set/train.pkl loaded success, size: 109242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[11304,  4098,  5197,  ..., 14665, 14665, 14665],\n",
       "         [10453,  7905,  8159,  ..., 14665, 14665, 14665],\n",
       "         [12066, 14665, 14665,  ..., 14665, 14665, 14665],\n",
       "         ...,\n",
       "         [ 8760,  3156, 13399,  ..., 14665, 14665, 14665],\n",
       "         [14432, 12528,  4100,  ..., 14665, 14665, 14665],\n",
       "         [12226,  1549,  1729,  ..., 14665, 14665, 14665]], device='cuda:0'),\n",
       " tensor([3, 4, 2, 2, 3, 3, 3, 2, 2, 4, 1, 2, 2, 1, 2, 1, 2, 1, 3, 2, 2, 2, 2, 4,\n",
       "         2, 2, 3, 2, 2, 2, 1, 2], device='cuda:0'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config()\n",
    "train_set, dev_set, test_set = build_dataset(config)\n",
    "data_iter = DatasetIterater(test_set, config.batch_size, config.device)\n",
    "next(data_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd8c546",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44478081",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff012d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text_RNN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Text_RNN, self).__init__()\n",
    "        if config.embedding_pretrained is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(config.embedding_pretrained, freeze=False, padding_idx=config.n_vocab - 1)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(config.n_vocab, config.embedding_len, padding_idx=config.n_vocab - 1)\n",
    "        self.rnn = nn.RNN(config.embedding_len, config.rnn_hidden, config.rnn_layer,\n",
    "                         bidirectional=config.bi_rnn, batch_first=True, dropout=config.drop_out)\n",
    "        self.fc = nn.Linear(config.rnn_hidden * (int(config.bi_rnn) + 1), config.label_num)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out, hn = self.rnn(out) # \n",
    "        out = self.fc(out[:, -1, :]) # out[:, -1, :] equal to hn[-1, :, :]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7119817b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab loaded sucessed, size : 14666 \n",
      "./data_set/test.pkl loaded success, size: 15605\n",
      "./data_set/dev.pkl loaded success, size: 31211\n",
      "./data_set/train.pkl loaded success, size: 109242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.7910e-02,  1.5546e-01, -9.2568e-03, -2.2759e-02,  2.7480e-02],\n",
       "        [-4.4977e-02,  1.0058e-01,  1.2299e-02,  4.2659e-02,  6.3014e-03],\n",
       "        [-3.5497e-02,  6.0311e-02,  2.4067e-02, -4.9262e-02,  3.5513e-02],\n",
       "        [ 1.6361e-02,  1.2930e-01,  5.4046e-03,  5.6890e-02,  3.0290e-02],\n",
       "        [-1.3749e-02,  8.9010e-02, -4.4920e-03, -2.7621e-02, -1.6021e-02],\n",
       "        [-2.4887e-05,  1.3266e-01, -1.0529e-02, -4.3234e-02, -1.9217e-03],\n",
       "        [-1.9821e-02,  1.1832e-01,  6.7932e-03, -5.0656e-02,  3.8980e-02],\n",
       "        [ 8.3512e-03,  1.0779e-01, -1.4798e-02, -1.0075e-02,  3.4323e-03],\n",
       "        [-2.3932e-02,  5.5860e-02, -7.8181e-03,  7.7881e-03,  3.6686e-02],\n",
       "        [ 8.5755e-03,  7.8868e-02,  1.8836e-02, -1.0101e-02,  2.5203e-02],\n",
       "        [-1.9197e-02,  8.6813e-02,  5.5052e-02,  2.5683e-02, -1.8534e-02],\n",
       "        [-2.4522e-02,  9.8887e-02, -3.3557e-02,  6.6333e-03,  2.4462e-03],\n",
       "        [-5.3875e-02,  7.4225e-02, -7.5694e-03, -2.3797e-02,  4.8307e-03],\n",
       "        [-3.8163e-02,  8.0687e-02,  2.5012e-02, -4.2316e-02,  3.8004e-02],\n",
       "        [-2.5624e-02,  6.6479e-02, -8.3950e-03, -2.1423e-02,  4.1920e-02],\n",
       "        [-3.8876e-02,  1.2149e-01, -2.7184e-02,  1.3358e-02, -4.1898e-03],\n",
       "        [ 1.2794e-02,  1.0295e-01, -2.4104e-02, -1.0734e-02,  3.3960e-04],\n",
       "        [-2.9449e-02,  8.4123e-02,  1.9039e-02,  1.9703e-03,  1.0867e-02],\n",
       "        [-4.6034e-02,  8.6299e-02, -2.3726e-02, -4.0899e-03,  6.0077e-03],\n",
       "        [-1.9863e-02,  9.7922e-02,  1.1990e-02, -6.9847e-02,  5.2722e-04],\n",
       "        [ 3.3663e-03,  9.4509e-02,  8.0875e-03, -1.0499e-02,  1.0149e-02],\n",
       "        [ 7.9250e-04,  8.6420e-02,  2.5542e-02, -3.4802e-02, -4.7451e-03],\n",
       "        [ 2.0348e-02,  9.2341e-02,  1.8238e-02, -8.9096e-03,  5.6611e-02],\n",
       "        [-2.1229e-02,  7.6543e-02, -5.0211e-03, -1.5104e-02,  3.1583e-02],\n",
       "        [ 3.3275e-02,  1.1615e-01, -2.1482e-02, -8.1111e-03,  2.7096e-02],\n",
       "        [-1.4047e-02,  1.0978e-01,  6.3198e-03, -2.1107e-02, -1.7644e-02],\n",
       "        [-1.8293e-02,  1.2840e-01,  1.1166e-03, -2.7462e-02, -4.7288e-03],\n",
       "        [ 2.0429e-03,  8.4106e-02,  3.6163e-02,  2.4356e-04,  1.2295e-02],\n",
       "        [ 3.4426e-02,  1.0687e-01,  8.2326e-03, -2.5989e-02, -2.5407e-02],\n",
       "        [-3.4645e-02,  9.6835e-02, -1.9714e-02,  4.1547e-02, -2.3411e-02],\n",
       "        [ 2.7902e-02,  9.5954e-02,  4.1857e-02, -8.3385e-03,  4.0964e-02],\n",
       "        [ 1.8912e-02,  1.1149e-01,  8.7732e-03, -1.7855e-02, -2.0706e-02]],\n",
       "       device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config()\n",
    "train_set, dev_set, test_set = build_dataset(config)\n",
    "test_iter = DatasetIterater(test_set, config.batch_size, config.device)\n",
    "model = Text_RNN(config).to(config.device)\n",
    "model(next(test_iter)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3da7dc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab loaded sucessed, size : 400002 \n",
      "./data_set/test.pkl loaded success, size: 15605\n",
      "./data_set/dev.pkl loaded success, size: 31211\n",
      "./data_set/train.pkl loaded success, size: 109242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0366,  0.0027, -0.2050,  0.0321, -0.0569],\n",
       "        [-0.0663, -0.0724, -0.2401,  0.0217, -0.0550],\n",
       "        [ 0.0969, -0.0429, -0.2705, -0.0067,  0.0577],\n",
       "        [ 0.0935,  0.1445, -0.1963, -0.0608, -0.0881],\n",
       "        [ 0.1546,  0.0303, -0.2143,  0.0089, -0.0382],\n",
       "        [ 0.0967,  0.0165, -0.1687,  0.1061, -0.1828],\n",
       "        [ 0.1313,  0.0032, -0.2822, -0.0645, -0.2047],\n",
       "        [ 0.1485, -0.0350, -0.3598, -0.0492, -0.0327],\n",
       "        [ 0.1158,  0.0384, -0.1869, -0.0207, -0.0876],\n",
       "        [ 0.0111,  0.0610, -0.2645,  0.0637, -0.0687],\n",
       "        [ 0.1175,  0.0344, -0.2516, -0.0766, -0.0372],\n",
       "        [ 0.1109,  0.0572, -0.2740,  0.0721, -0.1134],\n",
       "        [ 0.0825,  0.0200, -0.3079, -0.0410,  0.0839],\n",
       "        [ 0.1488,  0.0948, -0.3173,  0.0884, -0.0226],\n",
       "        [ 0.1034,  0.1938, -0.2781, -0.0257,  0.0184],\n",
       "        [ 0.0577, -0.0589, -0.2229,  0.0811, -0.0558],\n",
       "        [-0.0291,  0.0730, -0.3126, -0.0565, -0.0738],\n",
       "        [ 0.0334, -0.0811, -0.2774,  0.0448, -0.0016],\n",
       "        [ 0.0581,  0.1068, -0.3462, -0.1238, -0.0766],\n",
       "        [-0.0005, -0.0372, -0.2309,  0.0252, -0.1240],\n",
       "        [ 0.1005,  0.1138, -0.2813,  0.0200, -0.0797],\n",
       "        [ 0.0831,  0.0059, -0.3066, -0.0146,  0.0693],\n",
       "        [ 0.1382,  0.0415, -0.1957,  0.0346, -0.1850],\n",
       "        [ 0.1026,  0.0722, -0.2822, -0.0754, -0.0922],\n",
       "        [ 0.0610,  0.0741, -0.2594,  0.0250,  0.0365],\n",
       "        [ 0.1003,  0.0826, -0.4002,  0.0688, -0.0105],\n",
       "        [ 0.1009, -0.0370, -0.3343, -0.0577,  0.0407],\n",
       "        [ 0.0496, -0.0106, -0.2429,  0.0260,  0.1444],\n",
       "        [-0.0604,  0.0082, -0.2046,  0.0439, -0.0589],\n",
       "        [ 0.0380, -0.0855, -0.2194,  0.0018, -0.0985],\n",
       "        [ 0.0785,  0.1320, -0.1876, -0.0056, -0.0315],\n",
       "        [ 0.0572,  0.0377, -0.3880,  0.1240,  0.0549]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config(embed_path = r'D:\\workspace\\pretrained_models\\word2vec\\standford\\glove.6B.50d.pkl',\n",
    "                vocab_path = r'D:\\workspace\\pretrained_models\\word2vec\\standford\\vocab.pkl')\n",
    "train_set, dev_set, test_set = build_dataset(config)\n",
    "test_iter = DatasetIterater(test_set, config.batch_size, config.device)\n",
    "model = Text_RNN(config).to(config.device)\n",
    "model(next(test_iter)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc6284e",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034cd47c",
   "metadata": {},
   "source": [
    "## train and evl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d5fa691",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_time_dif(start_time):\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    return timedelta(seconds=int(round(time_dif)))\n",
    "\n",
    "def init_network(model, method='xavier', exclude='embedding', seed=123):\n",
    "    for name, w in model.named_parameters():\n",
    "        if exclude not in name:\n",
    "            if 'weight' in name:\n",
    "                if method == 'xavier':\n",
    "                    nn.init.xavier_normal_(w)\n",
    "                elif method == 'kaiming':\n",
    "                    nn.init.kaiming_normal_(w)\n",
    "                else:\n",
    "                    nn.init.normal_(w)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(w, 0)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "def train(model, config, train_set, dev_set, test_set):\n",
    "    train_iter = DatasetIterater(train_set, config.batch_size, config.device)\n",
    "    dev_iter = DatasetIterater(dev_set, config.batch_size, config.device)\n",
    "    test_iter = DatasetIterater(test_set, config.batch_size, config.device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = config.learning_rate)\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    total_iter = 0\n",
    "    best_dev_acc = 0\n",
    "    for epoch in range(config.epoch):\n",
    "        print('Epoch [{}/{}]'.format(epoch + 1, config.epoch))\n",
    "        for i, (x, y_true) in enumerate(train_iter):\n",
    "            out = model(x)\n",
    "            model.zero_grad()\n",
    "            loss = F.cross_entropy(out, y_true)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_iter += 1\n",
    "            if total_iter % config.log_step == 0  or i+1 == len(train_iter):\n",
    "                acc_train, loss_train = evalute(model, config, DatasetIterater(train_set, config.batch_size, config.device))\n",
    "                acc_dev, loss_dev = evalute(model, config, dev_iter)\n",
    "                model.train()\n",
    "                improve = \"\"\n",
    "                if acc_dev > best_dev_acc:\n",
    "                    best_dev_acc = acc_dev\n",
    "                    improve = \"*\"\n",
    "                time_dif = get_time_dif(start_time)\n",
    "                msg = 'Iter: {0:>6},  Train Loss: {1:>5.2},  Train Acc: {2:>6.2%},  Val Loss: {3:>5.2},  Val Acc: {4:>6.2%},  Time: {5}{6}'\n",
    "                print(msg.format(total_iter, loss_train, acc_train, loss_dev, acc_dev, time_dif, improve))\n",
    "    acc_test, loss_test = evalute(model, config, test_iter)\n",
    "    msg = 'Train Loss: {0:>5.2},  Train Acc: {1:>6.2%}'\n",
    "    print(msg.format(loss_test, acc_test))\n",
    "\n",
    "def evalute(model, config, data_iter):\n",
    "    model.eval()\n",
    "    loss_total = 0\n",
    "    labels_predict = np.arange(0)\n",
    "    labels_true = []\n",
    "    with torch.no_grad():\n",
    "        for x, y_true in data_iter:\n",
    "            out = model(x)\n",
    "            loss = F.cross_entropy(out, y_true)\n",
    "            loss_total += loss\n",
    "            labels_predict = np.concatenate((labels_predict, out.data.cpu().numpy().argmax(axis=1)), axis=0)\n",
    "            labels_true += y_true\n",
    "        try:\n",
    "            assert len(labels_true) == labels_predict.size\n",
    "        except AssertionError as e:\n",
    "            print(len(x))\n",
    "            print(out.shape)\n",
    "            print(\"labels_true:\", len(labels_true))\n",
    "            print(\"labels_predict:\",labels_predict.size)\n",
    "            raise AssertionError\n",
    "    acc = (labels_true == labels_predict).mean()\n",
    "    loss = loss_total.data.cpu().numpy()/len(data_iter)\n",
    "    return acc, loss_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908cc186",
   "metadata": {},
   "source": [
    "## run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5d270f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab loaded sucessed, size : 14666 \n",
      "./data_set/test.pkl loaded success, size: 15605\n",
      "./data_set/dev.pkl loaded success, size: 31211\n",
      "./data_set/train.pkl loaded success, size: 109242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1bf3b72e8e8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20]\n",
      "Iter:   1000,  Train Loss: 4.4e+03,  Train Acc: 51.00%,  Val Loss: 1.3e+03,  Val Acc: 50.82%,  Time: 0:00:44*\n",
      "Iter:   2000,  Train Loss: 4.3e+03,  Train Acc: 51.08%,  Val Loss: 1.2e+03,  Val Acc: 50.96%,  Time: 0:01:29*\n",
      "Iter:   3000,  Train Loss: 4.3e+03,  Train Acc: 51.01%,  Val Loss: 1.2e+03,  Val Acc: 50.87%,  Time: 0:02:14\n",
      "Iter:   3414,  Train Loss: 4.3e+03,  Train Acc: 51.16%,  Val Loss: 1.2e+03,  Val Acc: 51.10%,  Time: 0:02:49*\n",
      "Epoch [2/20]\n",
      "Iter:   4000,  Train Loss: 4.3e+03,  Train Acc: 51.26%,  Val Loss: 1.2e+03,  Val Acc: 51.21%,  Time: 0:03:26*\n",
      "Iter:   5000,  Train Loss: 4.3e+03,  Train Acc: 51.44%,  Val Loss: 1.2e+03,  Val Acc: 51.29%,  Time: 0:04:11*\n",
      "Iter:   6000,  Train Loss: 4.3e+03,  Train Acc: 51.25%,  Val Loss: 1.2e+03,  Val Acc: 51.25%,  Time: 0:04:56\n",
      "Iter:   6828,  Train Loss: 4.3e+03,  Train Acc: 51.30%,  Val Loss: 1.2e+03,  Val Acc: 51.29%,  Time: 0:05:37\n",
      "Epoch [3/20]\n",
      "Iter:   7000,  Train Loss: 4.3e+03,  Train Acc: 51.53%,  Val Loss: 1.2e+03,  Val Acc: 51.40%,  Time: 0:06:08*\n",
      "Iter:   8000,  Train Loss: 4.3e+03,  Train Acc: 51.24%,  Val Loss: 1.2e+03,  Val Acc: 51.19%,  Time: 0:06:53\n",
      "Iter:   9000,  Train Loss: 4.2e+03,  Train Acc: 51.53%,  Val Loss: 1.2e+03,  Val Acc: 51.41%,  Time: 0:07:38*\n",
      "Iter:  10000,  Train Loss: 4.2e+03,  Train Acc: 51.40%,  Val Loss: 1.2e+03,  Val Acc: 51.24%,  Time: 0:08:24\n",
      "Iter:  10242,  Train Loss: 4.3e+03,  Train Acc: 51.30%,  Val Loss: 1.2e+03,  Val Acc: 51.38%,  Time: 0:08:56\n",
      "Epoch [4/20]\n",
      "Iter:  11000,  Train Loss: 4.2e+03,  Train Acc: 51.53%,  Val Loss: 1.2e+03,  Val Acc: 51.40%,  Time: 0:09:37\n",
      "Iter:  12000,  Train Loss: 4.2e+03,  Train Acc: 51.43%,  Val Loss: 1.2e+03,  Val Acc: 51.38%,  Time: 0:10:22\n",
      "Iter:  13000,  Train Loss: 4.2e+03,  Train Acc: 51.61%,  Val Loss: 1.2e+03,  Val Acc: 51.50%,  Time: 0:11:07*\n",
      "Iter:  13656,  Train Loss: 4.2e+03,  Train Acc: 51.63%,  Val Loss: 1.2e+03,  Val Acc: 51.41%,  Time: 0:11:47\n",
      "Epoch [5/20]\n",
      "Iter:  14000,  Train Loss: 4.2e+03,  Train Acc: 51.74%,  Val Loss: 1.2e+03,  Val Acc: 51.46%,  Time: 0:12:20\n",
      "Iter:  15000,  Train Loss: 4.2e+03,  Train Acc: 51.82%,  Val Loss: 1.2e+03,  Val Acc: 51.53%,  Time: 0:13:05*\n",
      "Iter:  16000,  Train Loss: 4.2e+03,  Train Acc: 51.84%,  Val Loss: 1.2e+03,  Val Acc: 51.53%,  Time: 0:13:50*\n",
      "Iter:  17000,  Train Loss: 4.2e+03,  Train Acc: 51.77%,  Val Loss: 1.2e+03,  Val Acc: 51.55%,  Time: 0:14:34*\n",
      "Iter:  17070,  Train Loss: 4.2e+03,  Train Acc: 51.90%,  Val Loss: 1.2e+03,  Val Acc: 51.57%,  Time: 0:15:03*\n",
      "Epoch [6/20]\n",
      "Iter:  18000,  Train Loss: 4.2e+03,  Train Acc: 51.97%,  Val Loss: 1.2e+03,  Val Acc: 51.66%,  Time: 0:15:47*\n",
      "Iter:  19000,  Train Loss: 4.2e+03,  Train Acc: 51.95%,  Val Loss: 1.2e+03,  Val Acc: 51.76%,  Time: 0:16:32*\n",
      "Iter:  20000,  Train Loss: 4.2e+03,  Train Acc: 52.03%,  Val Loss: 1.2e+03,  Val Acc: 51.85%,  Time: 0:17:17*\n",
      "Iter:  20484,  Train Loss: 4.2e+03,  Train Acc: 52.16%,  Val Loss: 1.2e+03,  Val Acc: 51.98%,  Time: 0:17:53*\n",
      "Epoch [7/20]\n",
      "Iter:  21000,  Train Loss: 4.3e+03,  Train Acc: 48.42%,  Val Loss: 1.2e+03,  Val Acc: 48.51%,  Time: 0:18:30\n",
      "Iter:  22000,  Train Loss: 4.2e+03,  Train Acc: 52.08%,  Val Loss: 1.2e+03,  Val Acc: 51.93%,  Time: 0:19:14\n",
      "Iter:  23000,  Train Loss: 4.1e+03,  Train Acc: 52.27%,  Val Loss: 1.2e+03,  Val Acc: 52.00%,  Time: 0:20:00*\n",
      "Iter:  23898,  Train Loss: 4.1e+03,  Train Acc: 52.41%,  Val Loss: 1.2e+03,  Val Acc: 52.12%,  Time: 0:20:43*\n",
      "Epoch [8/20]\n",
      "Iter:  24000,  Train Loss: 4.1e+03,  Train Acc: 52.12%,  Val Loss: 1.2e+03,  Val Acc: 51.96%,  Time: 0:21:12\n",
      "Iter:  25000,  Train Loss: 4.2e+03,  Train Acc: 52.07%,  Val Loss: 1.2e+03,  Val Acc: 51.84%,  Time: 0:21:57\n",
      "Iter:  26000,  Train Loss: 4.1e+03,  Train Acc: 52.40%,  Val Loss: 1.2e+03,  Val Acc: 52.12%,  Time: 0:22:42*\n",
      "Iter:  27000,  Train Loss: 4.1e+03,  Train Acc: 52.43%,  Val Loss: 1.2e+03,  Val Acc: 52.20%,  Time: 0:23:28*\n",
      "Iter:  27312,  Train Loss: 4.1e+03,  Train Acc: 52.57%,  Val Loss: 1.2e+03,  Val Acc: 52.24%,  Time: 0:24:01*\n",
      "Epoch [9/20]\n",
      "Iter:  28000,  Train Loss: 4.1e+03,  Train Acc: 52.24%,  Val Loss: 1.2e+03,  Val Acc: 52.09%,  Time: 0:24:41\n",
      "Iter:  29000,  Train Loss: 4.1e+03,  Train Acc: 52.20%,  Val Loss: 1.2e+03,  Val Acc: 52.07%,  Time: 0:25:26\n",
      "Iter:  30000,  Train Loss: 4.1e+03,  Train Acc: 52.42%,  Val Loss: 1.2e+03,  Val Acc: 52.03%,  Time: 0:26:11\n",
      "Iter:  30726,  Train Loss: 4.1e+03,  Train Acc: 52.72%,  Val Loss: 1.2e+03,  Val Acc: 52.44%,  Time: 0:26:52*\n",
      "Epoch [10/20]\n",
      "Iter:  31000,  Train Loss: 4.1e+03,  Train Acc: 52.72%,  Val Loss: 1.2e+03,  Val Acc: 52.46%,  Time: 0:27:24*\n",
      "Iter:  32000,  Train Loss: 4.1e+03,  Train Acc: 52.75%,  Val Loss: 1.2e+03,  Val Acc: 52.42%,  Time: 0:28:10\n",
      "Iter:  33000,  Train Loss: 4.1e+03,  Train Acc: 52.45%,  Val Loss: 1.2e+03,  Val Acc: 52.19%,  Time: 0:28:54\n",
      "Iter:  34000,  Train Loss: 4.1e+03,  Train Acc: 52.34%,  Val Loss: 1.2e+03,  Val Acc: 52.19%,  Time: 0:29:39\n",
      "Iter:  34140,  Train Loss: 4.1e+03,  Train Acc: 52.89%,  Val Loss: 1.2e+03,  Val Acc: 52.71%,  Time: 0:30:09*\n",
      "Epoch [11/20]\n",
      "Iter:  35000,  Train Loss: 4e+03,  Train Acc: 52.97%,  Val Loss: 1.2e+03,  Val Acc: 52.79%,  Time: 0:30:52*\n",
      "Iter:  36000,  Train Loss: 4e+03,  Train Acc: 52.81%,  Val Loss: 1.2e+03,  Val Acc: 52.59%,  Time: 0:31:37\n",
      "Iter:  37000,  Train Loss: 4e+03,  Train Acc: 53.06%,  Val Loss: 1.2e+03,  Val Acc: 52.83%,  Time: 0:32:22*\n",
      "Iter:  37554,  Train Loss: 4e+03,  Train Acc: 53.09%,  Val Loss: 1.2e+03,  Val Acc: 53.00%,  Time: 0:32:59*\n",
      "Epoch [12/20]\n",
      "Iter:  38000,  Train Loss: 4e+03,  Train Acc: 53.08%,  Val Loss: 1.2e+03,  Val Acc: 52.91%,  Time: 0:33:35\n",
      "Iter:  39000,  Train Loss: 4e+03,  Train Acc: 53.15%,  Val Loss: 1.2e+03,  Val Acc: 52.97%,  Time: 0:34:20\n",
      "Iter:  40000,  Train Loss: 4e+03,  Train Acc: 53.18%,  Val Loss: 1.2e+03,  Val Acc: 52.86%,  Time: 0:35:05\n",
      "Iter:  40968,  Train Loss: 4e+03,  Train Acc: 53.25%,  Val Loss: 1.2e+03,  Val Acc: 53.16%,  Time: 0:35:49*\n",
      "Epoch [13/20]\n",
      "Iter:  41000,  Train Loss: 4e+03,  Train Acc: 53.14%,  Val Loss: 1.2e+03,  Val Acc: 52.92%,  Time: 0:36:18\n",
      "Iter:  42000,  Train Loss: 4e+03,  Train Acc: 53.05%,  Val Loss: 1.2e+03,  Val Acc: 52.93%,  Time: 0:37:02\n",
      "Iter:  43000,  Train Loss: 4e+03,  Train Acc: 52.95%,  Val Loss: 1.2e+03,  Val Acc: 52.57%,  Time: 0:37:48\n",
      "Iter:  44000,  Train Loss: 4e+03,  Train Acc: 53.40%,  Val Loss: 1.2e+03,  Val Acc: 53.00%,  Time: 0:38:32\n",
      "Iter:  44382,  Train Loss: 4e+03,  Train Acc: 53.45%,  Val Loss: 1.1e+03,  Val Acc: 53.30%,  Time: 0:39:07*\n",
      "Epoch [14/20]\n",
      "Iter:  45000,  Train Loss: 4e+03,  Train Acc: 53.31%,  Val Loss: 1.2e+03,  Val Acc: 53.08%,  Time: 0:39:45\n",
      "Iter:  46000,  Train Loss: 4e+03,  Train Acc: 53.48%,  Val Loss: 1.1e+03,  Val Acc: 53.29%,  Time: 0:40:30\n",
      "Iter:  47000,  Train Loss: 4e+03,  Train Acc: 53.32%,  Val Loss: 1.2e+03,  Val Acc: 53.08%,  Time: 0:41:15\n",
      "Iter:  47796,  Train Loss: 4e+03,  Train Acc: 53.62%,  Val Loss: 1.1e+03,  Val Acc: 53.46%,  Time: 0:41:56*\n",
      "Epoch [15/20]\n",
      "Iter:  48000,  Train Loss: 4e+03,  Train Acc: 53.16%,  Val Loss: 1.2e+03,  Val Acc: 52.96%,  Time: 0:42:27\n",
      "Iter:  49000,  Train Loss: 4e+03,  Train Acc: 53.68%,  Val Loss: 1.1e+03,  Val Acc: 53.37%,  Time: 0:43:12\n",
      "Iter:  50000,  Train Loss: 4e+03,  Train Acc: 53.52%,  Val Loss: 1.1e+03,  Val Acc: 53.06%,  Time: 0:43:57\n",
      "Iter:  51000,  Train Loss: 4e+03,  Train Acc: 53.75%,  Val Loss: 1.1e+03,  Val Acc: 53.45%,  Time: 0:44:42\n",
      "Iter:  51210,  Train Loss: 4e+03,  Train Acc: 53.83%,  Val Loss: 1.1e+03,  Val Acc: 53.63%,  Time: 0:45:13*\n",
      "Epoch [16/20]\n",
      "Iter:  52000,  Train Loss: 3.9e+03,  Train Acc: 53.91%,  Val Loss: 1.1e+03,  Val Acc: 53.60%,  Time: 0:45:54\n",
      "Iter:  53000,  Train Loss: 3.9e+03,  Train Acc: 53.98%,  Val Loss: 1.1e+03,  Val Acc: 53.70%,  Time: 0:46:39*\n",
      "Iter:  54000,  Train Loss: 3.9e+03,  Train Acc: 53.96%,  Val Loss: 1.1e+03,  Val Acc: 53.58%,  Time: 0:47:25\n",
      "Iter:  54624,  Train Loss: 3.9e+03,  Train Acc: 54.00%,  Val Loss: 1.1e+03,  Val Acc: 53.76%,  Time: 0:48:03*\n",
      "Epoch [17/20]\n",
      "Iter:  55000,  Train Loss: 4e+03,  Train Acc: 53.66%,  Val Loss: 1.1e+03,  Val Acc: 53.14%,  Time: 0:48:38\n",
      "Iter:  56000,  Train Loss: 3.9e+03,  Train Acc: 54.04%,  Val Loss: 1.1e+03,  Val Acc: 53.54%,  Time: 0:49:23\n",
      "Iter:  57000,  Train Loss: 3.9e+03,  Train Acc: 54.03%,  Val Loss: 1.1e+03,  Val Acc: 53.63%,  Time: 0:50:08\n",
      "Iter:  58000,  Train Loss: 3.9e+03,  Train Acc: 54.14%,  Val Loss: 1.1e+03,  Val Acc: 53.67%,  Time: 0:50:53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  58038,  Train Loss: 3.9e+03,  Train Acc: 54.23%,  Val Loss: 1.1e+03,  Val Acc: 53.77%,  Time: 0:51:22*\n",
      "Epoch [18/20]\n",
      "Iter:  59000,  Train Loss: 3.9e+03,  Train Acc: 54.21%,  Val Loss: 1.1e+03,  Val Acc: 53.67%,  Time: 0:52:06\n",
      "Iter:  60000,  Train Loss: 3.9e+03,  Train Acc: 54.09%,  Val Loss: 1.1e+03,  Val Acc: 53.73%,  Time: 0:52:51\n",
      "Iter:  61000,  Train Loss: 3.9e+03,  Train Acc: 54.16%,  Val Loss: 1.1e+03,  Val Acc: 53.44%,  Time: 0:53:36\n",
      "Iter:  61452,  Train Loss: 3.9e+03,  Train Acc: 54.35%,  Val Loss: 1.1e+03,  Val Acc: 53.87%,  Time: 0:54:12*\n",
      "Epoch [19/20]\n",
      "Iter:  62000,  Train Loss: 3.9e+03,  Train Acc: 54.40%,  Val Loss: 1.1e+03,  Val Acc: 53.84%,  Time: 0:54:49\n",
      "Iter:  63000,  Train Loss: 3.9e+03,  Train Acc: 54.50%,  Val Loss: 1.1e+03,  Val Acc: 53.89%,  Time: 0:55:34*\n",
      "Iter:  64000,  Train Loss: 3.9e+03,  Train Acc: 54.43%,  Val Loss: 1.1e+03,  Val Acc: 53.83%,  Time: 0:56:18\n",
      "Iter:  64866,  Train Loss: 3.9e+03,  Train Acc: 54.57%,  Val Loss: 1.1e+03,  Val Acc: 54.00%,  Time: 0:57:00*\n",
      "Epoch [20/20]\n",
      "Iter:  65000,  Train Loss: 3.9e+03,  Train Acc: 54.42%,  Val Loss: 1.1e+03,  Val Acc: 53.99%,  Time: 0:57:31\n",
      "Iter:  66000,  Train Loss: 3.9e+03,  Train Acc: 53.57%,  Val Loss: 1.1e+03,  Val Acc: 53.21%,  Time: 0:58:16\n",
      "Iter:  67000,  Train Loss: 3.9e+03,  Train Acc: 54.45%,  Val Loss: 1.1e+03,  Val Acc: 53.99%,  Time: 0:59:01\n",
      "Iter:  68000,  Train Loss: 3.9e+03,  Train Acc: 54.67%,  Val Loss: 1.1e+03,  Val Acc: 54.01%,  Time: 0:59:45*\n",
      "Iter:  68280,  Train Loss: 3.9e+03,  Train Acc: 54.75%,  Val Loss: 1.1e+03,  Val Acc: 54.19%,  Time: 1:00:18*\n",
      "Train Loss: 5.6e+02,  Train Acc: 54.26%\n"
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "train_set, dev_set, test_set = build_dataset(config)\n",
    "### customized hyper param\n",
    "config.learning_rate = 1e-5 # lr = 1e-3 导致模型震荡，acc上不去\n",
    "config.log_step = 1000\n",
    "config.drop_out = 0.3\n",
    "###\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed_all(1)\n",
    "torch.backends.cudnn.deterministic = True  # 保证每次结果一样\n",
    "\n",
    "model = Text_RNN(config).to(config.device)\n",
    "init_network(model)\n",
    "\n",
    "train(model, config, train_set, dev_set, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eeed5acb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab loaded sucessed, size : 400002 \n",
      "./data_set/test.pkl loaded success, size: 15605\n",
      "./data_set/dev.pkl loaded success, size: 31211\n",
      "./data_set/train.pkl loaded success, size: 109242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1bf3b72e8e8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20]\n",
      "Iter:   1000,  Train Loss: 5.4e+03,  Train Acc: 51.00%,  Val Loss: 1.5e+03,  Val Acc: 50.82%,  Time: 0:01:12*\n",
      "Iter:   2000,  Train Loss: 4.9e+03,  Train Acc: 17.52%,  Val Loss: 1.4e+03,  Val Acc: 17.67%,  Time: 0:02:24\n",
      "Iter:   3000,  Train Loss: 4.9e+03,  Train Acc: 51.00%,  Val Loss: 1.4e+03,  Val Acc: 50.82%,  Time: 0:03:34\n",
      "Iter:   3414,  Train Loss: 6.1e+03,  Train Acc: 21.11%,  Val Loss: 1.8e+03,  Val Acc: 20.97%,  Time: 0:04:20\n",
      "Epoch [2/20]\n",
      "Iter:   4000,  Train Loss: 5.1e+03,  Train Acc: 51.00%,  Val Loss: 1.5e+03,  Val Acc: 50.82%,  Time: 0:05:13\n",
      "Iter:   5000,  Train Loss: 4.6e+03,  Train Acc: 51.00%,  Val Loss: 1.3e+03,  Val Acc: 50.82%,  Time: 0:06:25\n",
      "Iter:   6000,  Train Loss: 5.5e+03,  Train Acc: 21.11%,  Val Loss: 1.6e+03,  Val Acc: 20.97%,  Time: 0:07:35\n",
      "Iter:   6828,  Train Loss: 6.1e+03,  Train Acc: 21.11%,  Val Loss: 1.8e+03,  Val Acc: 20.97%,  Time: 0:08:39\n",
      "Epoch [3/20]\n",
      "Iter:   7000,  Train Loss: 5e+03,  Train Acc: 51.00%,  Val Loss: 1.4e+03,  Val Acc: 50.82%,  Time: 0:09:14\n",
      "Iter:   8000,  Train Loss: 4.9e+03,  Train Acc: 21.11%,  Val Loss: 1.4e+03,  Val Acc: 20.97%,  Time: 0:10:24\n",
      "Iter:   9000,  Train Loss: 4.5e+03,  Train Acc: 51.00%,  Val Loss: 1.3e+03,  Val Acc: 50.82%,  Time: 0:11:36\n",
      "Iter:  10000,  Train Loss: 6e+03,  Train Acc: 51.00%,  Val Loss: 1.7e+03,  Val Acc: 50.82%,  Time: 0:12:47\n",
      "Iter:  10242,  Train Loss: 6.1e+03,  Train Acc: 21.11%,  Val Loss: 1.8e+03,  Val Acc: 20.97%,  Time: 0:13:25\n",
      "Epoch [4/20]\n",
      "Iter:  11000,  Train Loss: 5.6e+03,  Train Acc: 17.52%,  Val Loss: 1.6e+03,  Val Acc: 17.67%,  Time: 0:14:25\n",
      "Iter:  12000,  Train Loss: 5.2e+03,  Train Acc: 51.00%,  Val Loss: 1.5e+03,  Val Acc: 50.82%,  Time: 0:15:37\n",
      "Iter:  13000,  Train Loss: 4.6e+03,  Train Acc: 51.00%,  Val Loss: 1.3e+03,  Val Acc: 50.82%,  Time: 0:16:49\n",
      "Iter:  13656,  Train Loss: 6.1e+03,  Train Acc: 21.11%,  Val Loss: 1.8e+03,  Val Acc: 20.97%,  Time: 0:17:44\n",
      "Epoch [5/20]\n",
      "Iter:  14000,  Train Loss: 5.4e+03,  Train Acc: 51.00%,  Val Loss: 1.6e+03,  Val Acc: 50.82%,  Time: 0:18:27\n",
      "Iter:  15000,  Train Loss: 4.8e+03,  Train Acc: 51.00%,  Val Loss: 1.4e+03,  Val Acc: 50.82%,  Time: 0:19:38\n",
      "Iter:  16000,  Train Loss: 4.8e+03,  Train Acc: 51.00%,  Val Loss: 1.4e+03,  Val Acc: 50.82%,  Time: 0:20:48\n",
      "Iter:  17000,  Train Loss: 5.3e+03,  Train Acc: 51.00%,  Val Loss: 1.5e+03,  Val Acc: 50.82%,  Time: 0:22:00\n",
      "Iter:  17070,  Train Loss: 6.1e+03,  Train Acc: 21.11%,  Val Loss: 1.8e+03,  Val Acc: 20.97%,  Time: 0:22:31\n",
      "Epoch [6/20]\n",
      "Iter:  18000,  Train Loss: 5.3e+03,  Train Acc: 51.00%,  Val Loss: 1.5e+03,  Val Acc: 50.82%,  Time: 0:23:39\n",
      "Iter:  19000,  Train Loss: 5e+03,  Train Acc: 51.00%,  Val Loss: 1.4e+03,  Val Acc: 50.82%,  Time: 0:24:49\n",
      "Iter:  20000,  Train Loss: 5.2e+03,  Train Acc: 51.00%,  Val Loss: 1.5e+03,  Val Acc: 50.82%,  Time: 0:26:00\n",
      "Iter:  20484,  Train Loss: 6.1e+03,  Train Acc: 21.11%,  Val Loss: 1.8e+03,  Val Acc: 20.97%,  Time: 0:26:48\n",
      "Epoch [7/20]\n",
      "Iter:  21000,  Train Loss: 5.1e+03,  Train Acc: 21.11%,  Val Loss: 1.5e+03,  Val Acc: 20.97%,  Time: 0:27:38\n",
      "Iter:  22000,  Train Loss: 5.9e+03,  Train Acc: 51.00%,  Val Loss: 1.7e+03,  Val Acc: 50.82%,  Time: 0:28:49\n",
      "Iter:  23000,  Train Loss: 5.4e+03,  Train Acc:  5.84%,  Val Loss: 1.5e+03,  Val Acc:  5.83%,  Time: 0:30:00\n",
      "Iter:  23898,  Train Loss: 6.1e+03,  Train Acc: 21.11%,  Val Loss: 1.8e+03,  Val Acc: 20.97%,  Time: 0:31:07\n",
      "Epoch [8/20]\n",
      "Iter:  24000,  Train Loss: 5.9e+03,  Train Acc: 17.52%,  Val Loss: 1.7e+03,  Val Acc: 17.67%,  Time: 0:31:39\n",
      "Iter:  25000,  Train Loss: 5.3e+03,  Train Acc: 51.00%,  Val Loss: 1.5e+03,  Val Acc: 50.82%,  Time: 0:32:49\n",
      "Iter:  26000,  Train Loss: 5.7e+03,  Train Acc: 51.00%,  Val Loss: 1.6e+03,  Val Acc: 50.82%,  Time: 0:34:00\n",
      "Iter:  27000,  Train Loss: 5.5e+03,  Train Acc: 51.00%,  Val Loss: 1.6e+03,  Val Acc: 50.82%,  Time: 0:35:10\n",
      "Iter:  27312,  Train Loss: 6.1e+03,  Train Acc: 21.11%,  Val Loss: 1.8e+03,  Val Acc: 20.97%,  Time: 0:35:52\n",
      "Epoch [9/20]\n",
      "Iter:  28000,  Train Loss: 5e+03,  Train Acc: 51.00%,  Val Loss: 1.4e+03,  Val Acc: 50.82%,  Time: 0:36:49\n",
      "Iter:  29000,  Train Loss: 4.7e+03,  Train Acc: 51.00%,  Val Loss: 1.3e+03,  Val Acc: 50.82%,  Time: 0:37:59\n",
      "Iter:  30000,  Train Loss: 4.7e+03,  Train Acc: 21.11%,  Val Loss: 1.4e+03,  Val Acc: 20.97%,  Time: 0:39:09\n",
      "Iter:  30726,  Train Loss: 6.1e+03,  Train Acc: 21.11%,  Val Loss: 1.8e+03,  Val Acc: 20.97%,  Time: 0:40:09\n",
      "Epoch [10/20]\n",
      "Iter:  31000,  Train Loss: 4.5e+03,  Train Acc: 51.00%,  Val Loss: 1.3e+03,  Val Acc: 50.82%,  Time: 0:40:49\n",
      "Iter:  32000,  Train Loss: 5.4e+03,  Train Acc: 51.00%,  Val Loss: 1.6e+03,  Val Acc: 50.82%,  Time: 0:41:59\n",
      "Iter:  33000,  Train Loss: 4.9e+03,  Train Acc: 51.00%,  Val Loss: 1.4e+03,  Val Acc: 50.82%,  Time: 0:43:10\n",
      "Iter:  34000,  Train Loss: 5.8e+03,  Train Acc: 21.11%,  Val Loss: 1.7e+03,  Val Acc: 20.97%,  Time: 0:44:20\n",
      "Iter:  34140,  Train Loss: 6.1e+03,  Train Acc: 21.11%,  Val Loss: 1.8e+03,  Val Acc: 20.97%,  Time: 0:44:54\n",
      "Epoch [11/20]\n",
      "Iter:  35000,  Train Loss: 4.6e+03,  Train Acc: 51.00%,  Val Loss: 1.3e+03,  Val Acc: 50.82%,  Time: 0:45:58\n",
      "Iter:  36000,  Train Loss: 5.2e+03,  Train Acc: 17.52%,  Val Loss: 1.5e+03,  Val Acc: 17.67%,  Time: 0:47:09\n",
      "Iter:  37000,  Train Loss: 4.8e+03,  Train Acc: 51.00%,  Val Loss: 1.4e+03,  Val Acc: 50.82%,  Time: 0:48:21\n",
      "Iter:  37554,  Train Loss: 6.1e+03,  Train Acc: 21.11%,  Val Loss: 1.8e+03,  Val Acc: 20.97%,  Time: 0:49:13\n",
      "Epoch [12/20]\n",
      "Iter:  38000,  Train Loss: 4.7e+03,  Train Acc: 51.00%,  Val Loss: 1.4e+03,  Val Acc: 50.82%,  Time: 0:50:00\n",
      "Iter:  39000,  Train Loss: 5e+03,  Train Acc: 51.00%,  Val Loss: 1.4e+03,  Val Acc: 50.82%,  Time: 0:51:12\n",
      "Iter:  40000,  Train Loss: 5.2e+03,  Train Acc: 51.00%,  Val Loss: 1.5e+03,  Val Acc: 50.82%,  Time: 0:52:22\n",
      "Iter:  40968,  Train Loss: 6.1e+03,  Train Acc: 21.11%,  Val Loss: 1.8e+03,  Val Acc: 20.97%,  Time: 0:53:32\n",
      "Epoch [13/20]\n",
      "Iter:  41000,  Train Loss: 5.3e+03,  Train Acc: 51.00%,  Val Loss: 1.5e+03,  Val Acc: 50.82%,  Time: 0:54:01\n",
      "Iter:  42000,  Train Loss: 4.5e+03,  Train Acc: 51.00%,  Val Loss: 1.3e+03,  Val Acc: 50.82%,  Time: 0:55:13\n",
      "Iter:  43000,  Train Loss: 5.2e+03,  Train Acc: 51.00%,  Val Loss: 1.5e+03,  Val Acc: 50.82%,  Time: 0:56:25\n",
      "Iter:  44000,  Train Loss: 6e+03,  Train Acc: 51.00%,  Val Loss: 1.7e+03,  Val Acc: 50.82%,  Time: 0:57:35\n",
      "Iter:  44382,  Train Loss: 6.1e+03,  Train Acc: 21.11%,  Val Loss: 1.8e+03,  Val Acc: 20.97%,  Time: 0:58:20\n",
      "Epoch [14/20]\n",
      "Iter:  45000,  Train Loss: 6.2e+03,  Train Acc: 21.11%,  Val Loss: 1.8e+03,  Val Acc: 20.97%,  Time: 0:59:14\n",
      "Iter:  46000,  Train Loss: 7.1e+03,  Train Acc: 51.00%,  Val Loss: 2e+03,  Val Acc: 50.82%,  Time: 1:00:26\n",
      "Iter:  47000,  Train Loss: 5e+03,  Train Acc: 51.00%,  Val Loss: 1.4e+03,  Val Acc: 50.82%,  Time: 1:01:36\n",
      "Iter:  47796,  Train Loss: 6.1e+03,  Train Acc: 21.11%,  Val Loss: 1.8e+03,  Val Acc: 20.97%,  Time: 1:02:37\n",
      "Epoch [15/20]\n",
      "Iter:  48000,  Train Loss: 4.6e+03,  Train Acc: 51.00%,  Val Loss: 1.3e+03,  Val Acc: 50.82%,  Time: 1:03:13\n",
      "Iter:  49000,  Train Loss: 4.6e+03,  Train Acc: 51.00%,  Val Loss: 1.3e+03,  Val Acc: 50.82%,  Time: 1:04:24\n",
      "Iter:  50000,  Train Loss: 6.5e+03,  Train Acc: 21.11%,  Val Loss: 1.9e+03,  Val Acc: 20.97%,  Time: 1:05:35\n",
      "Iter:  51000,  Train Loss: 4.8e+03,  Train Acc: 21.11%,  Val Loss: 1.4e+03,  Val Acc: 20.97%,  Time: 1:06:45\n",
      "Iter:  51210,  Train Loss: 6.1e+03,  Train Acc: 21.11%,  Val Loss: 1.8e+03,  Val Acc: 20.97%,  Time: 1:07:22\n",
      "Epoch [16/20]\n",
      "Iter:  52000,  Train Loss: 5e+03,  Train Acc: 51.00%,  Val Loss: 1.4e+03,  Val Acc: 50.82%,  Time: 1:08:22\n",
      "Iter:  53000,  Train Loss: 6e+03,  Train Acc: 51.00%,  Val Loss: 1.7e+03,  Val Acc: 50.82%,  Time: 1:09:31\n",
      "Iter:  54000,  Train Loss: 5e+03,  Train Acc: 21.11%,  Val Loss: 1.4e+03,  Val Acc: 20.97%,  Time: 1:10:40\n",
      "Iter:  54624,  Train Loss: 6.1e+03,  Train Acc: 21.11%,  Val Loss: 1.8e+03,  Val Acc: 20.97%,  Time: 1:11:33\n",
      "Epoch [17/20]\n",
      "Iter:  55000,  Train Loss: 4.7e+03,  Train Acc: 51.00%,  Val Loss: 1.3e+03,  Val Acc: 50.82%,  Time: 1:12:16\n",
      "Iter:  56000,  Train Loss: 5e+03,  Train Acc: 51.00%,  Val Loss: 1.4e+03,  Val Acc: 50.82%,  Time: 1:13:25\n",
      "Iter:  57000,  Train Loss: 5e+03,  Train Acc: 51.00%,  Val Loss: 1.4e+03,  Val Acc: 50.82%,  Time: 1:14:36\n",
      "Iter:  58000,  Train Loss: 4.7e+03,  Train Acc: 51.00%,  Val Loss: 1.3e+03,  Val Acc: 50.82%,  Time: 1:15:45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  58038,  Train Loss: 6.1e+03,  Train Acc: 21.11%,  Val Loss: 1.8e+03,  Val Acc: 20.97%,  Time: 1:16:14\n",
      "Epoch [18/20]\n",
      "Iter:  59000,  Train Loss: 4.5e+03,  Train Acc: 51.00%,  Val Loss: 1.3e+03,  Val Acc: 50.82%,  Time: 1:17:23\n",
      "Iter:  60000,  Train Loss: 4.7e+03,  Train Acc: 51.00%,  Val Loss: 1.3e+03,  Val Acc: 50.82%,  Time: 1:18:32\n",
      "Iter:  61000,  Train Loss: 4.9e+03,  Train Acc: 21.11%,  Val Loss: 1.4e+03,  Val Acc: 20.97%,  Time: 1:19:41\n",
      "Iter:  61452,  Train Loss: 6.1e+03,  Train Acc: 21.11%,  Val Loss: 1.8e+03,  Val Acc: 20.97%,  Time: 1:20:28\n",
      "Epoch [19/20]\n",
      "Iter:  62000,  Train Loss: 5e+03,  Train Acc: 51.00%,  Val Loss: 1.4e+03,  Val Acc: 50.82%,  Time: 1:21:18\n",
      "Iter:  63000,  Train Loss: 4.9e+03,  Train Acc: 51.00%,  Val Loss: 1.4e+03,  Val Acc: 50.82%,  Time: 1:22:28\n",
      "Iter:  64000,  Train Loss: 4.6e+03,  Train Acc: 51.00%,  Val Loss: 1.3e+03,  Val Acc: 50.82%,  Time: 1:23:36\n",
      "Iter:  64866,  Train Loss: 6.1e+03,  Train Acc: 21.11%,  Val Loss: 1.8e+03,  Val Acc: 20.97%,  Time: 1:24:40\n",
      "Epoch [20/20]\n",
      "Iter:  65000,  Train Loss: 5.2e+03,  Train Acc: 21.11%,  Val Loss: 1.5e+03,  Val Acc: 20.97%,  Time: 1:25:13\n",
      "Iter:  66000,  Train Loss: 4.7e+03,  Train Acc: 51.00%,  Val Loss: 1.3e+03,  Val Acc: 50.82%,  Time: 1:26:22\n",
      "Iter:  67000,  Train Loss: 4.6e+03,  Train Acc: 51.00%,  Val Loss: 1.3e+03,  Val Acc: 50.82%,  Time: 1:27:31\n",
      "Iter:  68000,  Train Loss: 4.5e+03,  Train Acc: 51.00%,  Val Loss: 1.3e+03,  Val Acc: 50.82%,  Time: 1:28:40\n",
      "Iter:  68280,  Train Loss: 6.1e+03,  Train Acc: 21.11%,  Val Loss: 1.8e+03,  Val Acc: 20.97%,  Time: 1:29:20\n",
      "Train Loss: 8.8e+02,  Train Acc: 20.93%\n"
     ]
    }
   ],
   "source": [
    "config = Config(embed_path = r'D:\\workspace\\pretrained_models\\word2vec\\standford\\glove.6B.300d.pkl',\n",
    "                vocab_path = r'D:\\workspace\\pretrained_models\\word2vec\\standford\\vocab.pkl')\n",
    "train_set, dev_set, test_set = build_dataset(config)\n",
    "### customized hyper param\n",
    "config.batch_size = 32\n",
    "config.learning_rate = 3e-2\n",
    "config.log_step = 1000\n",
    "config.drop_out = 0.3\n",
    "###\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed_all(1)\n",
    "torch.backends.cudnn.deterministic = True  # 保证每次结果一样\n",
    "\n",
    "model = Text_RNN(config).to(config.device)\n",
    "init_network(model)\n",
    "train(model, config, train_set, dev_set, test_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "327.68px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
