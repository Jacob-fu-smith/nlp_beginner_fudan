{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec71534b",
   "metadata": {},
   "source": [
    "start with __*__ should be run first. <br>\n",
    "read with '__table of comment__' is suggested! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73fd198",
   "metadata": {},
   "source": [
    "### *import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "550de543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle as pkl\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0dcc9b",
   "metadata": {},
   "source": [
    "## Data preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab495ae8",
   "metadata": {},
   "source": [
    "### split trian dev test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55aadb46",
   "metadata": {},
   "source": [
    "see task1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdabe9e9",
   "metadata": {},
   "source": [
    "### padding size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "543c7ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = pd.read_csv('./raw_data/train.tsv', sep='\\t')\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a758d738",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['Phrase_len'] = df_data['Phrase'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96111952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e89dda13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\nlp\\lib\\site-packages\\seaborn\\_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Phrase_len', ylabel='count'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "([], [])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(df_data['Phrase_len'])\n",
    "plt.xticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24bb48e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\nlp\\lib\\site-packages\\seaborn\\distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Phrase_len', ylabel='Density'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(df_data['Phrase_len']) # plt.hist and sns.kdeplot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d78c05d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_size = sorted(df_data['Phrase_len'])[round(len(df_data)*0.95)]\n",
    "pad_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4c306d",
   "metadata": {},
   "source": [
    "### *build my vocab and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d18ed0e",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "UNK, PAD = '<UNK>', '<PAD>'  # 未知字，padding符号\n",
    "\n",
    "def tokenizer_lemma(line):\n",
    "    '''\n",
    "    diff to task 1\n",
    "    '''\n",
    "    def get_wordnet_pos(tag):\n",
    "        '''\n",
    "        get the part-of-speech\n",
    "        '''\n",
    "        if tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN    \n",
    "    wnl = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(line.lower())              # 分词,同时大写换小写\n",
    "    tagged_sent = pos_tag(tokens, tagset='universal')     # 词性标注\n",
    "    tokens_lemma = [wnl.lemmatize(word, pos=get_wordnet_pos(tag)) for word, tag in tagged_sent]\n",
    "    return tokens_lemma\n",
    "    \n",
    "def build_vocab(data_path, vocab_path):\n",
    "    data = [_.strip().split('\\t')[0] for _ in open(data_path, 'r', encoding='utf-8').readlines()]\n",
    "    word_cnt = dict()\n",
    "    for sentence in tqdm(data):\n",
    "        for token in tokenizer_lemma(sentence):\n",
    "            word_cnt[token] = word_cnt.get(token, 0) + 1\n",
    "    word_cnt = sorted(word_cnt.items(), key=lambda x:x[0], reverse=True)\n",
    "    print(len(word_cnt))\n",
    "    vocab = {_[0]: idx for idx, _ in enumerate(word_cnt)}\n",
    "    vocab.update({UNK: len(vocab), PAD: len(vocab) + 1})\n",
    "    pkl.dump(vocab, open(vocab_path, 'wb'))\n",
    "    print(\"vocab build successed, size : %d\" %len(vocab))\n",
    "    return vocab\n",
    "\n",
    "def build_dataset(config):\n",
    "    '''\n",
    "    变成[([],y),([],y),([],y),([],y)]\n",
    "    '''\n",
    "    if os.path.exists(config.vocab_path):\n",
    "        vocab = pkl.load(open(config.vocab_path, 'rb'))\n",
    "    else:\n",
    "        vocab = build_vocab(config.train_path, config.vocab_path)\n",
    "    config.n_vocab = len(vocab)\n",
    "    print(\"vocab loaded sucessed, size : %d \" %len(vocab))\n",
    "    def load_data(file_path, output_path, pad_size=config.pad_size):\n",
    "        if os.path.exists(output_path):\n",
    "            data = pkl.load(open(output_path, 'rb'))\n",
    "            print(\"%s loaded success, size: %d\" %(output_path, len(data)))\n",
    "            return data\n",
    "        data = open(file_path, 'r', encoding='utf-8').readlines()\n",
    "        all_data = list()\n",
    "        for line in tqdm(data):\n",
    "            try:\n",
    "                x, y = line.strip().split('\\t')\n",
    "            except:\n",
    "                print(line)\n",
    "            tokens = tokenizer_lemma(x)\n",
    "            if len(tokens) < pad_size:\n",
    "                tokens.extend([PAD] * (pad_size - len(tokens)))\n",
    "            else:\n",
    "                tokens = tokens[:pad_size]\n",
    "            token_map = [vocab.get(token, vocab.get(UNK)) for token in tokens] # diff to one-hot\n",
    "            all_data.append((token_map, int(y)))\n",
    "        pkl.dump(all_data, open(output_path, 'wb'))\n",
    "        print(\"%s loaded success, size: %d\" %(file_path, len(all_data)))\n",
    "        return all_data\n",
    "    test_set = load_data(config.test_path, config.test_set_path)\n",
    "    dev_set = load_data(config.dev_path, config.dev_set_path)\n",
    "    train_set = load_data(config.train_path, config.train_set_path)\n",
    "    return train_set, dev_set, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79523b1",
   "metadata": {},
   "source": [
    "### glove process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20082a64",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-bc4103f7c31a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mw2vlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw2vlist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2vlist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "glove_path = r'D:\\workspace\\pretrained_models\\glove\\standford\\glove.6B.300d.txt'\n",
    "with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "    vocab = {}\n",
    "    data = list()\n",
    "    for i,line in enumerate(f.readlines()):\n",
    "        w2vlist = line.split()\n",
    "        word, vector = w2vlist[0], w2vlist[1:]\n",
    "        vocab[word] = i\n",
    "        data.append(vector)\n",
    "    data.append([0]*len(data[0]))\n",
    "    data.append([0]*len(data[0]))\n",
    "    vocab.update({UNK: len(vocab), PAD: len(vocab) + 1})\n",
    "    data = np.array(data,float)\n",
    "pkl.dump(vocab, open(r'D:\\workspace\\pretrained_models\\glove\\standford\\vocab.pkl', 'wb'))\n",
    "pkl.dump(data, open(r'D:\\workspace\\pretrained_models\\glove\\standford\\glove.6B.300d.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901da372",
   "metadata": {},
   "source": [
    "### word2vec process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a1f88548",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = r'/home2/rzhao/pretrained_models/word2vec/GoogleNews/word2vec-google-news-300.model'\n",
    "# vec_path = r'/home2/rzhao/pretrained_models/word2vec/GoogleNews/word2vec-google-news-300.model.vectors.npy'\n",
    "# word_map_path = r'/home2/rzhao/pretrained_models/word2vec/GoogleNews/word2vec-combined-ms-256.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b99ea752",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "w2v = KeyedVectors.load(model_path)\n",
    "# vec = np.load(vec_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "62d7a76c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home2/rzhao/pretrained_models/word2vec/GoogleNews/vocab.pkl'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'/home2/rzhao/pretrained_models/word2vec/GoogleNews/word2vec-google-news-300.pkl'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(os.path.dirname(model_path), 'vocab.pkl')\n",
    "os.path.join(os.path.dirname(vec_path), 'word2vec-google-news-300.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a143f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.key_to_index['<PAD>'] = w2v.get_index('PAD')\n",
    "w2v.key_to_index.pop('PAD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "274bc0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(w2v.key_to_index, open(r'/home2/rzhao/pretrained_models/word2vec/GoogleNews/vocab.pkl', 'wb'))\n",
    "pkl.dump(w2v.vectors, open(r'/home2/rzhao/pretrained_models/word2vec/GoogleNews/word2vec-google-news-300.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74edbf7e",
   "metadata": {},
   "source": [
    "## unit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bfafe9",
   "metadata": {},
   "source": [
    "### *config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f3300e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    def __init__(self, embed_path=None, vocab_path=None):\n",
    "        \n",
    "        # pararms for dataset\n",
    "        self.train_path = r'./data_set/train.txt'\n",
    "        self.dev_path = r'./data_set/dev.txt'\n",
    "        self.test_path = r'./data_set/test.txt'\n",
    "        self.vocab_path = vocab_path if vocab_path else r'./data_set/vocab.pkl'\n",
    "        self.train_set_path = r'./data_set/train.pkl'\n",
    "        self.dev_set_path = r'./data_set/dev.pkl'\n",
    "        self.test_set_path = r'./data_set/test.pkl'\n",
    "        self.class_list = [x.strip() for x in open(r'./data_set/labels.txt', encoding='utf-8').readlines()]\n",
    "        self.label_num = len(self.class_list)\n",
    "        self.n_vocab = 0    # assign after dataset built\n",
    "        \n",
    "        # params for train\n",
    "        self.batch_size = 32\n",
    "        self.epoch = 20\n",
    "        self.learning_rate = 1e-3\n",
    "        self.drop_out = 0.3\n",
    "        self.log_step = 1000\n",
    "        self.pad_size = 128\n",
    "        \n",
    "        # params for embedding\n",
    "        self.embedding_pretrained = torch.FloatTensor(pkl.load(open(embed_path, 'rb'))) if embed_path is not None else None\n",
    "        self.embedding_len = self.embedding_pretrained.size(1) if self.embedding_pretrained is not None else 300\n",
    "        \n",
    "        # params for rnn model\n",
    "        self.rnn_hidden = 128\n",
    "        self.rnn_layer = 2\n",
    "        self.bi_rnn = False\n",
    "        \n",
    "        # params for cnn model\n",
    "        self.num_channel = 256\n",
    "        self.kernel_sizes = (3, 4, 5)\n",
    "        \n",
    "        # device\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db7cd113",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(embed_path = r'/home2/rzhao/pretrained_models/glove/standford/glove.6B.50d.pkl',\n",
    "                vocab_path = r'/home2/rzhao/pretrained_models/glove/standford/vocab.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6a4a73",
   "metadata": {},
   "source": [
    "### *datasetIterater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45feda88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetIterater(object):\n",
    "    '''\n",
    "    return the batch index of dataset\n",
    "    '''\n",
    "    def __init__(self, data_set, batch_size, device):\n",
    "        self.device = device\n",
    "        self.data_set = data_set\n",
    "        self.batch_size = batch_size\n",
    "        self.n_batches = len(data_set) // batch_size\n",
    "        self.residue = True if len(data_set) % self.n_batches != 0 else False\n",
    "        self.index = 0\n",
    "        \n",
    "    def to_tensor(self, raw_batch):\n",
    "        x = torch.LongTensor([_[0] for _ in raw_batch]).to(self.device)\n",
    "        y = torch.LongTensor([_[1] for _ in raw_batch]).to(self.device)\n",
    "        return x,y\n",
    "        \n",
    "    def __next__(self):\n",
    "        if self.index == self.n_batches and self.residue:\n",
    "            raw_batch = self.data_set[self.index * self.batch_size: len(self.data_set)]\n",
    "            batch = self.to_tensor(raw_batch)\n",
    "            self.index += 1\n",
    "            return batch\n",
    "        elif self.index >= self.n_batches:\n",
    "            self.index = 0\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            raw_batch = self.data_set[self.index * self.batch_size: (self.index+1) * self.batch_size]\n",
    "            batch = self.to_tensor(raw_batch)\n",
    "            self.index += 1\n",
    "            return batch\n",
    "           \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_batches+1 if self.residue else self.n_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569f47c0",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e93bab25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab loaded sucessed, size : 14666 \n",
      "./data_set/test.pkl loaded success, size: 15605\n",
      "./data_set/dev.pkl loaded success, size: 31211\n",
      "./data_set/train.pkl loaded success, size: 109242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[11304,  4098,  5197,  ..., 14665, 14665, 14665],\n",
       "         [10453,  7905,  8159,  ..., 14665, 14665, 14665],\n",
       "         [12066, 14665, 14665,  ..., 14665, 14665, 14665],\n",
       "         ...,\n",
       "         [ 8760,  3156, 13399,  ..., 14665, 14665, 14665],\n",
       "         [14432, 12528,  4100,  ..., 14665, 14665, 14665],\n",
       "         [12226,  1549,  1729,  ..., 14665, 14665, 14665]], device='cuda:2'),\n",
       " tensor([3, 4, 2, 2, 3, 3, 3, 2, 2, 4, 1, 2, 2, 1, 2, 1, 2, 1, 3, 2, 2, 2, 2, 4,\n",
       "         2, 2, 3, 2, 2, 2, 1, 2], device='cuda:2'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config()\n",
    "train_set, dev_set, test_set = build_dataset(config)\n",
    "data_iter = DatasetIterater(test_set, config.batch_size, config.device)\n",
    "next(data_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd8c546",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44478081",
   "metadata": {},
   "source": [
    "### *RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff012d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text_RNN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Text_RNN, self).__init__()\n",
    "        if config.embedding_pretrained is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(config.embedding_pretrained, freeze=False, padding_idx=config.n_vocab - 1)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(config.n_vocab, config.embedding_len, padding_idx=config.n_vocab - 1)\n",
    "        self.rnn = nn.RNN(config.embedding_len, config.rnn_hidden, config.rnn_layer,\n",
    "                         bidirectional=config.bi_rnn, batch_first=True, dropout=config.drop_out)\n",
    "        self.fc = nn.Linear(config.rnn_hidden * (int(config.bi_rnn) + 1), config.label_num)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out, hn = self.rnn(out) # \n",
    "        out = self.fc(out[:, -1, :]) # out[:, -1, :] equal to hn[-1, :, :]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33b05ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab loaded sucessed, size : 14666 \n",
      "./data_set/test.pkl loaded success, size: 15605\n",
      "./data_set/dev.pkl loaded success, size: 31211\n",
      "./data_set/train.pkl loaded success, size: 109242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0578, -0.0882, -0.1269,  0.0738, -0.1413],\n",
       "        [ 0.0449, -0.0586, -0.1590,  0.0545, -0.1305],\n",
       "        [-0.0016, -0.0869, -0.1557,  0.0246, -0.1547],\n",
       "        [ 0.0395, -0.1018, -0.1495, -0.0057, -0.1169],\n",
       "        [ 0.0330, -0.0335, -0.1034,  0.0384, -0.1343],\n",
       "        [ 0.0182, -0.0998, -0.1502,  0.0231, -0.1522],\n",
       "        [ 0.0313, -0.0892, -0.1534, -0.0076, -0.1047],\n",
       "        [ 0.0413, -0.0655, -0.1689,  0.0423, -0.1119],\n",
       "        [ 0.0525, -0.0219, -0.1426,  0.0331, -0.0981],\n",
       "        [ 0.0557, -0.0421, -0.1882,  0.0445, -0.1272],\n",
       "        [ 0.0397, -0.0823, -0.1360,  0.0191, -0.1550],\n",
       "        [ 0.0313, -0.0525, -0.1787,  0.0908, -0.1697],\n",
       "        [ 0.0430, -0.0468, -0.1680,  0.0632, -0.1187],\n",
       "        [ 0.0420, -0.0739, -0.1469,  0.0287, -0.1285],\n",
       "        [ 0.0660, -0.0705, -0.1858,  0.0868, -0.1247],\n",
       "        [ 0.0452, -0.0730, -0.1502,  0.0417, -0.1506],\n",
       "        [ 0.0387, -0.0943, -0.1777,  0.0217, -0.1707],\n",
       "        [ 0.0124, -0.0424, -0.1227,  0.0809, -0.1487],\n",
       "        [ 0.0182, -0.0399, -0.1660,  0.0590, -0.1198],\n",
       "        [ 0.1094, -0.0700, -0.1812,  0.0414, -0.1136],\n",
       "        [ 0.0541, -0.0869, -0.1586,  0.0396, -0.1123],\n",
       "        [ 0.0420, -0.1099, -0.1515,  0.0520, -0.1392],\n",
       "        [ 0.0439,  0.0092, -0.1403,  0.0197, -0.1659],\n",
       "        [ 0.0284, -0.0694, -0.1917,  0.0613, -0.1238],\n",
       "        [ 0.0230, -0.0660, -0.1224,  0.0694, -0.1489],\n",
       "        [ 0.0351, -0.0795, -0.1697,  0.0769, -0.1163],\n",
       "        [ 0.0469, -0.0866, -0.1375,  0.0440, -0.1387],\n",
       "        [ 0.0264, -0.0992, -0.1291,  0.0616, -0.1442],\n",
       "        [ 0.0595, -0.0798, -0.1567,  0.0388, -0.1537],\n",
       "        [ 0.0355, -0.0460, -0.1083,  0.0634, -0.1315],\n",
       "        [ 0.0644, -0.0712, -0.1248,  0.0372, -0.0821],\n",
       "        [ 0.0572, -0.1125, -0.1857,  0.0673, -0.1258]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config()\n",
    "train_set, dev_set, test_set = build_dataset(config)\n",
    "test_iter = DatasetIterater(test_set, config.batch_size, config.device)\n",
    "model = Text_RNN(config).to(config.device)\n",
    "model(next(test_iter)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3da7dc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab loaded sucessed, size : 400002 \n",
      "./data_set/test.pkl loaded success, size: 15605\n",
      "./data_set/dev.pkl loaded success, size: 31211\n",
      "./data_set/train.pkl loaded success, size: 109242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0009,  0.0502, -0.0450, -0.2276, -0.0976],\n",
       "        [ 0.0955, -0.0842,  0.0492, -0.2244,  0.0926],\n",
       "        [ 0.0271, -0.0445, -0.0486, -0.1505,  0.0639],\n",
       "        [-0.1559, -0.0810, -0.0259, -0.1507,  0.0435],\n",
       "        [ 0.1281, -0.0728, -0.0193, -0.4005,  0.1208],\n",
       "        [ 0.0186, -0.1044,  0.1145, -0.2236,  0.1161],\n",
       "        [ 0.0318, -0.0188,  0.0540, -0.2473, -0.0505],\n",
       "        [ 0.0508, -0.0312,  0.0672, -0.2199,  0.0037],\n",
       "        [ 0.0661, -0.1311, -0.0693, -0.2742,  0.0928],\n",
       "        [ 0.1299, -0.0147, -0.1106, -0.1785,  0.0247],\n",
       "        [-0.0894, -0.0932,  0.0872, -0.1140, -0.0783],\n",
       "        [ 0.0462,  0.0581, -0.0128, -0.2875,  0.1313],\n",
       "        [-0.0070, -0.0749,  0.0804, -0.3238,  0.0787],\n",
       "        [ 0.0339, -0.0240,  0.0244, -0.2182,  0.0014],\n",
       "        [ 0.0378, -0.1635, -0.0187, -0.2843,  0.0632],\n",
       "        [-0.0229, -0.0781,  0.1537, -0.1138,  0.0857],\n",
       "        [ 0.2159, -0.0474,  0.0295, -0.2566,  0.0335],\n",
       "        [-0.0117, -0.0581,  0.1017, -0.4465,  0.0386],\n",
       "        [ 0.0541, -0.1314, -0.0188, -0.2728, -0.0148],\n",
       "        [ 0.0876, -0.0412,  0.0550, -0.2479,  0.0815],\n",
       "        [-0.0487, -0.0380,  0.0998, -0.0934,  0.1095],\n",
       "        [-0.0095, -0.0346,  0.0632, -0.3647,  0.1244],\n",
       "        [-0.0023, -0.0366,  0.0445, -0.2893,  0.1366],\n",
       "        [ 0.0843,  0.0074,  0.0327, -0.2334,  0.0173],\n",
       "        [-0.0443,  0.0550,  0.0046, -0.2446, -0.0307],\n",
       "        [ 0.1550, -0.1265,  0.0181, -0.2044,  0.0805],\n",
       "        [-0.1020, -0.0228, -0.0278, -0.2670,  0.1331],\n",
       "        [ 0.1671, -0.1879, -0.0220, -0.1035,  0.2591],\n",
       "        [-0.1640, -0.0622, -0.0343, -0.1658,  0.0595],\n",
       "        [-0.0375, -0.0233, -0.1088, -0.0851,  0.0373],\n",
       "        [ 0.1098,  0.0330, -0.0107, -0.2462,  0.1068],\n",
       "        [-0.0486, -0.0649,  0.0379, -0.1821,  0.0946]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config(embed_path = r'/home2/rzhao/pretrained_models/glove/standford/glove.6B.50d.pkl',\n",
    "                vocab_path = r'/home2/rzhao/pretrained_models/glove/standford/vocab.pkl')\n",
    "train_set, dev_set, test_set = build_dataset(config)\n",
    "test_iter = DatasetIterater(test_set, config.batch_size, config.device)\n",
    "model = Text_RNN(config).to(config.device)\n",
    "model(next(test_iter)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc6284e",
   "metadata": {},
   "source": [
    "### *CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05e71c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text_CNN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Text_CNN, self).__init__()\n",
    "        if config.embedding_pretrained is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(config.embedding_pretrained, freeze=False, padding_idx=config.n_vocab - 1)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(config.n_vocab, config.embedding_len, padding_idx=config.n_vocab - 1)\n",
    "        self.convs = nn.ModuleList(\n",
    "            [nn.Conv2d(1, config.num_channel, (size, config.embedding_len)) for size in config.kernel_sizes])\n",
    "#         self.conv1 = nn.Conv2d(in_channels=1, out_channels=config.num_channel, kernel_size=(, config.embedding_len))\n",
    "        self.fc = nn.Linear(config.num_channel * len(config.kernel_sizes), config.label_num)\n",
    "        \n",
    "    def conv_and_pool(self, x, conv):\n",
    "        conv_out = F.relu(conv(x)).squeeze(3)\n",
    "        pooled_out = F.max_pool1d(conv_out, conv_out.size(2)).squeeze(2)\n",
    "        return pooled_out\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = out.unsqueeze(1)\n",
    "        out = torch.cat([self.conv_and_pool(out, conv) for conv in self.convs], 1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "522240b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab loaded sucessed, size : 400002 \n",
      "./data_set/test.pkl loaded success, size: 15605\n",
      "./data_set/dev.pkl loaded success, size: 31211\n",
      "./data_set/train.pkl loaded success, size: 109242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.6035e-01,  4.6677e-01,  2.3761e-01,  2.8745e-02,  3.0743e-01],\n",
       "        [ 1.0245e+00,  5.0316e-01,  1.8430e-02, -3.4109e-01,  3.0579e-01],\n",
       "        [ 4.8702e-01,  2.7138e-01,  6.3447e-02, -4.4807e-02,  1.2226e-01],\n",
       "        [ 6.6586e-01,  5.3008e-01,  2.8441e-03, -1.6934e-01,  8.8650e-02],\n",
       "        [ 9.2439e-01,  4.4143e-01, -3.3740e-02, -2.7215e-01,  1.5879e-01],\n",
       "        [ 6.6027e-01,  4.8743e-01,  1.6585e-01, -2.3087e-01,  5.2419e-02],\n",
       "        [ 8.0646e-01,  5.1191e-01,  9.1721e-02, -4.5864e-01, -1.5819e-01],\n",
       "        [ 8.8792e-01,  4.3649e-01, -1.7078e-02, -8.5447e-02,  6.7030e-02],\n",
       "        [ 5.9375e-01,  4.7643e-01, -7.3071e-02, -8.8662e-02,  2.7353e-01],\n",
       "        [ 7.2837e-01,  5.4883e-01,  1.3848e-02, -1.2166e-02,  1.7551e-01],\n",
       "        [ 1.1687e+00,  7.7771e-01, -1.8716e-02, -3.1136e-01,  3.6114e-01],\n",
       "        [ 7.6785e-01,  5.7504e-01,  2.1751e-02, -3.7635e-02,  1.4264e-01],\n",
       "        [ 5.4522e-01,  5.6846e-01,  1.8392e-01, -8.5888e-02,  2.3925e-01],\n",
       "        [ 4.7425e-01,  3.4606e-01,  3.2703e-02, -2.5442e-01,  2.1270e-01],\n",
       "        [ 6.1175e-01,  4.7373e-01,  3.5501e-02, -1.2358e-01,  1.6540e-01],\n",
       "        [ 8.7591e-01,  3.6957e-01,  1.3225e-01, -3.1009e-01, -1.1473e-01],\n",
       "        [ 6.1737e-01,  4.1116e-01,  8.6703e-02, -1.1662e-01,  1.2153e-01],\n",
       "        [ 9.0013e-01,  4.5316e-01,  1.5179e-01, -5.3495e-01,  2.4714e-01],\n",
       "        [ 1.0429e+00,  4.6298e-01,  1.3945e-01, -5.4489e-01,  6.7818e-02],\n",
       "        [ 4.8960e-01,  3.3613e-01,  4.6760e-02,  2.7178e-02,  1.3607e-01],\n",
       "        [ 8.4288e-01,  4.4232e-01, -3.6933e-04, -3.2035e-01,  1.6015e-01],\n",
       "        [ 6.3530e-01,  6.2175e-01,  1.8994e-01, -1.8248e-01,  3.4007e-01],\n",
       "        [ 7.9657e-01,  6.4647e-01,  1.3060e-01, -2.6991e-01,  2.3612e-01],\n",
       "        [ 4.7941e-01,  4.5086e-01, -2.0719e-02, -2.3209e-01, -5.1210e-02],\n",
       "        [ 7.2698e-01,  5.3330e-01,  7.7709e-02, -1.8964e-01,  2.6046e-01],\n",
       "        [ 7.5384e-01,  4.4413e-01, -1.5944e-01, -1.2632e-01,  1.7938e-02],\n",
       "        [ 8.4328e-01,  4.0872e-01,  9.7870e-02, -2.8175e-01,  2.8070e-01],\n",
       "        [ 6.1601e-01,  4.3783e-01,  9.6811e-02, -1.8658e-01, -3.1898e-03],\n",
       "        [ 5.4026e-01,  5.1589e-01,  1.5174e-01, -6.0795e-02,  3.0134e-01],\n",
       "        [ 9.6281e-01,  3.9601e-01,  1.8470e-03, -2.6882e-01,  2.4656e-03],\n",
       "        [ 8.2325e-01,  5.8237e-01,  5.2237e-03, -3.7445e-01,  1.2685e-01],\n",
       "        [ 6.5317e-01,  4.3829e-01,  7.1014e-02, -1.2933e-01,  3.1004e-02]],\n",
       "       device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config(embed_path = r'/home2/rzhao/pretrained_models/glove/standford/glove.6B.50d.pkl',\n",
    "                vocab_path = r'/home2/rzhao/pretrained_models/glove/standford/vocab.pkl')\n",
    "train_set, dev_set, test_set = build_dataset(config)\n",
    "test_iter = DatasetIterater(test_set, config.batch_size, config.device)\n",
    "model = Text_CNN(config).to(config.device)\n",
    "model(next(test_iter)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8ac2c933",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "conv1 = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=(2,5))\n",
    "conv2 = nn.Conv2d(1, 4, (3,5))\n",
    "conv3 = nn.Conv2d(1, 4, (4,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9544dae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 9, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.randn(8, 10, 5)\n",
    "data = data.unsqueeze(1)\n",
    "out = conv(data)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f272cd37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 9, 1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 8, 1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 7, 1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1(data).shape\n",
    "conv2(data).shape\n",
    "conv3(data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8292f267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 24, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([conv1(data), conv2(data), conv3(data)], 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d9f63fbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 9])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 1])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = conv1(data).squeeze(3)\n",
    "out.shape\n",
    "F.max_pool1d(out, out.size(2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4a9f1372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function max_pool1d in module torch.nn.functional:\n",
      "\n",
      "max_pool1d(*args, **kwargs)\n",
      "    Applies a 1D max pooling over an input signal composed of several input\n",
      "    planes.\n",
      "    \n",
      "    See :class:`~torch.nn.MaxPool1d` for details.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(F.max_pool1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034cd47c",
   "metadata": {},
   "source": [
    "## *train and evl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d5fa691",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_time_dif(start_time):\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    return timedelta(seconds=int(round(time_dif)))\n",
    "\n",
    "def init_network(model, method='xavier', exclude='embedding', seed=123):\n",
    "    for name, w in model.named_parameters():\n",
    "        if exclude not in name:\n",
    "            if 'weight' in name:\n",
    "                if method == 'xavier':\n",
    "                    nn.init.xavier_normal_(w)\n",
    "                elif method == 'kaiming':\n",
    "                    nn.init.kaiming_normal_(w)\n",
    "                else:\n",
    "                    nn.init.normal_(w)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(w, 0)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "def train(model, config, train_set, dev_set, test_set):\n",
    "    train_iter = DatasetIterater(train_set, config.batch_size, config.device)\n",
    "    dev_iter = DatasetIterater(dev_set, config.batch_size, config.device)\n",
    "    test_iter = DatasetIterater(test_set, config.batch_size, config.device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = config.learning_rate)\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    total_iter = 0\n",
    "    best_dev_acc = 0\n",
    "    for epoch in range(config.epoch):\n",
    "        print('Epoch [{}/{}]'.format(epoch + 1, config.epoch))\n",
    "        for i, (x, y_true) in enumerate(train_iter):\n",
    "            out = model(x)\n",
    "            model.zero_grad()\n",
    "            loss = F.cross_entropy(out, y_true)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_iter += 1\n",
    "            if total_iter % config.log_step == 0  or i+1 == len(train_iter):\n",
    "                acc_train, loss_train = evalute(model, config, DatasetIterater(train_set, config.batch_size, config.device))\n",
    "                acc_dev, loss_dev = evalute(model, config, dev_iter)\n",
    "                model.train()\n",
    "                improve = \"\"\n",
    "                if acc_dev > best_dev_acc:\n",
    "                    best_dev_acc = acc_dev\n",
    "                    improve = \"*\"\n",
    "                time_dif = get_time_dif(start_time)\n",
    "                msg = 'Iter: {0:>6},  Train Loss: {1:>5.2},  Train Acc: {2:>6.2%},  Val Loss: {3:>5.2},  Val Acc: {4:>6.2%},  Time: {5}{6}'\n",
    "                print(msg.format(total_iter, loss_train, acc_train, loss_dev, acc_dev, time_dif, improve))\n",
    "    acc_test, loss_test = evalute(model, config, test_iter)\n",
    "    msg = 'Test Loss: {0:>5.2},  Test Acc: {1:>6.2%}'\n",
    "    print(msg.format(loss_test, acc_test))\n",
    "\n",
    "def evalute(model, config, data_iter):\n",
    "    model.eval()\n",
    "    loss_total = 0\n",
    "    labels_predict = np.arange(0)\n",
    "    labels_true = np.arange(0)\n",
    "    with torch.no_grad():\n",
    "        for x, y_true in data_iter:\n",
    "            out = model(x)\n",
    "            loss = F.cross_entropy(out, y_true)\n",
    "            loss_total += loss\n",
    "            labels_predict = np.concatenate((labels_predict, out.data.cpu().numpy().argmax(axis=1)), axis=0)\n",
    "            labels_true = np.concatenate((labels_true, y_true.data.cpu().numpy()), axis=0)\n",
    "        try:\n",
    "            assert len(labels_true) == labels_predict.size\n",
    "        except AssertionError as e:\n",
    "            print(len(x))\n",
    "            print(out.shape)\n",
    "            print(\"labels_true:\", len(labels_true))\n",
    "            print(\"labels_predict:\",labels_predict.size)\n",
    "            raise AssertionError\n",
    "    acc = (labels_true == labels_predict).mean()\n",
    "    loss = loss_total.data.cpu().numpy()/len(data_iter)\n",
    "    return acc, loss_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908cc186",
   "metadata": {},
   "source": [
    "## run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5f8d1f",
   "metadata": {},
   "source": [
    "### rnn with random embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d5d270f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab loaded sucessed, size : 14666 \n",
      "./data_set/test.pkl loaded success, size: 15605\n",
      "./data_set/dev.pkl loaded success, size: 31211\n",
      "./data_set/train.pkl loaded success, size: 109242\n",
      "Epoch [1/20]\n",
      "Iter:   1000,  Train Loss: 4.4e+03,  Train Acc: 51.00%,  Val Loss: 1.2e+03,  Val Acc: 50.82%,  Time: 0:00:40*\n",
      "Iter:   2000,  Train Loss: 4.3e+03,  Train Acc: 51.18%,  Val Loss: 1.2e+03,  Val Acc: 51.09%,  Time: 0:01:20*\n",
      "Iter:   3000,  Train Loss: 4.3e+03,  Train Acc: 51.02%,  Val Loss: 1.2e+03,  Val Acc: 50.85%,  Time: 0:02:00\n",
      "Iter:   3414,  Train Loss: 4.3e+03,  Train Acc: 51.25%,  Val Loss: 1.2e+03,  Val Acc: 51.18%,  Time: 0:02:31*\n",
      "Epoch [2/20]\n",
      "Iter:   4000,  Train Loss: 4.3e+03,  Train Acc: 51.39%,  Val Loss: 1.2e+03,  Val Acc: 51.27%,  Time: 0:03:05*\n",
      "Iter:   5000,  Train Loss: 4.3e+03,  Train Acc: 51.39%,  Val Loss: 1.2e+03,  Val Acc: 51.19%,  Time: 0:03:44\n",
      "Iter:   6000,  Train Loss: 4.3e+03,  Train Acc: 51.09%,  Val Loss: 1.2e+03,  Val Acc: 51.01%,  Time: 0:04:24\n",
      "Iter:   6828,  Train Loss: 4.3e+03,  Train Acc: 51.38%,  Val Loss: 1.2e+03,  Val Acc: 51.30%,  Time: 0:05:01*\n",
      "Epoch [3/20]\n",
      "Iter:   7000,  Train Loss: 4.3e+03,  Train Acc: 51.35%,  Val Loss: 1.2e+03,  Val Acc: 51.30%,  Time: 0:05:29*\n",
      "Iter:   8000,  Train Loss: 4.3e+03,  Train Acc: 51.34%,  Val Loss: 1.2e+03,  Val Acc: 51.22%,  Time: 0:06:09\n",
      "Iter:   9000,  Train Loss: 4.2e+03,  Train Acc: 51.52%,  Val Loss: 1.2e+03,  Val Acc: 51.35%,  Time: 0:06:48*\n",
      "Iter:  10000,  Train Loss: 4.2e+03,  Train Acc: 51.48%,  Val Loss: 1.2e+03,  Val Acc: 51.29%,  Time: 0:07:28\n",
      "Iter:  10242,  Train Loss: 4.2e+03,  Train Acc: 51.48%,  Val Loss: 1.2e+03,  Val Acc: 51.40%,  Time: 0:07:57*\n",
      "Epoch [4/20]\n",
      "Iter:  11000,  Train Loss: 4.2e+03,  Train Acc: 51.53%,  Val Loss: 1.2e+03,  Val Acc: 51.27%,  Time: 0:08:33\n",
      "Iter:  12000,  Train Loss: 4.2e+03,  Train Acc: 51.51%,  Val Loss: 1.2e+03,  Val Acc: 51.40%,  Time: 0:09:13\n",
      "Iter:  13000,  Train Loss: 4.2e+03,  Train Acc: 51.63%,  Val Loss: 1.2e+03,  Val Acc: 51.42%,  Time: 0:09:52*\n",
      "Iter:  13656,  Train Loss: 4.2e+03,  Train Acc: 51.65%,  Val Loss: 1.2e+03,  Val Acc: 51.54%,  Time: 0:10:27*\n",
      "Epoch [5/20]\n",
      "Iter:  14000,  Train Loss: 4.2e+03,  Train Acc: 51.69%,  Val Loss: 1.2e+03,  Val Acc: 51.47%,  Time: 0:10:57\n",
      "Iter:  15000,  Train Loss: 4.2e+03,  Train Acc: 51.82%,  Val Loss: 1.2e+03,  Val Acc: 51.52%,  Time: 0:11:37\n",
      "Iter:  16000,  Train Loss: 4.2e+03,  Train Acc: 51.83%,  Val Loss: 1.2e+03,  Val Acc: 51.55%,  Time: 0:12:17*\n",
      "Iter:  17000,  Train Loss: 4.2e+03,  Train Acc: 51.83%,  Val Loss: 1.2e+03,  Val Acc: 51.59%,  Time: 0:12:57*\n",
      "Iter:  17070,  Train Loss: 4.2e+03,  Train Acc: 51.94%,  Val Loss: 1.2e+03,  Val Acc: 51.65%,  Time: 0:13:23*\n",
      "Epoch [6/20]\n",
      "Iter:  18000,  Train Loss: 4.2e+03,  Train Acc: 51.98%,  Val Loss: 1.2e+03,  Val Acc: 51.76%,  Time: 0:14:02*\n",
      "Iter:  19000,  Train Loss: 4.2e+03,  Train Acc: 52.02%,  Val Loss: 1.2e+03,  Val Acc: 51.86%,  Time: 0:14:42*\n",
      "Iter:  20000,  Train Loss: 4.2e+03,  Train Acc: 52.03%,  Val Loss: 1.2e+03,  Val Acc: 51.77%,  Time: 0:15:21\n",
      "Iter:  20484,  Train Loss: 4.2e+03,  Train Acc: 52.07%,  Val Loss: 1.2e+03,  Val Acc: 51.77%,  Time: 0:15:54\n",
      "Epoch [7/20]\n",
      "Iter:  21000,  Train Loss: 4.3e+03,  Train Acc: 49.54%,  Val Loss: 1.2e+03,  Val Acc: 49.58%,  Time: 0:16:26\n",
      "Iter:  22000,  Train Loss: 4.1e+03,  Train Acc: 52.17%,  Val Loss: 1.2e+03,  Val Acc: 51.87%,  Time: 0:17:06*\n",
      "Iter:  23000,  Train Loss: 4.2e+03,  Train Acc: 52.02%,  Val Loss: 1.2e+03,  Val Acc: 51.68%,  Time: 0:17:46\n",
      "Iter:  23898,  Train Loss: 4.2e+03,  Train Acc: 52.21%,  Val Loss: 1.2e+03,  Val Acc: 51.90%,  Time: 0:18:24*\n",
      "Epoch [8/20]\n",
      "Iter:  24000,  Train Loss: 4.1e+03,  Train Acc: 52.08%,  Val Loss: 1.2e+03,  Val Acc: 51.77%,  Time: 0:18:51\n",
      "Iter:  25000,  Train Loss: 4.2e+03,  Train Acc: 52.25%,  Val Loss: 1.2e+03,  Val Acc: 52.02%,  Time: 0:19:31*\n",
      "Iter:  26000,  Train Loss: 4.1e+03,  Train Acc: 52.44%,  Val Loss: 1.2e+03,  Val Acc: 52.23%,  Time: 0:20:10*\n",
      "Iter:  27000,  Train Loss: 4.1e+03,  Train Acc: 52.48%,  Val Loss: 1.2e+03,  Val Acc: 52.21%,  Time: 0:20:50\n",
      "Iter:  27312,  Train Loss: 4.1e+03,  Train Acc: 52.38%,  Val Loss: 1.2e+03,  Val Acc: 52.23%,  Time: 0:21:20\n",
      "Epoch [9/20]\n",
      "Iter:  28000,  Train Loss: 4.1e+03,  Train Acc: 52.43%,  Val Loss: 1.2e+03,  Val Acc: 52.20%,  Time: 0:21:55\n",
      "Iter:  29000,  Train Loss: 4.1e+03,  Train Acc: 52.24%,  Val Loss: 1.2e+03,  Val Acc: 52.01%,  Time: 0:22:35\n",
      "Iter:  30000,  Train Loss: 4.1e+03,  Train Acc: 52.41%,  Val Loss: 1.2e+03,  Val Acc: 52.09%,  Time: 0:23:14\n",
      "Iter:  30726,  Train Loss: 4.1e+03,  Train Acc: 52.59%,  Val Loss: 1.2e+03,  Val Acc: 52.35%,  Time: 0:23:50*\n",
      "Epoch [10/20]\n",
      "Iter:  31000,  Train Loss: 4.1e+03,  Train Acc: 52.74%,  Val Loss: 1.2e+03,  Val Acc: 52.38%,  Time: 0:24:19*\n",
      "Iter:  32000,  Train Loss: 4.1e+03,  Train Acc: 52.81%,  Val Loss: 1.2e+03,  Val Acc: 52.52%,  Time: 0:24:59*\n",
      "Iter:  33000,  Train Loss: 4.1e+03,  Train Acc: 52.76%,  Val Loss: 1.2e+03,  Val Acc: 52.61%,  Time: 0:25:39*\n",
      "Iter:  34000,  Train Loss: 4.1e+03,  Train Acc: 52.64%,  Val Loss: 1.2e+03,  Val Acc: 52.44%,  Time: 0:26:18\n",
      "Iter:  34140,  Train Loss: 4.1e+03,  Train Acc: 52.83%,  Val Loss: 1.2e+03,  Val Acc: 52.55%,  Time: 0:26:46\n",
      "Epoch [11/20]\n",
      "Iter:  35000,  Train Loss: 4.1e+03,  Train Acc: 52.18%,  Val Loss: 1.2e+03,  Val Acc: 51.86%,  Time: 0:27:23\n",
      "Iter:  36000,  Train Loss: 4.1e+03,  Train Acc: 53.03%,  Val Loss: 1.2e+03,  Val Acc: 52.74%,  Time: 0:28:03*\n",
      "Iter:  37000,  Train Loss: 4.1e+03,  Train Acc: 53.05%,  Val Loss: 1.2e+03,  Val Acc: 52.76%,  Time: 0:28:43*\n",
      "Iter:  37554,  Train Loss: 4.1e+03,  Train Acc: 53.03%,  Val Loss: 1.2e+03,  Val Acc: 52.74%,  Time: 0:29:18\n",
      "Epoch [12/20]\n",
      "Iter:  38000,  Train Loss: 4.1e+03,  Train Acc: 53.17%,  Val Loss: 1.2e+03,  Val Acc: 52.89%,  Time: 0:29:50*\n",
      "Iter:  39000,  Train Loss: 4e+03,  Train Acc: 53.19%,  Val Loss: 1.2e+03,  Val Acc: 52.96%,  Time: 0:30:30*\n",
      "Iter:  40000,  Train Loss: 4e+03,  Train Acc: 53.29%,  Val Loss: 1.2e+03,  Val Acc: 52.97%,  Time: 0:31:09*\n",
      "Iter:  40968,  Train Loss: 4.1e+03,  Train Acc: 53.11%,  Val Loss: 1.2e+03,  Val Acc: 52.76%,  Time: 0:31:49\n",
      "Epoch [13/20]\n",
      "Iter:  41000,  Train Loss: 4e+03,  Train Acc: 53.29%,  Val Loss: 1.2e+03,  Val Acc: 53.04%,  Time: 0:32:14*\n",
      "Iter:  42000,  Train Loss: 4e+03,  Train Acc: 53.35%,  Val Loss: 1.2e+03,  Val Acc: 52.96%,  Time: 0:32:54\n",
      "Iter:  43000,  Train Loss: 4e+03,  Train Acc: 53.14%,  Val Loss: 1.2e+03,  Val Acc: 52.83%,  Time: 0:33:34\n",
      "Iter:  44000,  Train Loss: 4e+03,  Train Acc: 53.24%,  Val Loss: 1.2e+03,  Val Acc: 52.94%,  Time: 0:34:14\n",
      "Iter:  44382,  Train Loss: 4e+03,  Train Acc: 53.42%,  Val Loss: 1.2e+03,  Val Acc: 53.01%,  Time: 0:34:45\n",
      "Epoch [14/20]\n",
      "Iter:  45000,  Train Loss: 4e+03,  Train Acc: 53.38%,  Val Loss: 1.2e+03,  Val Acc: 53.10%,  Time: 0:35:20*\n",
      "Iter:  46000,  Train Loss: 4e+03,  Train Acc: 53.59%,  Val Loss: 1.2e+03,  Val Acc: 53.28%,  Time: 0:36:01*\n",
      "Iter:  47000,  Train Loss: 4e+03,  Train Acc: 53.48%,  Val Loss: 1.2e+03,  Val Acc: 53.27%,  Time: 0:36:43\n",
      "Iter:  47796,  Train Loss: 4e+03,  Train Acc: 53.43%,  Val Loss: 1.2e+03,  Val Acc: 52.96%,  Time: 0:37:21\n",
      "Epoch [15/20]\n",
      "Iter:  48000,  Train Loss: 4e+03,  Train Acc: 53.48%,  Val Loss: 1.2e+03,  Val Acc: 53.20%,  Time: 0:37:50\n",
      "Iter:  49000,  Train Loss: 4e+03,  Train Acc: 53.69%,  Val Loss: 1.1e+03,  Val Acc: 53.16%,  Time: 0:38:29\n",
      "Iter:  50000,  Train Loss: 4e+03,  Train Acc: 53.87%,  Val Loss: 1.1e+03,  Val Acc: 53.57%,  Time: 0:39:09*\n",
      "Iter:  51000,  Train Loss: 4e+03,  Train Acc: 53.81%,  Val Loss: 1.1e+03,  Val Acc: 53.44%,  Time: 0:39:49\n",
      "Iter:  51210,  Train Loss: 4e+03,  Train Acc: 53.75%,  Val Loss: 1.2e+03,  Val Acc: 53.23%,  Time: 0:40:17\n",
      "Epoch [16/20]\n",
      "Iter:  52000,  Train Loss: 4e+03,  Train Acc: 53.82%,  Val Loss: 1.1e+03,  Val Acc: 53.40%,  Time: 0:40:54\n",
      "Iter:  53000,  Train Loss: 4e+03,  Train Acc: 54.02%,  Val Loss: 1.1e+03,  Val Acc: 53.53%,  Time: 0:41:34\n",
      "Iter:  54000,  Train Loss: 4e+03,  Train Acc: 54.04%,  Val Loss: 1.1e+03,  Val Acc: 53.53%,  Time: 0:42:13\n",
      "Iter:  54624,  Train Loss: 4e+03,  Train Acc: 53.93%,  Val Loss: 1.1e+03,  Val Acc: 53.46%,  Time: 0:42:48\n",
      "Epoch [17/20]\n",
      "Iter:  55000,  Train Loss: 4e+03,  Train Acc: 54.03%,  Val Loss: 1.1e+03,  Val Acc: 53.56%,  Time: 0:43:18\n",
      "Iter:  56000,  Train Loss: 4.1e+03,  Train Acc: 52.50%,  Val Loss: 1.2e+03,  Val Acc: 51.67%,  Time: 0:43:59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  57000,  Train Loss: 3.9e+03,  Train Acc: 54.18%,  Val Loss: 1.1e+03,  Val Acc: 53.59%,  Time: 0:44:41*\n",
      "Iter:  58000,  Train Loss: 3.9e+03,  Train Acc: 54.29%,  Val Loss: 1.1e+03,  Val Acc: 53.81%,  Time: 0:45:22*\n",
      "Iter:  58038,  Train Loss: 4e+03,  Train Acc: 54.21%,  Val Loss: 1.1e+03,  Val Acc: 53.61%,  Time: 0:45:47\n",
      "Epoch [18/20]\n",
      "Iter:  59000,  Train Loss: 3.9e+03,  Train Acc: 54.41%,  Val Loss: 1.1e+03,  Val Acc: 53.91%,  Time: 0:46:27*\n",
      "Iter:  60000,  Train Loss: 3.9e+03,  Train Acc: 54.54%,  Val Loss: 1.1e+03,  Val Acc: 53.92%,  Time: 0:47:06*\n",
      "Iter:  61000,  Train Loss: 3.9e+03,  Train Acc: 53.98%,  Val Loss: 1.1e+03,  Val Acc: 53.18%,  Time: 0:47:46\n",
      "Iter:  61452,  Train Loss: 3.9e+03,  Train Acc: 54.47%,  Val Loss: 1.1e+03,  Val Acc: 53.84%,  Time: 0:48:18\n",
      "Epoch [19/20]\n",
      "Iter:  62000,  Train Loss: 3.9e+03,  Train Acc: 54.55%,  Val Loss: 1.1e+03,  Val Acc: 53.97%,  Time: 0:48:51*\n",
      "Iter:  63000,  Train Loss: 3.9e+03,  Train Acc: 54.69%,  Val Loss: 1.1e+03,  Val Acc: 54.04%,  Time: 0:49:31*\n",
      "Iter:  64000,  Train Loss: 3.9e+03,  Train Acc: 54.80%,  Val Loss: 1.1e+03,  Val Acc: 53.85%,  Time: 0:50:10\n",
      "Iter:  64866,  Train Loss: 3.9e+03,  Train Acc: 54.76%,  Val Loss: 1.1e+03,  Val Acc: 54.05%,  Time: 0:50:48*\n",
      "Epoch [20/20]\n",
      "Iter:  65000,  Train Loss: 3.9e+03,  Train Acc: 54.77%,  Val Loss: 1.1e+03,  Val Acc: 54.19%,  Time: 0:51:15*\n",
      "Iter:  66000,  Train Loss: 3.9e+03,  Train Acc: 54.43%,  Val Loss: 1.1e+03,  Val Acc: 53.94%,  Time: 0:51:55\n",
      "Iter:  67000,  Train Loss: 3.9e+03,  Train Acc: 54.91%,  Val Loss: 1.1e+03,  Val Acc: 54.30%,  Time: 0:52:35*\n",
      "Iter:  68000,  Train Loss: 3.9e+03,  Train Acc: 54.85%,  Val Loss: 1.1e+03,  Val Acc: 54.07%,  Time: 0:53:15\n",
      "Iter:  68280,  Train Loss: 3.9e+03,  Train Acc: 54.93%,  Val Loss: 1.1e+03,  Val Acc: 54.14%,  Time: 0:53:45\n",
      "Train Loss: 5.7e+02,  Train Acc: 53.87%\n"
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "train_set, dev_set, test_set = build_dataset(config)\n",
    "### customized hyper param\n",
    "config.learning_rate = 1e-5 # lr = 1e-3 导致模型震荡，acc上不去\n",
    "config.log_step = 1000\n",
    "config.drop_out = 0.3\n",
    "###\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed_all(1)\n",
    "torch.backends.cudnn.deterministic = True  # 保证每次结果一样\n",
    "\n",
    "model = Text_RNN(config).to(config.device)\n",
    "init_network(model)\n",
    "\n",
    "train(model, config, train_set, dev_set, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed778e39",
   "metadata": {},
   "source": [
    "### rnn with glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeed5acb",
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "config = Config(embed_path = r'/home2/rzhao/pretrained_models/glove/standford/glove.6B.50d.pkl',\n",
    "                vocab_path = r'/home2/rzhao/pretrained_models/glove/standford/vocab.pkl')\n",
    "train_set, dev_set, test_set = build_dataset(config)\n",
    "### customized hyper param\n",
    "config.batch_size = 32\n",
    "config.learning_rate = 1e-5\n",
    "config.log_step = 1000\n",
    "config.drop_out = 0.3\n",
    "###\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed_all(1)\n",
    "torch.backends.cudnn.deterministic = True  # 保证每次结果一样\n",
    "\n",
    "model = Text_RNN(config).to(config.device)\n",
    "init_network(model)\n",
    "train(model, config, train_set, dev_set, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fec4609",
   "metadata": {},
   "source": [
    "### rnn with word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "122f94a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab loaded sucessed, size : 3000000 \n",
      "./data_set/test.pkl loaded success, size: 15605\n",
      "./data_set/dev.pkl loaded success, size: 31211\n",
      "./data_set/train.pkl loaded success, size: 109242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ff54ecfe710>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20]\n",
      "Iter:   1000,  Train Loss: 5.4e+03,  Train Acc: 51.00%,  Val Loss: 1.6e+03,  Val Acc: 50.82%,  Time: 0:03:20*\n",
      "Iter:   2000,  Train Loss: 4.9e+03,  Train Acc: 17.52%,  Val Loss: 1.4e+03,  Val Acc: 17.67%,  Time: 0:06:38\n",
      "Iter:   3000,  Train Loss: 4.9e+03,  Train Acc: 51.00%,  Val Loss: 1.4e+03,  Val Acc: 50.82%,  Time: 0:09:56\n",
      "Iter:   3414,  Train Loss: 6.1e+03,  Train Acc: 21.11%,  Val Loss: 1.8e+03,  Val Acc: 20.97%,  Time: 0:11:33\n",
      "Epoch [2/20]\n",
      "Iter:   4000,  Train Loss: 5.1e+03,  Train Acc: 51.00%,  Val Loss: 1.5e+03,  Val Acc: 50.82%,  Time: 0:13:41\n",
      "Iter:   5000,  Train Loss: 4.6e+03,  Train Acc: 51.00%,  Val Loss: 1.3e+03,  Val Acc: 50.82%,  Time: 0:16:57\n",
      "Iter:   6000,  Train Loss: 5.5e+03,  Train Acc: 21.11%,  Val Loss: 1.6e+03,  Val Acc: 20.97%,  Time: 0:20:15\n",
      "Iter:   6828,  Train Loss: 6.1e+03,  Train Acc: 21.11%,  Val Loss: 1.8e+03,  Val Acc: 20.97%,  Time: 0:23:03\n",
      "Epoch [3/20]\n",
      "Iter:   7000,  Train Loss: 5e+03,  Train Acc: 51.00%,  Val Loss: 1.4e+03,  Val Acc: 50.82%,  Time: 0:24:00\n",
      "Iter:   8000,  Train Loss: 4.9e+03,  Train Acc: 21.11%,  Val Loss: 1.4e+03,  Val Acc: 20.97%,  Time: 0:27:17\n",
      "Iter:   9000,  Train Loss: 4.5e+03,  Train Acc: 51.00%,  Val Loss: 1.3e+03,  Val Acc: 50.82%,  Time: 0:30:34\n",
      "Iter:  10000,  Train Loss: 6e+03,  Train Acc: 51.00%,  Val Loss: 1.7e+03,  Val Acc: 50.82%,  Time: 0:33:51\n",
      "Iter:  10242,  Train Loss: 6.1e+03,  Train Acc: 21.11%,  Val Loss: 1.8e+03,  Val Acc: 20.97%,  Time: 0:35:00\n",
      "Epoch [4/20]\n",
      "Iter:  11000,  Train Loss: 5.6e+03,  Train Acc: 17.52%,  Val Loss: 1.6e+03,  Val Acc: 17.67%,  Time: 0:37:36\n",
      "Iter:  12000,  Train Loss: 5.2e+03,  Train Acc: 51.00%,  Val Loss: 1.5e+03,  Val Acc: 50.82%,  Time: 0:40:54\n",
      "Iter:  13000,  Train Loss: 4.6e+03,  Train Acc: 51.00%,  Val Loss: 1.3e+03,  Val Acc: 50.82%,  Time: 0:44:11\n",
      "Iter:  13656,  Train Loss: 6.1e+03,  Train Acc: 21.11%,  Val Loss: 1.8e+03,  Val Acc: 20.97%,  Time: 0:46:31\n",
      "Epoch [5/20]\n",
      "Iter:  14000,  Train Loss: 5.4e+03,  Train Acc: 51.00%,  Val Loss: 1.6e+03,  Val Acc: 50.82%,  Time: 0:47:57\n",
      "Iter:  15000,  Train Loss: 4.8e+03,  Train Acc: 51.00%,  Val Loss: 1.4e+03,  Val Acc: 50.82%,  Time: 0:51:15\n",
      "Iter:  16000,  Train Loss: 4.8e+03,  Train Acc: 51.00%,  Val Loss: 1.4e+03,  Val Acc: 50.82%,  Time: 0:54:32\n",
      "Iter:  17000,  Train Loss: 5.3e+03,  Train Acc: 51.00%,  Val Loss: 1.5e+03,  Val Acc: 50.82%,  Time: 0:57:50\n",
      "Iter:  17070,  Train Loss: 6.1e+03,  Train Acc: 21.11%,  Val Loss: 1.8e+03,  Val Acc: 20.97%,  Time: 0:58:30\n",
      "Epoch [6/20]\n",
      "Iter:  18000,  Train Loss: 5.3e+03,  Train Acc: 51.00%,  Val Loss: 1.5e+03,  Val Acc: 50.82%,  Time: 1:01:36\n",
      "Iter:  19000,  Train Loss: 5e+03,  Train Acc: 51.00%,  Val Loss: 1.4e+03,  Val Acc: 50.82%,  Time: 1:04:53\n",
      "Iter:  20000,  Train Loss: 5.2e+03,  Train Acc: 51.00%,  Val Loss: 1.5e+03,  Val Acc: 50.82%,  Time: 1:08:10\n",
      "Iter:  20484,  Train Loss: 6.1e+03,  Train Acc: 21.11%,  Val Loss: 1.8e+03,  Val Acc: 20.97%,  Time: 1:10:00\n",
      "Epoch [7/20]\n",
      "Iter:  21000,  Train Loss: 5.1e+03,  Train Acc: 21.11%,  Val Loss: 1.5e+03,  Val Acc: 20.97%,  Time: 1:11:55\n",
      "Iter:  22000,  Train Loss: 5.9e+03,  Train Acc: 51.00%,  Val Loss: 1.7e+03,  Val Acc: 50.82%,  Time: 1:15:12\n",
      "Iter:  23000,  Train Loss: 5.4e+03,  Train Acc:  5.84%,  Val Loss: 1.5e+03,  Val Acc:  5.83%,  Time: 1:18:29\n",
      "Iter:  23898,  Train Loss: 6.1e+03,  Train Acc: 21.11%,  Val Loss: 1.8e+03,  Val Acc: 20.97%,  Time: 1:21:29\n",
      "Epoch [8/20]\n",
      "Iter:  24000,  Train Loss: 5.9e+03,  Train Acc: 17.52%,  Val Loss: 1.7e+03,  Val Acc: 17.67%,  Time: 1:22:15\n",
      "Iter:  25000,  Train Loss: 5.3e+03,  Train Acc: 51.00%,  Val Loss: 1.5e+03,  Val Acc: 50.82%,  Time: 1:25:32\n",
      "Iter:  26000,  Train Loss: 5.7e+03,  Train Acc: 51.00%,  Val Loss: 1.6e+03,  Val Acc: 50.82%,  Time: 1:28:50\n",
      "Iter:  27000,  Train Loss: 5.5e+03,  Train Acc: 51.00%,  Val Loss: 1.6e+03,  Val Acc: 50.82%,  Time: 1:32:07\n",
      "Iter:  27312,  Train Loss: 6.1e+03,  Train Acc: 21.11%,  Val Loss: 1.8e+03,  Val Acc: 20.97%,  Time: 1:33:28\n",
      "Epoch [9/20]\n",
      "Iter:  28000,  Train Loss: 5e+03,  Train Acc: 51.00%,  Val Loss: 1.4e+03,  Val Acc: 50.82%,  Time: 1:35:53\n",
      "Iter:  29000,  Train Loss: 4.7e+03,  Train Acc: 51.00%,  Val Loss: 1.3e+03,  Val Acc: 50.82%,  Time: 1:39:10\n",
      "Iter:  30000,  Train Loss: 4.7e+03,  Train Acc: 21.11%,  Val Loss: 1.4e+03,  Val Acc: 20.97%,  Time: 1:42:28\n",
      "Iter:  30726,  Train Loss: 6.1e+03,  Train Acc: 21.11%,  Val Loss: 1.8e+03,  Val Acc: 20.97%,  Time: 1:44:59\n",
      "Epoch [10/20]\n",
      "Iter:  31000,  Train Loss: 4.5e+03,  Train Acc: 51.00%,  Val Loss: 1.3e+03,  Val Acc: 50.82%,  Time: 1:46:14\n",
      "Iter:  32000,  Train Loss: 5.5e+03,  Train Acc: 51.00%,  Val Loss: 1.6e+03,  Val Acc: 50.82%,  Time: 1:49:31\n",
      "Iter:  33000,  Train Loss: 4.9e+03,  Train Acc: 51.00%,  Val Loss: 1.4e+03,  Val Acc: 50.82%,  Time: 1:52:47\n",
      "Iter:  34000,  Train Loss: 5.8e+03,  Train Acc: 21.11%,  Val Loss: 1.7e+03,  Val Acc: 20.97%,  Time: 1:56:04\n",
      "Iter:  34140,  Train Loss: 6.1e+03,  Train Acc: 21.11%,  Val Loss: 1.8e+03,  Val Acc: 20.97%,  Time: 1:56:56\n",
      "Epoch [11/20]\n",
      "Iter:  35000,  Train Loss: 4.6e+03,  Train Acc: 51.00%,  Val Loss: 1.3e+03,  Val Acc: 50.82%,  Time: 1:59:50\n",
      "Iter:  36000,  Train Loss: 5.2e+03,  Train Acc: 17.52%,  Val Loss: 1.5e+03,  Val Acc: 17.67%,  Time: 2:03:07\n",
      "Iter:  37000,  Train Loss: 4.8e+03,  Train Acc: 51.00%,  Val Loss: 1.4e+03,  Val Acc: 50.82%,  Time: 2:06:24\n",
      "Iter:  37554,  Train Loss: 6.1e+03,  Train Acc: 21.11%,  Val Loss: 1.8e+03,  Val Acc: 20.97%,  Time: 2:08:26\n",
      "Epoch [12/20]\n",
      "Iter:  38000,  Train Loss: 4.7e+03,  Train Acc: 51.00%,  Val Loss: 1.4e+03,  Val Acc: 50.82%,  Time: 2:10:09\n",
      "Iter:  39000,  Train Loss: 5e+03,  Train Acc: 51.00%,  Val Loss: 1.4e+03,  Val Acc: 50.82%,  Time: 2:13:26\n",
      "Iter:  40000,  Train Loss: 5.2e+03,  Train Acc: 51.00%,  Val Loss: 1.5e+03,  Val Acc: 50.82%,  Time: 2:16:44\n",
      "Iter:  40968,  Train Loss: 6.1e+03,  Train Acc: 21.11%,  Val Loss: 1.8e+03,  Val Acc: 20.97%,  Time: 2:19:55\n",
      "Epoch [13/20]\n",
      "Iter:  41000,  Train Loss: 5.3e+03,  Train Acc: 51.00%,  Val Loss: 1.5e+03,  Val Acc: 50.82%,  Time: 2:20:28\n",
      "Iter:  42000,  Train Loss: 4.5e+03,  Train Acc: 51.00%,  Val Loss: 1.3e+03,  Val Acc: 50.82%,  Time: 2:23:45\n",
      "Iter:  43000,  Train Loss: 5.2e+03,  Train Acc: 51.00%,  Val Loss: 1.5e+03,  Val Acc: 50.82%,  Time: 2:27:03\n",
      "Iter:  44000,  Train Loss: 6e+03,  Train Acc: 51.00%,  Val Loss: 1.7e+03,  Val Acc: 50.82%,  Time: 2:30:19\n",
      "Iter:  44382,  Train Loss: 6.1e+03,  Train Acc: 21.11%,  Val Loss: 1.8e+03,  Val Acc: 20.97%,  Time: 2:31:52\n",
      "Epoch [14/20]\n",
      "Iter:  45000,  Train Loss: 6.2e+03,  Train Acc: 21.11%,  Val Loss: 1.8e+03,  Val Acc: 20.97%,  Time: 2:34:05\n",
      "Iter:  46000,  Train Loss: 7.1e+03,  Train Acc: 51.00%,  Val Loss: 2e+03,  Val Acc: 50.82%,  Time: 2:37:23\n",
      "Iter:  47000,  Train Loss: 5e+03,  Train Acc: 51.00%,  Val Loss: 1.4e+03,  Val Acc: 50.82%,  Time: 2:40:40\n",
      "Iter:  47796,  Train Loss: 6.1e+03,  Train Acc: 21.11%,  Val Loss: 1.8e+03,  Val Acc: 20.97%,  Time: 2:43:23\n",
      "Epoch [15/20]\n",
      "Iter:  49000,  Train Loss: 4.6e+03,  Train Acc: 51.00%,  Val Loss: 1.3e+03,  Val Acc: 50.82%,  Time: 2:47:43\n",
      "Iter:  50000,  Train Loss: 6.5e+03,  Train Acc: 21.11%,  Val Loss: 1.9e+03,  Val Acc: 20.97%,  Time: 2:51:00\n",
      "Iter:  51000,  Train Loss: 4.8e+03,  Train Acc: 21.11%,  Val Loss: 1.4e+03,  Val Acc: 20.97%,  Time: 2:54:18\n",
      "Iter:  51210,  Train Loss: 6.1e+03,  Train Acc: 21.11%,  Val Loss: 1.8e+03,  Val Acc: 20.97%,  Time: 2:55:22\n",
      "Epoch [16/20]\n",
      "Iter:  52000,  Train Loss: 5e+03,  Train Acc: 51.00%,  Val Loss: 1.4e+03,  Val Acc: 50.82%,  Time: 2:58:05\n",
      "Iter:  53000,  Train Loss: 6e+03,  Train Acc: 51.00%,  Val Loss: 1.7e+03,  Val Acc: 50.82%,  Time: 3:01:22\n",
      "Iter:  54000,  Train Loss: 5e+03,  Train Acc: 21.11%,  Val Loss: 1.4e+03,  Val Acc: 20.97%,  Time: 3:04:41\n",
      "Iter:  54624,  Train Loss: 6.1e+03,  Train Acc: 21.11%,  Val Loss: 1.8e+03,  Val Acc: 20.97%,  Time: 3:06:55\n",
      "Epoch [17/20]\n",
      "Iter:  55000,  Train Loss: 4.7e+03,  Train Acc: 51.00%,  Val Loss: 1.3e+03,  Val Acc: 50.82%,  Time: 3:08:28\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_40519/2242440174.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mText_RNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0minit_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_40519/2698871057.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, config, train_set, dev_set, test_set)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch [{}/{}]'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_40519/1838749181.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mraw_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_40519/1838749181.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(self, raw_batch)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "config = Config(embed_path = r'/home2/rzhao/pretrained_models/word2vec/GoogleNews/word2vec-google-news-300.pkl',\n",
    "                vocab_path = r'/home2/rzhao/pretrained_models/word2vec/GoogleNews/vocab.pkl')\n",
    "train_set, dev_set, test_set = build_dataset(config)\n",
    "### customized hyper param\n",
    "config.batch_size = 32\n",
    "config.learning_rate = 3e-2\n",
    "config.log_step = 1000\n",
    "config.drop_out = 0.3\n",
    "###\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed_all(1)\n",
    "torch.backends.cudnn.deterministic = True  # 保证每次结果一样\n",
    "\n",
    "model = Text_RNN(config).to(config.device)\n",
    "init_network(model)\n",
    "train(model, config, train_set, dev_set, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1feca66",
   "metadata": {},
   "source": [
    "### cnn with random embeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c919822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab loaded sucessed, size : 14666 \n",
      "./data_set/test.pkl loaded success, size: 15605\n",
      "./data_set/dev.pkl loaded success, size: 31211\n",
      "./data_set/train.pkl loaded success, size: 109242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1f7562cc618>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20]\n",
      "Iter:    500,  Train Loss: 4.7e+03,  Train Acc: 50.30%,  Val Loss: 1.4e+03,  Val Acc: 50.03%,  Time: 0:00:41*\n",
      "Iter:   1000,  Train Loss: 4.5e+03,  Train Acc: 52.71%,  Val Loss: 1.3e+03,  Val Acc: 52.29%,  Time: 0:01:22*\n",
      "Iter:   1500,  Train Loss: 4.3e+03,  Train Acc: 53.64%,  Val Loss: 1.2e+03,  Val Acc: 53.34%,  Time: 0:02:04*\n",
      "Iter:   2000,  Train Loss: 4.2e+03,  Train Acc: 54.52%,  Val Loss: 1.2e+03,  Val Acc: 54.23%,  Time: 0:02:46*\n",
      "Iter:   2500,  Train Loss: 4.1e+03,  Train Acc: 54.91%,  Val Loss: 1.2e+03,  Val Acc: 54.46%,  Time: 0:03:27*\n",
      "Iter:   3000,  Train Loss: 4e+03,  Train Acc: 55.98%,  Val Loss: 1.2e+03,  Val Acc: 55.54%,  Time: 0:04:09*\n",
      "Iter:   3414,  Train Loss: 4e+03,  Train Acc: 56.55%,  Val Loss: 1.2e+03,  Val Acc: 55.99%,  Time: 0:04:48*\n",
      "Epoch [2/20]\n",
      "Iter:   3500,  Train Loss: 4e+03,  Train Acc: 56.27%,  Val Loss: 1.1e+03,  Val Acc: 55.71%,  Time: 0:05:16\n",
      "Iter:   4000,  Train Loss: 3.9e+03,  Train Acc: 57.24%,  Val Loss: 1.1e+03,  Val Acc: 56.53%,  Time: 0:05:57*\n",
      "Iter:   4500,  Train Loss: 3.9e+03,  Train Acc: 57.37%,  Val Loss: 1.1e+03,  Val Acc: 56.46%,  Time: 0:06:39\n",
      "Iter:   5000,  Train Loss: 3.8e+03,  Train Acc: 58.40%,  Val Loss: 1.1e+03,  Val Acc: 57.49%,  Time: 0:07:20*\n",
      "Iter:   5500,  Train Loss: 3.8e+03,  Train Acc: 58.37%,  Val Loss: 1.1e+03,  Val Acc: 57.44%,  Time: 0:08:02\n",
      "Iter:   6000,  Train Loss: 3.7e+03,  Train Acc: 59.35%,  Val Loss: 1.1e+03,  Val Acc: 58.32%,  Time: 0:08:43*\n",
      "Iter:   6500,  Train Loss: 3.7e+03,  Train Acc: 59.62%,  Val Loss: 1.1e+03,  Val Acc: 58.62%,  Time: 0:09:25*\n",
      "Iter:   6828,  Train Loss: 3.7e+03,  Train Acc: 59.98%,  Val Loss: 1.1e+03,  Val Acc: 59.01%,  Time: 0:10:01*\n",
      "Epoch [3/20]\n",
      "Iter:   7000,  Train Loss: 3.6e+03,  Train Acc: 59.97%,  Val Loss: 1.1e+03,  Val Acc: 58.95%,  Time: 0:10:31\n",
      "Iter:   7500,  Train Loss: 3.6e+03,  Train Acc: 60.43%,  Val Loss: 1.1e+03,  Val Acc: 59.35%,  Time: 0:11:13*\n",
      "Iter:   8000,  Train Loss: 3.6e+03,  Train Acc: 60.76%,  Val Loss: 1e+03,  Val Acc: 59.53%,  Time: 0:11:55*\n",
      "Iter:   8500,  Train Loss: 3.5e+03,  Train Acc: 61.30%,  Val Loss: 1e+03,  Val Acc: 60.11%,  Time: 0:12:38*\n",
      "Iter:   9000,  Train Loss: 3.5e+03,  Train Acc: 61.53%,  Val Loss: 1e+03,  Val Acc: 60.31%,  Time: 0:13:20*\n",
      "Iter:   9500,  Train Loss: 3.5e+03,  Train Acc: 61.77%,  Val Loss: 1e+03,  Val Acc: 60.46%,  Time: 0:14:02*\n",
      "Iter:  10000,  Train Loss: 3.4e+03,  Train Acc: 62.39%,  Val Loss: 1e+03,  Val Acc: 61.16%,  Time: 0:14:44*\n",
      "Iter:  10242,  Train Loss: 3.4e+03,  Train Acc: 62.71%,  Val Loss: 1e+03,  Val Acc: 61.43%,  Time: 0:15:17*\n",
      "Epoch [4/20]\n",
      "Iter:  10500,  Train Loss: 3.4e+03,  Train Acc: 63.14%,  Val Loss: 1e+03,  Val Acc: 61.74%,  Time: 0:15:50*\n",
      "Iter:  11000,  Train Loss: 3.4e+03,  Train Acc: 63.08%,  Val Loss: 9.9e+02,  Val Acc: 61.53%,  Time: 0:16:31\n",
      "Iter:  11500,  Train Loss: 3.4e+03,  Train Acc: 63.77%,  Val Loss: 9.9e+02,  Val Acc: 62.20%,  Time: 0:17:13*\n",
      "Iter:  12000,  Train Loss: 3.3e+03,  Train Acc: 63.61%,  Val Loss: 9.8e+02,  Val Acc: 62.08%,  Time: 0:17:54\n"
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "train_set, dev_set, test_set = build_dataset(config)\n",
    "### customized hyper param\n",
    "config.learning_rate = 1e-5 # lr = 1e-3 导致模型震荡，acc上不去\n",
    "config.log_step = 500\n",
    "config.drop_out = 0.3\n",
    "###\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed_all(1)\n",
    "torch.backends.cudnn.deterministic = True  # 保证每次结果一样\n",
    "\n",
    "model = Text_CNN(config).to(config.device)\n",
    "init_network(model)\n",
    "\n",
    "train(model, config, train_set, dev_set, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8048e6e3",
   "metadata": {},
   "source": [
    "## Test code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "76d60e0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import  gc\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "70be2f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def empty_cache(var):\n",
    "    del var\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c732ee71",
   "metadata": {},
   "source": [
    "似乎占用的显存只能通过重启kernel或者kill进程释放。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "209.648px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
