{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b73fd198",
   "metadata": {},
   "source": [
    "### import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e01e25ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle as pkl\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0dcc9b",
   "metadata": {},
   "source": [
    "## Data preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a48cb2",
   "metadata": {},
   "source": [
    "see task1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74edbf7e",
   "metadata": {},
   "source": [
    "## unit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bfafe9",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3300e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    def __init__(self):\n",
    "        self.train_path = r'./data_set/train.txt'\n",
    "        self.dev_path = r'./data_set/dev.txt'\n",
    "        self.test_path = r'./data_set/test.txt'\n",
    "        self.vocab_path = r'./data_set/vocab.pkl'\n",
    "        self.train_set_path = r'./data_set/train.pkl'\n",
    "        self.dev_set_path = r'./data_set/dev.pkl'\n",
    "        self.test_set_path = r'./data_set/test.pkl'\n",
    "        self.class_list = [x.strip() for x in open(r'./data_set/labels.txt', encoding='utf-8').readlines()]\n",
    "        self.label_num = len(self.class_list)\n",
    "        self.batch_size = 16\n",
    "        self.epoch = 20\n",
    "        self.learning_rate = 5e-3\n",
    "        self.log_step = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6a4a73",
   "metadata": {},
   "source": [
    "### DatasetIterater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d02e93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(config):\n",
    "    '''\n",
    "    变成[([],y),([],y),([],y),([],y)]\n",
    "    '''\n",
    "    if os.path.exists(config.vocab_path):\n",
    "        vocab = pkl.load(open(config.vocab_path, 'rb'))\n",
    "    else:\n",
    "        vocab = build_vocab(config.train_path, config.vocab_path)\n",
    "    print(\"vocab loaded sucessed, size : %d \" %len(vocab))\n",
    "    def load_data(file_path, output_path):\n",
    "        if os.path.exists(output_path):\n",
    "            data = pkl.load(open(output_path, 'rb'))\n",
    "            print(\"%s loaded success, size: %d\" %(output_path, len(data)))\n",
    "            return data\n",
    "        data = open(file_path, 'r', encoding='utf-8').readlines()\n",
    "        lemmatized_data = list()\n",
    "        for line in tqdm(data):\n",
    "            try:\n",
    "                x, y = line.strip().split('\\t')\n",
    "            except:\n",
    "                print(line)\n",
    "            tokens = word_tokenize(x.lower())              # 分词,同时大写换小写\n",
    "            tagged_sent = pos_tag(tokens, tagset='universal')     # 词性标注\n",
    "            wnl = WordNetLemmatizer()\n",
    "            lemmatized_sentence = [wnl.lemmatize(word, pos=get_wordnet_pos(tag)) for word, tag in tagged_sent]\n",
    "            lemmatized_data.append((lemmatized_sentence, y))\n",
    "        pkl.dump(lemmatized_data, open(output_path, 'wb'))\n",
    "        print(\"%s loaded success, size: %d\" %(file_path, len(lemmatized_data)))\n",
    "        return lemmatized_data\n",
    "    test_set = load_data(config.test_path, config.test_set_path)\n",
    "    dev_set = load_data(config.dev_path, config.dev_set_path)\n",
    "    train_set = load_data(config.train_path, config.train_set_path)\n",
    "    return vocab, train_set, dev_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45feda88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetIterater(object):\n",
    "    '''\n",
    "    return the batch index of dataset\n",
    "    '''\n",
    "    def __init__(self, data_set, batch_size):\n",
    "        self.data_set = data_set\n",
    "        self.batch_size = batch_size\n",
    "        self.n_batches = len(data_set) // batch_size\n",
    "        self.residue = True if len(data_set) % self.n_batches != 0 else False\n",
    "        self.index = 0\n",
    "        \n",
    "    def to_couple(self, raw_batch):\n",
    "        x = [_[0] for _ in raw_batch]\n",
    "        y = [_[1] for _ in raw_batch]\n",
    "        return x,y\n",
    "        \n",
    "    def __next__(self):\n",
    "        if self.index == self.n_batches and self.residue:\n",
    "            raw_batch = self.data_set[self.index * self.batch_size: len(self.data_set)]\n",
    "            batch = self.to_couple(raw_batch)\n",
    "            self.index += 1\n",
    "            return batch\n",
    "        elif self.index >= self.n_batches:\n",
    "            self.index = 0\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            raw_batch = self.data_set[self.index * self.batch_size: (self.index+1) * self.batch_size]\n",
    "            batch = self.to_couple(raw_batch)\n",
    "            self.index += 1\n",
    "            return batch\n",
    "           \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_batches+1 if self.residue else self.n_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd8c546",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff012d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text_RNN(nn.Model):\n",
    "    def __init__(self, config):\n",
    "        super(Model, self).__init__()\n",
    "        if config.embedding_pretrained is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(config.embedding_pretrained, freeze=False)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(config.n_vocab, config.embed, padding_idx=config.n_vocab - 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034cd47c",
   "metadata": {},
   "source": [
    "## train and evl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3daa755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "908cc186",
   "metadata": {},
   "source": [
    "## run"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
