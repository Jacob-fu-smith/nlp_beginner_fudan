{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddb1517c",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f4f5a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle as pkl\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45afd34f",
   "metadata": {},
   "source": [
    "## data preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4c5a88",
   "metadata": {},
   "source": [
    "### spit evaluate set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6774929e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 156060 entries, 0 to 156059\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   PhraseId    156060 non-null  int64 \n",
      " 1   SentenceId  156060 non-null  int64 \n",
      " 2   Phrase      156060 non-null  object\n",
      " 3   Sentiment   156060 non-null  int64 \n",
      "dtypes: int64(3), object(1)\n",
      "memory usage: 4.8+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = pd.read_csv('./raw_data/train.tsv', sep='\\t')\n",
    "df_data.info()\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29a90db7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 0]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = df_data['Sentiment'].unique()\n",
    "labels = list(labels)\n",
    "labels\n",
    "with open(r'./data_set/labels.txt', 'w', encoding='utf-8') as f:\n",
    "    for label in labels:\n",
    "        f.write(str(label) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d410fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_index = np.random.permutation(len(df_data))\n",
    "shuffled_data = df_data[['Phrase','Sentiment']].iloc[shuffle_index]\n",
    "shuffled_data.iloc[:int(0.7*len(df_data))].to_csv(r'./data_set/train.txt', sep='\\t', encoding='utf-8', header=None, index=False)\n",
    "shuffled_data.iloc[int(0.7*len(df_data))+1:int(0.9*len(df_data))].to_csv(r'./data_set/dev.txt', sep='\\t', encoding='utf-8', header=None, index=False)\n",
    "shuffled_data.iloc[int(0.9*len(df_data))+1:].to_csv(r'./data_set/test.txt', sep='\\t', encoding='utf-8', header=None, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d192ecc",
   "metadata": {},
   "source": [
    "### build vocabulary（BOW） and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ff3179",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "UNK, PAD = '<UNK>', '<PAD>'  # 未知字，padding符号\n",
    "\n",
    "# 获取单词的词性\n",
    "def get_wordnet_pos(tag):\n",
    "    '''\n",
    "    for WordNetLemmatizer.lemmatize()\n",
    "    '''\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def build_vocab(data_path, vocab_path):\n",
    "    data = [_.strip().split('\\t')[0] for _ in open(data_path, 'r', encoding='utf-8').readlines()]\n",
    "    word_cnt = dict()\n",
    "    for sentence in tqdm(data):\n",
    "        tokens = word_tokenize(sentence.lower())              # 分词,同时大写换小写\n",
    "        tagged_sent = pos_tag(tokens, tagset='universal')     # 词性标注\n",
    "        wnl = WordNetLemmatizer()\n",
    "        for word, tag in tagged_sent:\n",
    "            lemmatized_word = wnl.lemmatize(word, pos=get_wordnet_pos(tag)) # 还原后的词\n",
    "            word_cnt[lemmatized_word] = word_cnt.get(lemmatized_word, 0) + 1\n",
    "    word_cnt = sorted(word_cnt.items(), key=lambda x:x[0], reverse=True)\n",
    "    print(len(word_cnt))\n",
    "    vocab = {_[0]: idx for idx, _ in enumerate(word_cnt)}\n",
    "    vocab.update({UNK: len(vocab), PAD: len(vocab) + 1})\n",
    "    pkl.dump(vocab, open(vocab_path, 'wb'))\n",
    "    print(\"vocab build successed, size : %d\" %len(vocab))\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42cabbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(config):\n",
    "    '''\n",
    "    变成[([],y),([],y),([],y),([],y)]\n",
    "    '''\n",
    "    if os.path.exists(config.vocab_path):\n",
    "        vocab = pkl.load(open(config.vocab_path, 'rb'))\n",
    "    else:\n",
    "        vocab = build_vocab(config.train_path, config.vocab_path)\n",
    "    print(\"vocab loaded sucessed, size : %d \" %len(vocab))\n",
    "    def load_data(file_path, output_path):\n",
    "        if os.path.exists(output_path):\n",
    "            data = pkl.load(open(output_path, 'rb'))\n",
    "            print(\"%s loaded success, size: %d\" %(output_path, len(data)))\n",
    "            return data\n",
    "        data = open(file_path, 'r', encoding='utf-8').readlines()\n",
    "        lemmatized_data = list()\n",
    "        for line in tqdm(data):\n",
    "            try:\n",
    "                x, y = line.strip().split('\\t')\n",
    "            except:\n",
    "                print(line)\n",
    "            tokens = word_tokenize(x.lower())              # 分词,同时大写换小写\n",
    "            tagged_sent = pos_tag(tokens, tagset='universal')     # 词性标注\n",
    "            wnl = WordNetLemmatizer()\n",
    "            lemmatized_sentence = [wnl.lemmatize(word, pos=get_wordnet_pos(tag)) for word, tag in tagged_sent]\n",
    "            lemmatized_data.append((lemmatized_sentence, y))\n",
    "        pkl.dump(lemmatized_data, open(output_path, 'wb'))\n",
    "        print(\"%s loaded success, size: %d\" %(file_path, len(lemmatized_data)))\n",
    "        return lemmatized_data\n",
    "    test_set = load_data(config.test_path, config.test_set_path)\n",
    "    dev_set = load_data(config.dev_path, config.dev_set_path)\n",
    "    train_set = load_data(config.train_path, config.train_set_path)\n",
    "    return vocab, train_set, dev_set, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcaa731",
   "metadata": {},
   "source": [
    "## units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a159b8",
   "metadata": {},
   "source": [
    "### config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48938037",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    def __init__(self):\n",
    "        self.train_path = r'./data_set/train.txt'\n",
    "        self.dev_path = r'./data_set/dev.txt'\n",
    "        self.test_path = r'./data_set/test.txt'\n",
    "        self.vocab_path = r'./data_set/vocab.pkl'\n",
    "        self.train_set_path = r'./data_set/train.pkl'\n",
    "        self.dev_set_path = r'./data_set/dev.pkl'\n",
    "        self.test_set_path = r'./data_set/test.pkl'\n",
    "        self.class_list = [x.strip() for x in open(r'./data_set/labels.txt', encoding='utf-8').readlines()]\n",
    "        self.label_num = len(self.class_list)\n",
    "        self.batch_size = 16\n",
    "        self.epoch = 20\n",
    "        self.learning_rate = 5e-3\n",
    "        self.log_step = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8018c48a",
   "metadata": {},
   "source": [
    "### DatasetIterater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01b5ffd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetIterater(object):\n",
    "    '''\n",
    "    return the batch index of dataset\n",
    "    '''\n",
    "    def __init__(self, data_set, batch_size):\n",
    "        self.data_set = data_set\n",
    "        self.batch_size = batch_size\n",
    "        self.n_batches = len(data_set) // batch_size\n",
    "        self.residue = True if len(data_set) % self.n_batches != 0 else False\n",
    "        self.index = 0\n",
    "        \n",
    "    def to_couple(self, raw_batch):\n",
    "        x = [_[0] for _ in raw_batch]\n",
    "        y = [_[1] for _ in raw_batch]\n",
    "        return x,y\n",
    "        \n",
    "    def __next__(self):\n",
    "        if self.index == self.n_batches and self.residue:\n",
    "            raw_batch = self.data_set[self.index * self.batch_size: len(self.data_set)]\n",
    "            batch = self.to_couple(raw_batch)\n",
    "            self.index += 1\n",
    "            return batch\n",
    "        elif self.index >= self.n_batches:\n",
    "            self.index = 0\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            raw_batch = self.data_set[self.index * self.batch_size: (self.index+1) * self.batch_size]\n",
    "            batch = self.to_couple(raw_batch)\n",
    "            self.index += 1\n",
    "            return batch\n",
    "           \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_batches+1 if self.residue else self.n_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f674af85",
   "metadata": {},
   "source": [
    "Test of config and dataload "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e23a9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab loaded sucessed, size : 14670 \n",
      "./data_set/test.pkl loaded success, size: 15605\n",
      "./data_set/dev.pkl loaded success, size: 31211\n",
      "./data_set/train.pkl loaded success, size: 109242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "7000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "9000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "11000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "12000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "13000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "13656"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config()\n",
    "vocab, train_set, dev_set, test_set = build_dataset(config)\n",
    "data_iter = DatasetIterater(data_set = train_set, batch_size = 8)\n",
    "for batch in data_iter:\n",
    "    if data_iter.index % 1000 == 0:\n",
    "        data_iter.index\n",
    "len(data_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368b0b78",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2691e81e",
   "metadata": {},
   "source": [
    "### one hot Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b81c37be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotEmbedding(object):\n",
    "    def __init__(self, config):\n",
    "        self.vocab = pkl.load(open(config.vocab_path, 'rb'))\n",
    "        self.embedding_shape = (config.batch_size, len(self.vocab))\n",
    "        self.embedding_length = len(self.vocab)\n",
    "        \n",
    "    def batch_vectorize(self, x):\n",
    "        '''\n",
    "        batch_data: batch_size * length\n",
    "        '''\n",
    "        vec = np.zeros((len(x), len(vocab)), dtype=int)\n",
    "        for i, tokens in enumerate(x):\n",
    "            vec[i,[vocab.get(token, vocab['<UNK>']) for token in tokens]] = 1\n",
    "        return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ea9ce4",
   "metadata": {},
   "source": [
    "Test of one-hot embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a7f79a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab loaded sucessed, size : 14670 \n",
      "./data_set/test.pkl loaded success, size: 15605\n",
      "./data_set/dev.pkl loaded success, size: 31211\n",
      "./data_set/train.pkl loaded success, size: 109242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(16, 14670)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(array([   56,   227,   269,  1295,  1452,  1543,  1682,  1724,  1941,\n",
       "         2497,  2751,  3697,  3699,  4093,  4654,  4676,  5516,  5609,\n",
       "         5719,  5720,  5739,  6241,  6464,  6885,  8714,  9302,  9617,\n",
       "         9829, 10380, 13401, 13638, 14438, 14637, 14640], dtype=int64),)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab, train_set, dev_set, test_set = build_dataset(config)\n",
    "data_iter = DatasetIterater(data_set = train_set, batch_size = config.batch_size)\n",
    "x, y  = next(data_iter)\n",
    "embedding = OneHotEmbedding(config)\n",
    "vec = embedding.batch_vectorize(x)\n",
    "vec.shape\n",
    "vec[0].nonzero()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c26324",
   "metadata": {},
   "source": [
    "### logistic model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3279f97a",
   "metadata": {},
   "source": [
    "$$\n",
    "logistic = \\frac{1}{1+exp(-w^Tx)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456f58b4",
   "metadata": {},
   "source": [
    "For binary classificaiton:\n",
    "$$\n",
    "loss = -\\frac{1}{N}\\sum^{N}_{n=1}(y^{(n)}log(\\hat{y}^{(n)})+(1-y^{(n)})log(1-\\hat{y}^{(n)}))\n",
    "$$\n",
    "For multi-classification:\n",
    "$$\n",
    "loss = -\\frac{1}{N}\\sum^{N}_{n=1}\\sum^{C}_{c=1}(y^{(c)}log(\\hat{y}^{(c)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc219786",
   "metadata": {},
   "source": [
    "Back Propagation:\n",
    "$$\n",
    "\\frac{\\partial{R(w)}}{\\partial{w}} =\n",
    "-\\frac{1}{N}\\sum^{N}_{n=1}x^{(n)}(y^{(n)}-\\hat{y}^{(n)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20f55088",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticModel(object):\n",
    "    def __init__(self, embedding, config):\n",
    "        self.embedding = embedding\n",
    "        self.weight = np.zeros((config.label_num, self.embedding.embedding_length))  # label_num*14760\n",
    "        self.bias = np.zeros((1, config.label_num))\n",
    "        self.learing_rate = config.learning_rate\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        for multi-logistic, argmax adopted\n",
    "        input: batch * embedding_length\n",
    "        weight: embedding_length * label_num\n",
    "        output: batch * label_num\n",
    "        '''\n",
    "        one_hot_embedding = self.embedding.batch_vectorize(x)\n",
    "        o = one_hot_embedding.dot(self.weight.T) + self.bias\n",
    "        out = 1/(1+np.exp(-o))\n",
    "        return out\n",
    "        \n",
    "    def backward(self, x, y_true, lr):\n",
    "        '''\n",
    "        learing_rate\n",
    "        x: batch*character_length    8*14670\n",
    "        weight: label*num*character  5*14670\n",
    "        bias: 1*label_num\n",
    "        \n",
    "        y_hat:  batch*label_num      8*5\n",
    "        y_true: 1*label_num -> batch*label_num\n",
    "        \n",
    "        '''\n",
    "        one_hot_embedding = self.embedding.batch_vectorize(x)\n",
    "        o = one_hot_embedding.dot(self.weight.T) + self.bias\n",
    "        y_hat = 1/(1+np.exp(-o))\n",
    "        \n",
    "        binary_labels = np.zeros(y_hat.shape)\n",
    "        for i,label in enumerate(y_true):\n",
    "            binary_labels[i,int(label)] = 1\n",
    "        \n",
    "        ### mini-batch gradient descent\n",
    "        ex_x = np.expand_dims(one_hot_embedding, axis=2)\n",
    "        ex_y = np.expand_dims((y_hat - binary_labels), axis=1)\n",
    "        partial_w = np.einsum('ijk,ikn->ijn', ex_x, ex_y).mean(axis=0)\n",
    "        partial_b = (y_hat - binary_labels).mean(axis=0)\n",
    "#         print(ex_x.shape)\n",
    "#         print(ex_y.shape)\n",
    "#         print(\"partial_w shape:\", partial_w.shape)\n",
    "#         print(\"partial_b shape:\", partial_b.shape)\n",
    "        self.weight = self.weight - lr*partial_w.T\n",
    "        self.bias = self.bias - lr*partial_b\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b3de034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=axis, keepdims=True)   \n",
    "\n",
    "def cross_entropy_loss(y_hat, y_true, sm=False):\n",
    "    binary_labels = np.zeros(y_hat.shape)\n",
    "    for i,label in enumerate(y_true):\n",
    "        binary_labels[i,int(label)] = 1\n",
    "    if sm:\n",
    "        y_hat = softmax(y_hat, axis=1)\n",
    "    eps = 1e-15\n",
    "    y_hat = np.where(y_hat > 1-eps, 1-eps, y_hat)\n",
    "    y_hat = np.where(y_hat < eps, eps, y_hat)\n",
    "#     corss_entry = binary_labels*np.log(y_hat) + (1-binary_labels)*np.log(1-y_hat) # binary classification\n",
    "    cross_entropy = binary_labels*np.log(y_hat)\n",
    "    loss = -cross_entropy.sum()/len(y_true)\n",
    "    return loss\n",
    "\n",
    "def get_time_dif(start_time):\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    return timedelta(seconds=int(round(time_dif)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "417543b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab loaded sucessed, size : 14670 \n",
      "./data_set/test.pkl loaded success, size: 15605\n",
      "./data_set/dev.pkl loaded success, size: 31211\n",
      "./data_set/train.pkl loaded success, size: 109242\n"
     ]
    }
   ],
   "source": [
    "## Test for logistic model\n",
    "vocab, train_set, dev_set, test_set = build_dataset(config)\n",
    "data_iter = DatasetIterater(data_set = train_set, batch_size = 8)\n",
    "x, y_true = next(data_iter)\n",
    "embedding = OneHotEmbedding(config)\n",
    "model = LogisticModel(embedding, config)\n",
    "out = model.forward(x)\n",
    "loss = cross_entropy_loss(out, y_true, sm=False)\n",
    "model.backward(x, y_true, config.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb31942",
   "metadata": {},
   "source": [
    "## train and eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40efc87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train(model, config, train_set, dev_set, test_set):\n",
    "def train(model, config, train_iter, dev_iter, test_iter):\n",
    "    start_time = time.time()\n",
    "    total_iter = 0\n",
    "    for epoch in range(config.epoch):\n",
    "        print('Epoch [{}/{}]'.format(epoch + 1, config.epoch))\n",
    "        for i, (x, y_true) in enumerate(train_iter):\n",
    "            out = model.forward(x)\n",
    "            loss = cross_entropy_loss(out, y_true, sm=False)\n",
    "            model.backward(x, y_true, config.learning_rate)\n",
    "            total_iter += 1\n",
    "            if total_iter % config.log_step == 0:\n",
    "                acc_train, loss_train = evalute(model, config, DatasetIterater(data_set = train_set, batch_size = config.batch_size))\n",
    "                acc_dev, loss_dev = evalute(model, config, DatasetIterater(data_set = dev_set, batch_size = config.batch_size))\n",
    "                time_dif = get_time_dif(start_time)\n",
    "                msg = 'Iter: {0:>6},  Train Loss: {1:>5.2},  Train Acc: {2:>6.2%},  Val Loss: {3:>5.2},  Val Acc: {4:>6.2%},  Time: {5}'\n",
    "                print(msg.format(total_iter, loss_train, acc_train, loss_dev, acc_dev, time_dif))\n",
    "    acc_test, loss_test = evalute(test_iter, batch_size = config.batch_size)\n",
    "    msg = 'Train Loss: {0:>5.2},  Train Acc: {1:>6.2%}'\n",
    "    print(msg.format(loss_test, acc_test))\n",
    "    \n",
    "def evalute(model, config, data_set):\n",
    "    loss_total = 0\n",
    "    labels_predict = np.arange(0)\n",
    "    labels_true = []\n",
    "    for (x, y_true) in data_set:\n",
    "        out = model.forward(x)\n",
    "        loss = cross_entropy_loss(out, y_true, sm=False)\n",
    "        loss_total += loss\n",
    "        labels_predict = np.concatenate((labels_predict, out.argmax(axis=1)), axis=0)\n",
    "        labels_true += y_true\n",
    "    try:\n",
    "        assert len(labels_true) == labels_predict.size\n",
    "    except AssertionError as e:\n",
    "        print(len(x))\n",
    "        print(out.shape)\n",
    "        print(\"labels_true:\", len(labels_true))\n",
    "        print(\"labels_predict:\",labels_predict.size)\n",
    "        raise AssertionError\n",
    "    labels_true = np.array([int(_) for _ in labels_true])\n",
    "    loss = loss_total/len(data_set)\n",
    "    acc = (labels_true == labels_predict).mean()\n",
    "    return acc, loss_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ee8a04",
   "metadata": {},
   "source": [
    "## run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6934033",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab loaded sucessed, size : 14670 \n",
      "./data_set/test.pkl loaded success, size: 15605\n",
      "./data_set/dev.pkl loaded success, size: 31211\n",
      "./data_set/train.pkl loaded success, size: 109242\n",
      "Epoch [1/20]\n",
      "Iter:   1000,  Train Loss: 7.2e+03,  Train Acc: 50.98%,  Val Loss: 2.1e+03,  Val Acc: 50.82%,  Time: 0:00:16\n",
      "Iter:   2000,  Train Loss: 7.9e+03,  Train Acc: 50.98%,  Val Loss: 2.3e+03,  Val Acc: 50.82%,  Time: 0:00:34\n",
      "Iter:   3000,  Train Loss: 8.2e+03,  Train Acc: 51.11%,  Val Loss: 2.3e+03,  Val Acc: 50.94%,  Time: 0:00:49\n",
      "Iter:   4000,  Train Loss: 8.2e+03,  Train Acc: 51.41%,  Val Loss: 2.4e+03,  Val Acc: 51.19%,  Time: 0:01:04\n",
      "Iter:   5000,  Train Loss: 8.3e+03,  Train Acc: 51.72%,  Val Loss: 2.4e+03,  Val Acc: 51.49%,  Time: 0:01:19\n",
      "Iter:   6000,  Train Loss: 8.3e+03,  Train Acc: 51.95%,  Val Loss: 2.4e+03,  Val Acc: 51.64%,  Time: 0:01:33\n",
      "Epoch [2/20]\n",
      "Iter:   7000,  Train Loss: 8.3e+03,  Train Acc: 52.11%,  Val Loss: 2.4e+03,  Val Acc: 51.82%,  Time: 0:01:47\n",
      "Iter:   8000,  Train Loss: 8.2e+03,  Train Acc: 52.19%,  Val Loss: 2.4e+03,  Val Acc: 51.96%,  Time: 0:02:01\n",
      "Iter:   9000,  Train Loss: 8.2e+03,  Train Acc: 52.35%,  Val Loss: 2.4e+03,  Val Acc: 52.20%,  Time: 0:02:15\n",
      "Iter:  10000,  Train Loss: 8.2e+03,  Train Acc: 52.42%,  Val Loss: 2.4e+03,  Val Acc: 52.28%,  Time: 0:02:30\n",
      "Iter:  11000,  Train Loss: 8.2e+03,  Train Acc: 52.44%,  Val Loss: 2.4e+03,  Val Acc: 52.31%,  Time: 0:02:45\n",
      "Iter:  12000,  Train Loss: 8.2e+03,  Train Acc: 52.54%,  Val Loss: 2.4e+03,  Val Acc: 52.39%,  Time: 0:03:00\n",
      "Iter:  13000,  Train Loss: 8.2e+03,  Train Acc: 52.66%,  Val Loss: 2.4e+03,  Val Acc: 52.50%,  Time: 0:03:15\n",
      "Epoch [3/20]\n",
      "Iter:  14000,  Train Loss: 8.2e+03,  Train Acc: 52.66%,  Val Loss: 2.3e+03,  Val Acc: 52.51%,  Time: 0:03:31\n",
      "Iter:  15000,  Train Loss: 8.2e+03,  Train Acc: 52.70%,  Val Loss: 2.3e+03,  Val Acc: 52.56%,  Time: 0:03:46\n",
      "Iter:  16000,  Train Loss: 8.2e+03,  Train Acc: 52.78%,  Val Loss: 2.3e+03,  Val Acc: 52.68%,  Time: 0:04:02\n",
      "Iter:  17000,  Train Loss: 8.2e+03,  Train Acc: 52.78%,  Val Loss: 2.3e+03,  Val Acc: 52.64%,  Time: 0:04:17\n",
      "Iter:  18000,  Train Loss: 8.1e+03,  Train Acc: 52.79%,  Val Loss: 2.3e+03,  Val Acc: 52.66%,  Time: 0:04:32\n",
      "Iter:  19000,  Train Loss: 8.1e+03,  Train Acc: 52.85%,  Val Loss: 2.3e+03,  Val Acc: 52.72%,  Time: 0:04:48\n",
      "Iter:  20000,  Train Loss: 8.1e+03,  Train Acc: 52.94%,  Val Loss: 2.3e+03,  Val Acc: 52.83%,  Time: 0:05:03\n",
      "Epoch [4/20]\n",
      "Iter:  21000,  Train Loss: 8.1e+03,  Train Acc: 52.96%,  Val Loss: 2.3e+03,  Val Acc: 52.81%,  Time: 0:05:19\n",
      "Iter:  22000,  Train Loss: 8.1e+03,  Train Acc: 52.98%,  Val Loss: 2.3e+03,  Val Acc: 52.87%,  Time: 0:05:34\n",
      "Iter:  23000,  Train Loss: 8.1e+03,  Train Acc: 53.03%,  Val Loss: 2.3e+03,  Val Acc: 52.93%,  Time: 0:05:51\n",
      "Iter:  24000,  Train Loss: 8.1e+03,  Train Acc: 53.02%,  Val Loss: 2.3e+03,  Val Acc: 52.86%,  Time: 0:06:07\n",
      "Iter:  25000,  Train Loss: 8.1e+03,  Train Acc: 53.04%,  Val Loss: 2.3e+03,  Val Acc: 52.89%,  Time: 0:06:24\n",
      "Iter:  26000,  Train Loss: 8.1e+03,  Train Acc: 53.08%,  Val Loss: 2.3e+03,  Val Acc: 52.98%,  Time: 0:06:40\n",
      "Iter:  27000,  Train Loss: 8.1e+03,  Train Acc: 53.14%,  Val Loss: 2.3e+03,  Val Acc: 53.03%,  Time: 0:06:56\n",
      "Epoch [5/20]\n",
      "Iter:  28000,  Train Loss: 8.1e+03,  Train Acc: 53.14%,  Val Loss: 2.3e+03,  Val Acc: 52.97%,  Time: 0:07:13\n",
      "Iter:  29000,  Train Loss: 8.1e+03,  Train Acc: 53.17%,  Val Loss: 2.3e+03,  Val Acc: 53.06%,  Time: 0:07:29\n",
      "Iter:  30000,  Train Loss: 8.1e+03,  Train Acc: 53.21%,  Val Loss: 2.3e+03,  Val Acc: 53.11%,  Time: 0:07:46\n",
      "Iter:  31000,  Train Loss: 8.1e+03,  Train Acc: 53.20%,  Val Loss: 2.3e+03,  Val Acc: 53.01%,  Time: 0:08:03\n",
      "Iter:  32000,  Train Loss: 8.1e+03,  Train Acc: 53.23%,  Val Loss: 2.3e+03,  Val Acc: 53.07%,  Time: 0:08:19\n",
      "Iter:  33000,  Train Loss: 8.1e+03,  Train Acc: 53.31%,  Val Loss: 2.3e+03,  Val Acc: 53.15%,  Time: 0:08:36\n",
      "Iter:  34000,  Train Loss: 8.1e+03,  Train Acc: 53.40%,  Val Loss: 2.3e+03,  Val Acc: 53.18%,  Time: 0:08:53\n",
      "Epoch [6/20]\n",
      "Iter:  35000,  Train Loss: 8e+03,  Train Acc: 53.37%,  Val Loss: 2.3e+03,  Val Acc: 53.14%,  Time: 0:09:09\n",
      "Iter:  36000,  Train Loss: 8e+03,  Train Acc: 53.39%,  Val Loss: 2.3e+03,  Val Acc: 53.15%,  Time: 0:09:25\n",
      "Iter:  37000,  Train Loss: 8e+03,  Train Acc: 53.40%,  Val Loss: 2.3e+03,  Val Acc: 53.14%,  Time: 0:09:41\n",
      "Iter:  38000,  Train Loss: 8e+03,  Train Acc: 53.41%,  Val Loss: 2.3e+03,  Val Acc: 53.18%,  Time: 0:09:59\n",
      "Iter:  39000,  Train Loss: 8e+03,  Train Acc: 53.45%,  Val Loss: 2.3e+03,  Val Acc: 53.22%,  Time: 0:10:15\n",
      "Iter:  40000,  Train Loss: 8e+03,  Train Acc: 53.54%,  Val Loss: 2.3e+03,  Val Acc: 53.35%,  Time: 0:10:31\n",
      "Epoch [7/20]\n",
      "Iter:  41000,  Train Loss: 8e+03,  Train Acc: 53.59%,  Val Loss: 2.3e+03,  Val Acc: 53.37%,  Time: 0:10:47\n",
      "Iter:  42000,  Train Loss: 8e+03,  Train Acc: 53.58%,  Val Loss: 2.3e+03,  Val Acc: 53.31%,  Time: 0:11:04\n",
      "Iter:  43000,  Train Loss: 8e+03,  Train Acc: 53.57%,  Val Loss: 2.3e+03,  Val Acc: 53.30%,  Time: 0:11:20\n",
      "Iter:  44000,  Train Loss: 8e+03,  Train Acc: 53.62%,  Val Loss: 2.3e+03,  Val Acc: 53.37%,  Time: 0:11:35\n",
      "Iter:  45000,  Train Loss: 8e+03,  Train Acc: 53.61%,  Val Loss: 2.3e+03,  Val Acc: 53.39%,  Time: 0:11:51\n",
      "Iter:  46000,  Train Loss: 8e+03,  Train Acc: 53.64%,  Val Loss: 2.3e+03,  Val Acc: 53.45%,  Time: 0:12:06\n",
      "Iter:  47000,  Train Loss: 8e+03,  Train Acc: 53.72%,  Val Loss: 2.3e+03,  Val Acc: 53.48%,  Time: 0:12:22\n",
      "Epoch [8/20]\n",
      "Iter:  48000,  Train Loss: 8e+03,  Train Acc: 53.77%,  Val Loss: 2.3e+03,  Val Acc: 53.48%,  Time: 0:12:37\n",
      "Iter:  49000,  Train Loss: 8e+03,  Train Acc: 53.73%,  Val Loss: 2.3e+03,  Val Acc: 53.44%,  Time: 0:12:53\n",
      "Iter:  50000,  Train Loss: 8e+03,  Train Acc: 53.81%,  Val Loss: 2.3e+03,  Val Acc: 53.48%,  Time: 0:13:08\n",
      "Iter:  51000,  Train Loss: 8e+03,  Train Acc: 53.84%,  Val Loss: 2.3e+03,  Val Acc: 53.55%,  Time: 0:13:24\n",
      "Iter:  52000,  Train Loss: 8e+03,  Train Acc: 53.77%,  Val Loss: 2.3e+03,  Val Acc: 53.52%,  Time: 0:13:39\n",
      "Iter:  53000,  Train Loss: 8e+03,  Train Acc: 53.83%,  Val Loss: 2.3e+03,  Val Acc: 53.55%,  Time: 0:13:55\n",
      "Iter:  54000,  Train Loss: 8e+03,  Train Acc: 53.91%,  Val Loss: 2.3e+03,  Val Acc: 53.63%,  Time: 0:14:11\n",
      "Epoch [9/20]\n",
      "Iter:  55000,  Train Loss: 7.9e+03,  Train Acc: 53.91%,  Val Loss: 2.3e+03,  Val Acc: 53.63%,  Time: 0:14:27\n",
      "Iter:  56000,  Train Loss: 8e+03,  Train Acc: 53.91%,  Val Loss: 2.3e+03,  Val Acc: 53.67%,  Time: 0:14:43\n",
      "Iter:  57000,  Train Loss: 8e+03,  Train Acc: 53.97%,  Val Loss: 2.3e+03,  Val Acc: 53.69%,  Time: 0:14:58\n",
      "Iter:  58000,  Train Loss: 8e+03,  Train Acc: 54.00%,  Val Loss: 2.3e+03,  Val Acc: 53.69%,  Time: 0:15:14\n",
      "Iter:  59000,  Train Loss: 7.9e+03,  Train Acc: 53.91%,  Val Loss: 2.3e+03,  Val Acc: 53.66%,  Time: 0:15:30\n",
      "Iter:  60000,  Train Loss: 7.9e+03,  Train Acc: 53.97%,  Val Loss: 2.3e+03,  Val Acc: 53.69%,  Time: 0:15:47\n",
      "Iter:  61000,  Train Loss: 7.9e+03,  Train Acc: 54.03%,  Val Loss: 2.3e+03,  Val Acc: 53.74%,  Time: 0:16:04\n",
      "Epoch [10/20]\n",
      "Iter:  62000,  Train Loss: 7.9e+03,  Train Acc: 54.06%,  Val Loss: 2.3e+03,  Val Acc: 53.73%,  Time: 0:16:21\n",
      "Iter:  63000,  Train Loss: 7.9e+03,  Train Acc: 54.05%,  Val Loss: 2.3e+03,  Val Acc: 53.72%,  Time: 0:16:39\n",
      "Iter:  64000,  Train Loss: 7.9e+03,  Train Acc: 54.11%,  Val Loss: 2.3e+03,  Val Acc: 53.77%,  Time: 0:16:58\n",
      "Iter:  65000,  Train Loss: 7.9e+03,  Train Acc: 54.07%,  Val Loss: 2.3e+03,  Val Acc: 53.78%,  Time: 0:17:14\n",
      "Iter:  66000,  Train Loss: 7.9e+03,  Train Acc: 54.10%,  Val Loss: 2.3e+03,  Val Acc: 53.80%,  Time: 0:17:31\n",
      "Iter:  67000,  Train Loss: 7.9e+03,  Train Acc: 54.13%,  Val Loss: 2.3e+03,  Val Acc: 53.79%,  Time: 0:17:48\n",
      "Iter:  68000,  Train Loss: 7.9e+03,  Train Acc: 54.14%,  Val Loss: 2.3e+03,  Val Acc: 53.85%,  Time: 0:18:05\n",
      "Epoch [11/20]\n",
      "Iter:  69000,  Train Loss: 7.9e+03,  Train Acc: 54.14%,  Val Loss: 2.3e+03,  Val Acc: 53.80%,  Time: 0:18:21\n",
      "Iter:  70000,  Train Loss: 7.9e+03,  Train Acc: 54.18%,  Val Loss: 2.3e+03,  Val Acc: 53.82%,  Time: 0:18:37\n",
      "Iter:  71000,  Train Loss: 7.9e+03,  Train Acc: 54.21%,  Val Loss: 2.3e+03,  Val Acc: 53.86%,  Time: 0:18:54\n",
      "Iter:  72000,  Train Loss: 7.9e+03,  Train Acc: 54.16%,  Val Loss: 2.3e+03,  Val Acc: 53.89%,  Time: 0:19:11\n",
      "Iter:  73000,  Train Loss: 7.9e+03,  Train Acc: 54.22%,  Val Loss: 2.3e+03,  Val Acc: 53.85%,  Time: 0:19:28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  74000,  Train Loss: 7.9e+03,  Train Acc: 54.24%,  Val Loss: 2.3e+03,  Val Acc: 53.89%,  Time: 0:19:44\n",
      "Iter:  75000,  Train Loss: 7.9e+03,  Train Acc: 54.33%,  Val Loss: 2.3e+03,  Val Acc: 54.04%,  Time: 0:20:00\n",
      "Epoch [12/20]\n",
      "Iter:  76000,  Train Loss: 7.9e+03,  Train Acc: 54.28%,  Val Loss: 2.3e+03,  Val Acc: 53.95%,  Time: 0:20:15\n",
      "Iter:  77000,  Train Loss: 7.9e+03,  Train Acc: 54.31%,  Val Loss: 2.3e+03,  Val Acc: 53.94%,  Time: 0:20:31\n",
      "Iter:  78000,  Train Loss: 7.9e+03,  Train Acc: 54.33%,  Val Loss: 2.3e+03,  Val Acc: 53.95%,  Time: 0:20:47\n",
      "Iter:  79000,  Train Loss: 7.9e+03,  Train Acc: 54.29%,  Val Loss: 2.3e+03,  Val Acc: 54.00%,  Time: 0:21:02\n",
      "Iter:  80000,  Train Loss: 7.9e+03,  Train Acc: 54.32%,  Val Loss: 2.3e+03,  Val Acc: 53.98%,  Time: 0:21:18\n",
      "Iter:  81000,  Train Loss: 7.9e+03,  Train Acc: 54.42%,  Val Loss: 2.3e+03,  Val Acc: 54.11%,  Time: 0:21:36\n",
      "Epoch [13/20]\n",
      "Iter:  82000,  Train Loss: 7.9e+03,  Train Acc: 54.43%,  Val Loss: 2.3e+03,  Val Acc: 54.09%,  Time: 0:21:52\n",
      "Iter:  83000,  Train Loss: 7.8e+03,  Train Acc: 54.42%,  Val Loss: 2.3e+03,  Val Acc: 54.03%,  Time: 0:22:09\n",
      "Iter:  84000,  Train Loss: 7.8e+03,  Train Acc: 54.43%,  Val Loss: 2.3e+03,  Val Acc: 54.05%,  Time: 0:22:27\n",
      "Iter:  85000,  Train Loss: 7.8e+03,  Train Acc: 54.43%,  Val Loss: 2.3e+03,  Val Acc: 54.05%,  Time: 0:22:43\n",
      "Iter:  86000,  Train Loss: 7.8e+03,  Train Acc: 54.44%,  Val Loss: 2.3e+03,  Val Acc: 54.05%,  Time: 0:23:01\n",
      "Iter:  87000,  Train Loss: 7.8e+03,  Train Acc: 54.46%,  Val Loss: 2.3e+03,  Val Acc: 54.08%,  Time: 0:23:18\n",
      "Iter:  88000,  Train Loss: 7.8e+03,  Train Acc: 54.55%,  Val Loss: 2.3e+03,  Val Acc: 54.13%,  Time: 0:23:35\n",
      "Epoch [14/20]\n",
      "Iter:  89000,  Train Loss: 7.8e+03,  Train Acc: 54.57%,  Val Loss: 2.3e+03,  Val Acc: 54.23%,  Time: 0:23:52\n",
      "Iter:  90000,  Train Loss: 7.8e+03,  Train Acc: 54.59%,  Val Loss: 2.3e+03,  Val Acc: 54.17%,  Time: 0:24:09\n",
      "Iter:  91000,  Train Loss: 7.8e+03,  Train Acc: 54.61%,  Val Loss: 2.3e+03,  Val Acc: 54.19%,  Time: 0:24:26\n",
      "Iter:  92000,  Train Loss: 7.8e+03,  Train Acc: 54.64%,  Val Loss: 2.3e+03,  Val Acc: 54.26%,  Time: 0:24:41\n",
      "Iter:  93000,  Train Loss: 7.8e+03,  Train Acc: 54.58%,  Val Loss: 2.3e+03,  Val Acc: 54.11%,  Time: 0:24:57\n",
      "Iter:  94000,  Train Loss: 7.8e+03,  Train Acc: 54.65%,  Val Loss: 2.3e+03,  Val Acc: 54.16%,  Time: 0:25:15\n",
      "Iter:  95000,  Train Loss: 7.8e+03,  Train Acc: 54.70%,  Val Loss: 2.3e+03,  Val Acc: 54.24%,  Time: 0:25:32\n",
      "Epoch [15/20]\n",
      "Iter:  96000,  Train Loss: 7.8e+03,  Train Acc: 54.69%,  Val Loss: 2.3e+03,  Val Acc: 54.27%,  Time: 0:25:51\n",
      "Iter:  97000,  Train Loss: 7.8e+03,  Train Acc: 54.72%,  Val Loss: 2.3e+03,  Val Acc: 54.22%,  Time: 0:26:09\n",
      "Iter:  98000,  Train Loss: 7.8e+03,  Train Acc: 54.75%,  Val Loss: 2.3e+03,  Val Acc: 54.28%,  Time: 0:26:26\n",
      "Iter:  99000,  Train Loss: 7.8e+03,  Train Acc: 54.76%,  Val Loss: 2.3e+03,  Val Acc: 54.33%,  Time: 0:26:41\n",
      "Iter: 100000,  Train Loss: 7.8e+03,  Train Acc: 54.70%,  Val Loss: 2.3e+03,  Val Acc: 54.22%,  Time: 0:26:57\n",
      "Iter: 101000,  Train Loss: 7.8e+03,  Train Acc: 54.73%,  Val Loss: 2.3e+03,  Val Acc: 54.26%,  Time: 0:27:12\n",
      "Iter: 102000,  Train Loss: 7.8e+03,  Train Acc: 54.75%,  Val Loss: 2.3e+03,  Val Acc: 54.27%,  Time: 0:27:28\n",
      "Epoch [16/20]\n",
      "Iter: 103000,  Train Loss: 7.8e+03,  Train Acc: 54.79%,  Val Loss: 2.2e+03,  Val Acc: 54.32%,  Time: 0:27:44\n",
      "Iter: 104000,  Train Loss: 7.8e+03,  Train Acc: 54.78%,  Val Loss: 2.2e+03,  Val Acc: 54.28%,  Time: 0:28:01\n",
      "Iter: 105000,  Train Loss: 7.8e+03,  Train Acc: 54.85%,  Val Loss: 2.3e+03,  Val Acc: 54.33%,  Time: 0:28:19\n",
      "Iter: 106000,  Train Loss: 7.8e+03,  Train Acc: 54.85%,  Val Loss: 2.2e+03,  Val Acc: 54.35%,  Time: 0:28:37\n",
      "Iter: 107000,  Train Loss: 7.8e+03,  Train Acc: 54.86%,  Val Loss: 2.2e+03,  Val Acc: 54.35%,  Time: 0:28:52\n",
      "Iter: 108000,  Train Loss: 7.8e+03,  Train Acc: 54.85%,  Val Loss: 2.2e+03,  Val Acc: 54.39%,  Time: 0:29:08\n",
      "Iter: 109000,  Train Loss: 7.8e+03,  Train Acc: 54.92%,  Val Loss: 2.2e+03,  Val Acc: 54.42%,  Time: 0:29:23\n",
      "Epoch [17/20]\n",
      "Iter: 110000,  Train Loss: 7.7e+03,  Train Acc: 54.86%,  Val Loss: 2.2e+03,  Val Acc: 54.40%,  Time: 0:29:39\n",
      "Iter: 111000,  Train Loss: 7.7e+03,  Train Acc: 54.93%,  Val Loss: 2.2e+03,  Val Acc: 54.39%,  Time: 0:29:55\n",
      "Iter: 112000,  Train Loss: 7.8e+03,  Train Acc: 54.92%,  Val Loss: 2.2e+03,  Val Acc: 54.40%,  Time: 0:30:13\n",
      "Iter: 113000,  Train Loss: 7.7e+03,  Train Acc: 54.88%,  Val Loss: 2.2e+03,  Val Acc: 54.32%,  Time: 0:30:29\n",
      "Iter: 114000,  Train Loss: 7.7e+03,  Train Acc: 54.95%,  Val Loss: 2.2e+03,  Val Acc: 54.38%,  Time: 0:30:44\n",
      "Iter: 115000,  Train Loss: 7.8e+03,  Train Acc: 55.00%,  Val Loss: 2.2e+03,  Val Acc: 54.48%,  Time: 0:31:02\n",
      "Iter: 116000,  Train Loss: 7.7e+03,  Train Acc: 55.09%,  Val Loss: 2.2e+03,  Val Acc: 54.55%,  Time: 0:31:21\n",
      "Epoch [18/20]\n",
      "Iter: 117000,  Train Loss: 7.7e+03,  Train Acc: 55.05%,  Val Loss: 2.2e+03,  Val Acc: 54.51%,  Time: 0:31:38\n",
      "Iter: 118000,  Train Loss: 7.7e+03,  Train Acc: 55.06%,  Val Loss: 2.2e+03,  Val Acc: 54.49%,  Time: 0:31:57\n",
      "Iter: 119000,  Train Loss: 7.7e+03,  Train Acc: 55.01%,  Val Loss: 2.2e+03,  Val Acc: 54.47%,  Time: 0:32:15\n",
      "Iter: 120000,  Train Loss: 7.7e+03,  Train Acc: 55.06%,  Val Loss: 2.2e+03,  Val Acc: 54.49%,  Time: 0:32:30\n",
      "Iter: 121000,  Train Loss: 7.7e+03,  Train Acc: 55.03%,  Val Loss: 2.2e+03,  Val Acc: 54.50%,  Time: 0:32:46\n",
      "Iter: 122000,  Train Loss: 7.7e+03,  Train Acc: 55.15%,  Val Loss: 2.2e+03,  Val Acc: 54.62%,  Time: 0:33:03\n",
      "Epoch [19/20]\n",
      "Iter: 123000,  Train Loss: 7.7e+03,  Train Acc: 55.13%,  Val Loss: 2.2e+03,  Val Acc: 54.59%,  Time: 0:33:19\n",
      "Iter: 124000,  Train Loss: 7.7e+03,  Train Acc: 55.11%,  Val Loss: 2.2e+03,  Val Acc: 54.53%,  Time: 0:33:35\n",
      "Iter: 125000,  Train Loss: 7.7e+03,  Train Acc: 55.13%,  Val Loss: 2.2e+03,  Val Acc: 54.54%,  Time: 0:33:52\n",
      "Iter: 126000,  Train Loss: 7.7e+03,  Train Acc: 55.13%,  Val Loss: 2.2e+03,  Val Acc: 54.58%,  Time: 0:34:08\n",
      "Iter: 127000,  Train Loss: 7.7e+03,  Train Acc: 55.14%,  Val Loss: 2.2e+03,  Val Acc: 54.55%,  Time: 0:34:24\n",
      "Iter: 128000,  Train Loss: 7.7e+03,  Train Acc: 55.10%,  Val Loss: 2.2e+03,  Val Acc: 54.57%,  Time: 0:34:42\n",
      "Iter: 129000,  Train Loss: 7.7e+03,  Train Acc: 55.18%,  Val Loss: 2.2e+03,  Val Acc: 54.65%,  Time: 0:34:58\n",
      "Epoch [20/20]\n",
      "Iter: 130000,  Train Loss: 7.7e+03,  Train Acc: 55.23%,  Val Loss: 2.2e+03,  Val Acc: 54.63%,  Time: 0:35:17\n",
      "Iter: 131000,  Train Loss: 7.7e+03,  Train Acc: 55.19%,  Val Loss: 2.2e+03,  Val Acc: 54.62%,  Time: 0:35:36\n",
      "Iter: 132000,  Train Loss: 7.7e+03,  Train Acc: 55.25%,  Val Loss: 2.2e+03,  Val Acc: 54.69%,  Time: 0:35:53\n",
      "Iter: 133000,  Train Loss: 7.7e+03,  Train Acc: 55.27%,  Val Loss: 2.2e+03,  Val Acc: 54.66%,  Time: 0:36:10\n",
      "Iter: 134000,  Train Loss: 7.7e+03,  Train Acc: 55.19%,  Val Loss: 2.2e+03,  Val Acc: 54.63%,  Time: 0:36:27\n",
      "Iter: 135000,  Train Loss: 7.7e+03,  Train Acc: 55.21%,  Val Loss: 2.2e+03,  Val Acc: 54.66%,  Time: 0:36:43\n",
      "Iter: 136000,  Train Loss: 7.7e+03,  Train Acc: 55.29%,  Val Loss: 2.2e+03,  Val Acc: 54.72%,  Time: 0:37:00\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "evalute() got an unexpected keyword argument 'batch_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-7bf18fdbee43>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0membedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOneHotEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-b65e05fd6a89>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, config, train_iter, dev_iter, test_iter)\u001b[0m\n\u001b[0;32m     16\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Iter: {0:>6},  Train Loss: {1:>5.2},  Train Acc: {2:>6.2%},  Val Loss: {3:>5.2},  Val Acc: {4:>6.2%},  Time: {5}'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_dev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc_dev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_dif\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0macc_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevalute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Train Loss: {0:>5.2},  Train Acc: {1:>6.2%}'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: evalute() got an unexpected keyword argument 'batch_size'"
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "vocab, train_set, dev_set, test_set = build_dataset(config)\n",
    "train_iter = DatasetIterater(data_set = train_set, batch_size = config.batch_size)\n",
    "dev_iter = DatasetIterater(data_set = dev_set, batch_size = config.batch_size)\n",
    "test_iter = DatasetIterater(data_set = test_set, batch_size = config.batch_size)\n",
    "embedding = OneHotEmbedding(config)\n",
    "model = LogisticModel(embedding, config)\n",
    "train(model, config, train_iter, dev_iter, test_iter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "285.26px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
