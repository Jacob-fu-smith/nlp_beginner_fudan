{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddb1517c",
   "metadata": {},
   "source": [
    "### import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f4f5a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle as pkl\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45afd34f",
   "metadata": {},
   "source": [
    "## data preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4c5a88",
   "metadata": {},
   "source": [
    "### spit evaluate set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6774929e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 156060 entries, 0 to 156059\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   PhraseId    156060 non-null  int64 \n",
      " 1   SentenceId  156060 non-null  int64 \n",
      " 2   Phrase      156060 non-null  object\n",
      " 3   Sentiment   156060 non-null  int64 \n",
      "dtypes: int64(3), object(1)\n",
      "memory usage: 4.8+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = pd.read_csv('./raw_data/train.tsv', sep='\\t')\n",
    "df_data.info()\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29a90db7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 0]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = df_data['Sentiment'].unique()\n",
    "labels = list(labels)\n",
    "labels\n",
    "with open(r'./data_set/labels.txt', 'w', encoding='utf-8') as f:\n",
    "    for label in labels:\n",
    "        f.write(str(label) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d410fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_index = np.random.permutation(len(df_data))\n",
    "shuffled_data = df_data[['Phrase','Sentiment']].iloc[shuffle_index]\n",
    "shuffled_data.iloc[:int(0.7*len(df_data))].to_csv(r'./data_set/train.txt', sep='\\t', encoding='utf-8', header=None, index=False)\n",
    "shuffled_data.iloc[int(0.7*len(df_data))+1:int(0.9*len(df_data))].to_csv(r'./data_set/dev.txt', sep='\\t', encoding='utf-8', header=None, index=False)\n",
    "shuffled_data.iloc[int(0.9*len(df_data))+1:].to_csv(r'./data_set/test.txt', sep='\\t', encoding='utf-8', header=None, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d192ecc",
   "metadata": {},
   "source": [
    "### build vocabulary（BOW） and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42ff3179",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "UNK, PAD = '<UNK>', '<PAD>'  # 未知字，padding符号\n",
    "\n",
    "# 获取单词的词性\n",
    "def get_wordnet_pos(tag):\n",
    "    '''\n",
    "    for WordNetLemmatizer.lemmatize()\n",
    "    '''\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def build_vocab(data_path, vocab_path):\n",
    "    data = [_.strip().split('\\t')[0] for _ in open(data_path, 'r', encoding='utf-8').readlines()]\n",
    "    word_cnt = dict()\n",
    "    for sentence in tqdm(data):\n",
    "        tokens = word_tokenize(sentence.lower())              # 分词,同时大写换小写\n",
    "        tagged_sent = pos_tag(tokens, tagset='universal')     # 词性标注\n",
    "        wnl = WordNetLemmatizer()\n",
    "        for word, tag in tagged_sent:\n",
    "            lemmatized_word = wnl.lemmatize(word, pos=get_wordnet_pos(tag)) # 还原后的词\n",
    "            word_cnt[lemmatized_word] = word_cnt.get(lemmatized_word, 0) + 1\n",
    "    word_cnt = sorted(word_cnt.items(), key=lambda x:x[0], reverse=True)\n",
    "    print(len(word_cnt))\n",
    "    vocab = {_[0]: idx for idx, _ in enumerate(word_cnt)}\n",
    "    vocab.update({UNK: len(vocab), PAD: len(vocab) + 1})\n",
    "    pkl.dump(vocab, open(vocab_path, 'wb'))\n",
    "    print(\"vocab build successed, size : %d\" %len(vocab))\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42cabbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(config):\n",
    "    '''\n",
    "    变成[([],y),([],y),([],y),([],y)]\n",
    "    '''\n",
    "    if os.path.exists(config.vocab_path):\n",
    "        vocab = pkl.load(open(config.vocab_path, 'rb'))\n",
    "    else:\n",
    "        vocab = build_vocab(config.train_path, config.vocab_path)\n",
    "    print(\"vocab loaded sucessed, size : %d \" %len(vocab))\n",
    "    def load_data(file_path, output_path):\n",
    "        if os.path.exists(output_path):\n",
    "            data = pkl.load(open(output_path, 'rb'))\n",
    "            print(\"%s loaded success, size: %d\" %(output_path, len(data)))\n",
    "            return data\n",
    "        data = open(file_path, 'r', encoding='utf-8').readlines()\n",
    "        lemmatized_data = list()\n",
    "        for line in tqdm(data):\n",
    "            try:\n",
    "                x, y = line.strip().split('\\t')\n",
    "            except:\n",
    "                print(line)\n",
    "            tokens = word_tokenize(x.lower())              # 分词,同时大写换小写\n",
    "            tagged_sent = pos_tag(tokens, tagset='universal')     # 词性标注\n",
    "            wnl = WordNetLemmatizer()\n",
    "            lemmatized_sentence = [wnl.lemmatize(word, pos=get_wordnet_pos(tag)) for word, tag in tagged_sent]\n",
    "            lemmatized_data.append((lemmatized_sentence, y))\n",
    "        pkl.dump(lemmatized_data, open(output_path, 'wb'))\n",
    "        print(\"%s loaded success, size: %d\" %(file_path, len(lemmatized_data)))\n",
    "        return lemmatized_data\n",
    "    test_set = load_data(config.test_path, config.test_set_path)\n",
    "    dev_set = load_data(config.dev_path, config.dev_set_path)\n",
    "    train_set = load_data(config.train_path, config.train_set_path)\n",
    "    return vocab, train_set, dev_set, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcaa731",
   "metadata": {},
   "source": [
    "## units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a159b8",
   "metadata": {},
   "source": [
    "### config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48938037",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    def __init__(self):\n",
    "        self.train_path = r'./data_set/train.txt'\n",
    "        self.dev_path = r'./data_set/dev.txt'\n",
    "        self.test_path = r'./data_set/test.txt'\n",
    "        self.vocab_path = r'./data_set/vocab.pkl'\n",
    "        self.train_set_path = r'./data_set/train.pkl'\n",
    "        self.dev_set_path = r'./data_set/dev.pkl'\n",
    "        self.test_set_path = r'./data_set/test.pkl'\n",
    "        self.class_list = [x.strip() for x in open(r'./data_set/labels.txt', encoding='utf-8').readlines()]\n",
    "        self.label_num = len(self.class_list)\n",
    "        self.batch_size = 32\n",
    "        self.epoch = 20\n",
    "        self.learning_rate = 1e-3\n",
    "        self.log_step = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8018c48a",
   "metadata": {},
   "source": [
    "### datasetIterater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01b5ffd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetIterater(object):\n",
    "    '''\n",
    "    return the batch index of dataset\n",
    "    '''\n",
    "    def __init__(self, data_set, batch_size):\n",
    "        self.data_set = data_set\n",
    "        self.batch_size = batch_size\n",
    "        self.n_batches = len(data_set) // batch_size\n",
    "        self.residue = True if len(data_set) % self.n_batches != 0 else False\n",
    "        self.index = 0\n",
    "        \n",
    "    def to_couple(self, raw_batch):\n",
    "        x = [_[0] for _ in raw_batch]\n",
    "        y = [_[1] for _ in raw_batch]\n",
    "        return x,y\n",
    "        \n",
    "    def __next__(self):\n",
    "        if self.index == self.n_batches and self.residue:\n",
    "            raw_batch = self.data_set[self.index * self.batch_size: len(self.data_set)]\n",
    "            batch = self.to_couple(raw_batch)\n",
    "            self.index += 1\n",
    "            return batch\n",
    "        elif self.index >= self.n_batches:\n",
    "            self.index = 0\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            raw_batch = self.data_set[self.index * self.batch_size: (self.index+1) * self.batch_size]\n",
    "            batch = self.to_couple(raw_batch)\n",
    "            self.index += 1\n",
    "            return batch\n",
    "           \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_batches+1 if self.residue else self.n_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f674af85",
   "metadata": {},
   "source": [
    "Test of config and dataload "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e23a9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab loaded sucessed, size : 14670 \n",
      "./data_set/test.pkl loaded success, size: 15605\n",
      "./data_set/dev.pkl loaded success, size: 31211\n",
      "./data_set/train.pkl loaded success, size: 109242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "7000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "9000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "11000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "12000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "13000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "13656"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config()\n",
    "vocab, train_set, dev_set, test_set = build_dataset(config)\n",
    "data_iter = DatasetIterater(data_set = train_set, batch_size = 8)\n",
    "for batch in data_iter:\n",
    "    if data_iter.index % 1000 == 0:\n",
    "        data_iter.index\n",
    "len(data_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368b0b78",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2691e81e",
   "metadata": {},
   "source": [
    "### one hot Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b81c37be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotEmbedding(object):\n",
    "    def __init__(self, config):\n",
    "        self.vocab = pkl.load(open(config.vocab_path, 'rb'))\n",
    "        self.embedding_shape = (config.batch_size, len(self.vocab))\n",
    "        self.embedding_length = len(self.vocab)\n",
    "        \n",
    "    def batch_vectorize(self, x):\n",
    "        '''\n",
    "        batch_data: batch_size * length\n",
    "        '''\n",
    "        vec = np.zeros((len(x), len(vocab)), dtype=int)\n",
    "        for i, tokens in enumerate(x):\n",
    "            vec[i,[vocab.get(token, vocab['<UNK>']) for token in tokens]] = 1\n",
    "        return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ea9ce4",
   "metadata": {},
   "source": [
    "Test of one-hot embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a7f79a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab loaded sucessed, size : 14670 \n",
      "./data_set/test.pkl loaded success, size: 15605\n",
      "./data_set/dev.pkl loaded success, size: 31211\n",
      "./data_set/train.pkl loaded success, size: 109242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(16, 14670)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(array([   56,   227,   269,  1295,  1452,  1543,  1682,  1724,  1941,\n",
       "         2497,  2751,  3697,  3699,  4093,  4654,  4676,  5516,  5609,\n",
       "         5719,  5720,  5739,  6241,  6464,  6885,  8714,  9302,  9617,\n",
       "         9829, 10380, 13401, 13638, 14438, 14637, 14640], dtype=int64),)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab, train_set, dev_set, test_set = build_dataset(config)\n",
    "data_iter = DatasetIterater(data_set = train_set, batch_size = config.batch_size)\n",
    "x, y  = next(data_iter)\n",
    "embedding = OneHotEmbedding(config)\n",
    "vec = embedding.batch_vectorize(x)\n",
    "vec.shape\n",
    "vec[0].nonzero()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c26324",
   "metadata": {},
   "source": [
    "### logistic model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3279f97a",
   "metadata": {},
   "source": [
    "$$\n",
    "logistic = \\frac{1}{1+exp(-w^Tx)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456f58b4",
   "metadata": {},
   "source": [
    "For binary classificaiton:\n",
    "$$\n",
    "loss = -\\frac{1}{N}\\sum^{N}_{n=1}(y^{(n)}log(\\hat{y}^{(n)})+(1-y^{(n)})log(1-\\hat{y}^{(n)}))\n",
    "$$\n",
    "For multi-classification:\n",
    "$$\n",
    "loss = -\\frac{1}{N}\\sum^{N}_{n=1}\\sum^{C}_{c=1}(y^{(c)}log(\\hat{y}^{(c)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc219786",
   "metadata": {},
   "source": [
    "Back Propagation:\n",
    "$$\n",
    "\\frac{\\partial{R(w)}}{\\partial{w}} =\n",
    "-\\frac{1}{N}\\sum^{N}_{n=1}x^{(n)}(y^{(n)}-\\hat{y}^{(n)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20f55088",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticModel(object):\n",
    "    def __init__(self, embedding, config):\n",
    "        self.embedding = embedding\n",
    "        self.weight = np.zeros((config.label_num, self.embedding.embedding_length))  # label_num*14760\n",
    "        self.bias = np.zeros((1, config.label_num))\n",
    "        self.learing_rate = config.learning_rate\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        for multi-logistic, argmax adopted\n",
    "        input: batch * embedding_length\n",
    "        weight: embedding_length * label_num\n",
    "        output: batch * label_num\n",
    "        '''\n",
    "        one_hot_embedding = self.embedding.batch_vectorize(x)\n",
    "        o = one_hot_embedding.dot(self.weight.T) + self.bias\n",
    "        out = 1/(1+np.exp(-o))\n",
    "        return out\n",
    "        \n",
    "    def backward(self, x, y_true, lr):\n",
    "        '''\n",
    "        learing_rate\n",
    "        x: batch*character_length    8*14670\n",
    "        weight: label*num*character  5*14670\n",
    "        bias: 1*label_num\n",
    "        \n",
    "        y_hat:  batch*label_num      8*5\n",
    "        y_true: 1*label_num -> batch*label_num\n",
    "        \n",
    "        '''\n",
    "        one_hot_embedding = self.embedding.batch_vectorize(x)\n",
    "        o = one_hot_embedding.dot(self.weight.T) + self.bias\n",
    "        y_hat = 1/(1+np.exp(-o))\n",
    "        \n",
    "        binary_labels = np.zeros(y_hat.shape)\n",
    "        for i,label in enumerate(y_true):\n",
    "            binary_labels[i,int(label)] = 1\n",
    "        \n",
    "        ### mini-batch gradient descent\n",
    "        ex_x = np.expand_dims(one_hot_embedding, axis=2)\n",
    "        ex_y = np.expand_dims((y_hat - binary_labels), axis=1)\n",
    "        partial_w = np.einsum('ijk,ikn->ijn', ex_x, ex_y).mean(axis=0)\n",
    "        partial_b = (y_hat - binary_labels).mean(axis=0)\n",
    "#         print(ex_x.shape)\n",
    "#         print(ex_y.shape)\n",
    "#         print(\"partial_w shape:\", partial_w.shape)\n",
    "#         print(\"partial_b shape:\", partial_b.shape)\n",
    "        self.weight = self.weight - lr*partial_w.T\n",
    "        self.bias = self.bias - lr*partial_b\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b3de034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=axis, keepdims=True)   \n",
    "\n",
    "def cross_entropy_loss(y_hat, y_true, sm=False):\n",
    "    binary_labels = np.zeros(y_hat.shape)\n",
    "    for i,label in enumerate(y_true):\n",
    "        binary_labels[i,int(label)] = 1\n",
    "    if sm:\n",
    "        y_hat = softmax(y_hat, axis=1)\n",
    "    eps = 1e-15\n",
    "    y_hat = np.where(y_hat > 1-eps, 1-eps, y_hat)\n",
    "    y_hat = np.where(y_hat < eps, eps, y_hat)\n",
    "#     corss_entry = binary_labels*np.log(y_hat) + (1-binary_labels)*np.log(1-y_hat) # binary classification\n",
    "    cross_entropy = binary_labels*np.log(y_hat)\n",
    "    loss = -cross_entropy.sum()/len(y_true)\n",
    "    return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "417543b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab loaded sucessed, size : 14670 \n",
      "./data_set/test.pkl loaded success, size: 15605\n",
      "./data_set/dev.pkl loaded success, size: 31211\n",
      "./data_set/train.pkl loaded success, size: 109242\n"
     ]
    }
   ],
   "source": [
    "## Test for logistic model\n",
    "vocab, train_set, dev_set, test_set = build_dataset(config)\n",
    "data_iter = DatasetIterater(data_set = train_set, batch_size = 8)\n",
    "x, y_true = next(data_iter)\n",
    "embedding = OneHotEmbedding(config)\n",
    "model = LogisticModel(embedding, config)\n",
    "out = model.forward(x)\n",
    "loss = cross_entropy_loss(out, y_true, sm=False)\n",
    "model.backward(x, y_true, config.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb31942",
   "metadata": {},
   "source": [
    "## train and eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40efc87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_dif(start_time):\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    return timedelta(seconds=int(round(time_dif)))\n",
    "\n",
    "def train(model, config, train_iter, dev_iter, test_iter):\n",
    "    start_time = time.time()\n",
    "    total_iter = 0\n",
    "    acc_dev = 0\n",
    "    for epoch in range(config.epoch):\n",
    "        print('Epoch [{}/{}]'.format(epoch + 1, config.epoch))\n",
    "        for i, (x, y_true) in enumerate(train_iter):\n",
    "            out = model.forward(x)\n",
    "            loss = cross_entropy_loss(out, y_true, sm=False)\n",
    "            model.backward(x, y_true, config.learning_rate)\n",
    "            total_iter += 1\n",
    "            if total_iter % config.log_step == 0 or i+1 == len(train_iter):\n",
    "                acc_train, loss_train = evalute(model, config, DatasetIterater(data_set = train_set, batch_size = config.batch_size))\n",
    "                acc_dev, loss_dev = evalute(model, config, DatasetIterater(data_set = dev_set, batch_size = config.batch_size))\n",
    "                improve = \"\"\n",
    "                if acc_dev > best_dev_acc:\n",
    "                    best_dev_acc = acc_dev\n",
    "                    improve = \"*\"\n",
    "                time_dif = get_time_d\n",
    "                time_dif = get_time_dif(start_time)\n",
    "                msg = 'Iter: {0:>6},  Train Loss: {1:>5.2},  Train Acc: {2:>6.2%},  Val Loss: {3:>5.2},  Val Acc: {4:>6.2%},  Time: {5} {6}'\n",
    "                print(msg.format(total_iter, loss_train, acc_train, loss_dev, acc_dev, time_dif, improve))\n",
    "    acc_test, loss_test = evalute(model, config, test_iter)\n",
    "    msg = 'Train Loss: {0:>5.2},  Train Acc: {1:>6.2%}'\n",
    "    print(msg.format(loss_test, acc_test))\n",
    "    \n",
    "def evalute(model, config, data_iter):\n",
    "    loss_total = 0\n",
    "    labels_predict = np.arange(0)\n",
    "    labels_true = []\n",
    "    for (x, y_true) in data_iter:\n",
    "        out = model.forward(x)\n",
    "        loss = cross_entropy_loss(out, y_true, sm=False)\n",
    "        loss_total += loss\n",
    "        labels_predict = np.concatenate((labels_predict, out.argmax(axis=1)), axis=0)\n",
    "        labels_true += y_true\n",
    "    try:\n",
    "        assert len(labels_true) == labels_predict.size\n",
    "    except AssertionError as e:\n",
    "        print(len(x))\n",
    "        print(out.shape)\n",
    "        print(\"labels_true:\", len(labels_true))\n",
    "        print(\"labels_predict:\",labels_predict.size)\n",
    "        raise AssertionError\n",
    "    labels_true = np.array([int(_) for _ in labels_true])\n",
    "    loss = loss_total/len(data_iter)\n",
    "    acc = (labels_true == labels_predict).mean()\n",
    "    return acc, loss_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ee8a04",
   "metadata": {},
   "source": [
    "## run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6934033",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-972bfdd4a79c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1e-2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Config' is not defined"
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "vocab, train_set, dev_set, test_set = build_dataset(config)\n",
    "config.batch_size = 32\n",
    "config.epoch = 50\n",
    "config.learning_rate = 1e-2\n",
    "train_iter = DatasetIterater(data_set = train_set, batch_size = config.batch_size)\n",
    "dev_iter = DatasetIterater(data_set = dev_set, batch_size = config.batch_size)\n",
    "test_iter = DatasetIterater(data_set = test_set, batch_size = config.batch_size)\n",
    "embedding = OneHotEmbedding(config)\n",
    "model = LogisticModel(embedding, config)\n",
    "train(model, config, train_iter, dev_iter, test_iter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "285.26px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
