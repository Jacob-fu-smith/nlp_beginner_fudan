{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddb1517c",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1f4f5a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle as pkl\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45afd34f",
   "metadata": {},
   "source": [
    "## data preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4c5a88",
   "metadata": {},
   "source": [
    "### spit evaluate set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6774929e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 156060 entries, 0 to 156059\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   PhraseId    156060 non-null  int64 \n",
      " 1   SentenceId  156060 non-null  int64 \n",
      " 2   Phrase      156060 non-null  object\n",
      " 3   Sentiment   156060 non-null  int64 \n",
      "dtypes: int64(3), object(1)\n",
      "memory usage: 4.8+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = pd.read_csv('./raw_data/train.tsv', sep='\\t')\n",
    "df_data.info()\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29a90db7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 0]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = df_data['Sentiment'].unique()\n",
    "labels = list(labels)\n",
    "labels\n",
    "with open(r'./data_set/labels.txt', 'w', encoding='utf-8') as f:\n",
    "    for label in labels:\n",
    "        f.write(str(label) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d410fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_index = np.random.permutation(len(df_data))\n",
    "shuffled_data = df_data[['Phrase','Sentiment']].iloc[shuffle_index]\n",
    "shuffled_data.iloc[:int(0.7*len(df_data))].to_csv(r'./data_set/train.txt', sep='\\t', encoding='utf-8', header=None, index=False)\n",
    "shuffled_data.iloc[int(0.7*len(df_data))+1:int(0.9*len(df_data))].to_csv(r'./data_set/dev.txt', sep='\\t', encoding='utf-8', header=None, index=False)\n",
    "shuffled_data.iloc[int(0.9*len(df_data))+1:].to_csv(r'./data_set/test.txt', sep='\\t', encoding='utf-8', header=None, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d192ecc",
   "metadata": {},
   "source": [
    "### build vocabulary and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42cabbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "UNK, PAD = '<UNK>', '<PAD>'  # 未知字，padding符号\n",
    "\n",
    "# 获取单词的词性\n",
    "def get_wordnet_pos(tag):\n",
    "    '''\n",
    "    for WordNetLemmatizer.lemmatize()\n",
    "    '''\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def build_vocab(data_path, vocab_path):\n",
    "    data = [_.strip().split('\\t')[0] for _ in open(data_path, 'r', encoding='utf-8').readlines()]\n",
    "    word_cnt = dict()\n",
    "    for sentence in tqdm(data):\n",
    "        tokens = word_tokenize(sentence.lower())              # 分词,同时大写换小写\n",
    "        tagged_sent = pos_tag(tokens, tagset='universal')     # 词性标注\n",
    "        wnl = WordNetLemmatizer()\n",
    "        for word, tag in tagged_sent:\n",
    "            lemmatized_word = wnl.lemmatize(word, pos=get_wordnet_pos(tag)) # 还原后的词\n",
    "            word_cnt[lemmatized_word] = word_cnt.get(lemmatized_word, 0) + 1\n",
    "    word_cnt = sorted(word_cnt.items(), key=lambda x:x[0], reverse=True)\n",
    "    print(len(word_cnt))\n",
    "    vocab = {_[0]: idx for idx, _ in enumerate(word_cnt)}\n",
    "    vocab.update({UNK: len(vocab), PAD: len(vocab) + 1})\n",
    "    pkl.dump(vocab, open(vocab_path, 'wb'))\n",
    "    print(\"vocab build successed, size : %d\" %len(vocab))\n",
    "    return vocab\n",
    "\n",
    "def build_dataset(config):\n",
    "    '''\n",
    "    变成[([],y),([],y),([],y),([],y)]\n",
    "    '''\n",
    "    if os.path.exists(config.vocab_path):\n",
    "        vocab = pkl.load(open(config.vocab_path, 'rb'))\n",
    "    else:\n",
    "        vocab = build_vocab(config.train_path, config.vocab_path)\n",
    "    print(\"vocab loaded sucessed, size : %d \" %len(vocab))\n",
    "    def load_data(file_path, output_path):\n",
    "        if os.path.exists(output_path):\n",
    "            data = pkl.load(open(output_path, 'rb'))\n",
    "            print(\"%s loaded success, size: %d\" %(output_path, len(data)))\n",
    "            return data\n",
    "        data = open(file_path, 'r', encoding='utf-8').readlines()\n",
    "        lemmatized_data = list()\n",
    "        for line in tqdm(data):\n",
    "            try:\n",
    "                x, y = line.strip().split('\\t')\n",
    "            except:\n",
    "                print(line)\n",
    "            tokens = word_tokenize(x.lower())              # 分词,同时大写换小写\n",
    "            tagged_sent = pos_tag(tokens, tagset='universal')     # 词性标注\n",
    "            wnl = WordNetLemmatizer()\n",
    "            lemmatized_sentence = [wnl.lemmatize(word, pos=get_wordnet_pos(tag)) for word, tag in tagged_sent]\n",
    "            lemmatized_data.append((lemmatized_sentence, y))\n",
    "        pkl.dump(lemmatized_data, open(output_path, 'wb'))\n",
    "        print(\"%s loaded success, size: %d\" %(file_path, len(lemmatized_data)))\n",
    "        return lemmatized_data\n",
    "    test_set = load_data(config.test_path, config.test_set_path)\n",
    "    dev_set = load_data(config.dev_path, config.dev_set_path)\n",
    "    train_set = load_data(config.train_path, config.train_set_path)\n",
    "    return vocab, train_set, dev_set, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcaa731",
   "metadata": {},
   "source": [
    "## config and data loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a159b8",
   "metadata": {},
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48938037",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    def __init__(self):\n",
    "        self.train_path = r'./data_set/train.txt'\n",
    "        self.dev_path = r'./data_set/dev.txt'\n",
    "        self.test_path = r'./data_set/test.txt'\n",
    "        self.vocab_path = r'./data_set/vocab.pkl'\n",
    "        self.train_set_path = r'./data_set/train.pkl'\n",
    "        self.dev_set_path = r'./data_set/dev.pkl'\n",
    "        self.test_set_path = r'./data_set/test.pkl'\n",
    "        self.class_list = [x.strip() for x in open(r'./data_set/labels.txt', encoding='utf-8').readlines()]\n",
    "        self.label_num = len(self.class_list)\n",
    "        self.batch_size = 8\n",
    "        self.epoch = 10\n",
    "        self.learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8018c48a",
   "metadata": {},
   "source": [
    "DatasetIterater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01b5ffd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetIterater(object):\n",
    "    '''\n",
    "    return the batch index of dataset\n",
    "    '''\n",
    "    def __init__(self, data_set, batch_size):\n",
    "        self.data_set = data_set\n",
    "        self.batch_size = batch_size\n",
    "        self.n_batches = len(data_set) // batch_size\n",
    "        self.residue = True if len(data_set) % self.n_batches != 0 else False\n",
    "        self.index = 0\n",
    "        \n",
    "    def to_couple(self, raw_batch):\n",
    "        x = [_[0] for _ in raw_batch]\n",
    "        y = [_[1] for _ in raw_batch]\n",
    "        return x,y\n",
    "        \n",
    "    def __next__(self):\n",
    "        if self.index == self.n_batches and self.residue:\n",
    "            raw_batch = self.data_set[self.index * self.batch_size: len(data_set)]\n",
    "            batch = self.to_couple(raw_batch)\n",
    "            self.index += 1\n",
    "            return batch\n",
    "        elif self.index >= self.n_batches:\n",
    "            self.index = 0\n",
    "            return StopIteration\n",
    "        else:\n",
    "            raw_batch = self.data_set[self.index * self.batch_size: (self.index+1) * self.batch_size]\n",
    "            batch = self.to_couple(raw_batch)\n",
    "            self.index += 1\n",
    "            return batch\n",
    "           \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_batches+1 if self.residue else self.n_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ff1185e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab loaded sucessed, size : 14670 \n",
      "./data_set/test.pkl loaded success, size: 15605\n",
      "./data_set/dev.pkl loaded success, size: 31211\n",
      "./data_set/train.pkl loaded success, size: 109242\n"
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "vocab, train_set, dev_set, test_set = build_dataset(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368b0b78",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2691e81e",
   "metadata": {},
   "source": [
    "### one hot Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b81c37be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotEmbedding(object):\n",
    "    def __init__(self, config):\n",
    "        self.vocab = pkl.load(open(config.vocab_path, 'rb'))\n",
    "        self.embedding_shape = (config.batch_size, len(self.vocab))\n",
    "        self.embedding_length = len(self.vocab)\n",
    "        \n",
    "    def batch_vectorize(self, batch_data):\n",
    "        '''\n",
    "        batch_data: batch_size * length\n",
    "        '''\n",
    "        x, y = batch_data[0], batch_data[1]\n",
    "        vec = np.zeros((len(x), len(vocab)), dtype=int)\n",
    "        for i, tokens in enumerate(x):\n",
    "            vec[i,[vocab.get(token, vocab['<UNK>']) for token in tokens]] = 1\n",
    "        return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c26324",
   "metadata": {},
   "source": [
    "### logistic model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3279f97a",
   "metadata": {},
   "source": [
    "$$\n",
    "logistic = \\frac{1}{1+exp(-w^Tx)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "20f55088",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticModel(object):\n",
    "    def __init__(self, embedding, config):\n",
    "        self.embedding = embedding\n",
    "        self.weight = np.zeros((self.embedding.embedding_length, config.label_num))\n",
    "        self.bias = np.zeros((1, config.label_num)) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        for multi-logistic, argmax adopted\n",
    "        input: batch * embedding_length\n",
    "        weight: embedding_length * label_num\n",
    "        output: batch * label_num\n",
    "        '''\n",
    "        one_hot_embedding = self.embedding.batch_vectorize(x)\n",
    "        o = one_hot_embedding.dot(self.weight) + self.bias\n",
    "        out = 1/(1+np.exp(-o))\n",
    "        return out\n",
    "        \n",
    "#     def backword(self, loss):\n",
    "#         self.weight\n",
    "#         self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7ea8b1",
   "metadata": {},
   "source": [
    "For binary classificaiton:\n",
    "$$\n",
    "loss = -\\frac{1}{N}\\sum^{N}_{n=1}(y^{(n)}log(\\hat{y}^{(n)})+(1-y^{(n)})log(1-\\hat{y}^{(n)}))\n",
    "$$\n",
    "For multi-classification:\n",
    "$$\n",
    "loss = -\\frac{1}{N}\\sum^{N}_{n=1}\\sum^{C}_{c=1}(y^{(c)}log(\\hat{y}^{(c)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "7b3de034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_hat, target):\n",
    "    binary_labels = np.zeros(y_hat.shape)\n",
    "    for i,label in enumerate(target):\n",
    "        binary_labels[i,int(label)] = 1\n",
    "    eps = 1e-15\n",
    "    y_hat = np.where(y_hat > 1-eps, 1-eps, y_hat)\n",
    "    y_hat = np.where(y_hat < eps, eps, y_hat)\n",
    "#     corss_entry = binary_labels*np.log(y_hat) + (1-binary_labels)*np.log(1-y_hat) # binary classification\n",
    "    cross_entropy = binary_labels*np.log(y_hat)\n",
    "    loss = -cross_entropy.sum()/len(target)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "500fdf66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab loaded sucessed, size : 14670 \n",
      "./data_set/test.pkl loaded success, size: 15605\n",
      "./data_set/dev.pkl loaded success, size: 31211\n",
      "./data_set/train.pkl loaded success, size: 109242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8, 14670)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(array([   56,   227,   269,  1295,  1452,  1543,  1682,  1724,  1941,\n",
       "         2497,  2751,  3697,  3699,  4093,  4654,  4676,  5516,  5609,\n",
       "         5719,  5720,  5739,  6241,  6464,  6885,  8714,  9302,  9617,\n",
       "         9829, 10380, 13401, 13638, 14438, 14637, 14640], dtype=int64),)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab, train_set, dev_set, test_set = build_dataset(config)\n",
    "data_iter = DatasetIterater(data_set = train_set, batch_size = 8)\n",
    "iter_test = next(data_iter)\n",
    "embedding = OneHotEmbedding(config)\n",
    "vec = embedding.batch_vectorize(iter_test)\n",
    "vec.shape\n",
    "vec[0].nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "aaf35fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5, 0.5, 0.5, 0.5, 0.5],\n",
       "       [0.5, 0.5, 0.5, 0.5, 0.5],\n",
       "       [0.5, 0.5, 0.5, 0.5, 0.5],\n",
       "       [0.5, 0.5, 0.5, 0.5, 0.5],\n",
       "       [0.5, 0.5, 0.5, 0.5, 0.5],\n",
       "       [0.5, 0.5, 0.5, 0.5, 0.5],\n",
       "       [0.5, 0.5, 0.5, 0.5, 0.5],\n",
       "       [0.5, 0.5, 0.5, 0.5, 0.5]])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['0', '2', '2', '2', '4', '3', '2', '2']"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticModel(embedding, config)\n",
    "y_hat, target = model.forward(iter_test), iter_test[1]\n",
    "y_hat\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "769ed1e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.0291, -0.4038, -1.6993, -2.0425],\n",
       "        [-2.6909,  0.8483, -0.2365, -2.3098],\n",
       "        [ 0.1845,  0.6412,  0.4283, -1.0026]])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([3, 1, 2])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.2245)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "11.850385700662931"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr1 = torch.randn(3, 4)\n",
    "arr2 = torch.randint(4, (3,), dtype=torch.int64)\n",
    "arr1\n",
    "arr2\n",
    "F.cross_entropy(arr1, arr2)\n",
    "y_hat = arr1.detach().numpy()\n",
    "target = arr2.detach().numpy()\n",
    "cross_entropy_loss(y_hat, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb31942",
   "metadata": {},
   "source": [
    "## train and eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a19184",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90ee8a04",
   "metadata": {},
   "source": [
    "## run"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "285.273px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
